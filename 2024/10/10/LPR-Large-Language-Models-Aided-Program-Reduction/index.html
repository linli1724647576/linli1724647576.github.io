<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>LPR: Large Language Models-Aided Program Reduction | LinLi's Blog</title><meta name="author" content="Lin Li"><meta name="copyright" content="Lin Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LPR: Large Language Models-Aided Program Reduction程序简化是一种广泛使用的技术，用于通过自动最小化触发编译器错误的程序来促进调试编译器。现有的程序简化技术要么是通用的，可以适用于多种编程语言（如Perses和Vulcan），要么是专门为某一特定语言优化的，通过利用特定语言的知识（例如，C-Reduce）。然而，如何在程序简化中将跨语言的通用性与针对">
<meta property="og:type" content="article">
<meta property="og:title" content="LPR: Large Language Models-Aided Program Reduction">
<meta property="og:url" content="http://example.com/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/index.html">
<meta property="og:site_name" content="LinLi&#39;s Blog">
<meta property="og:description" content="LPR: Large Language Models-Aided Program Reduction程序简化是一种广泛使用的技术，用于通过自动最小化触发编译器错误的程序来促进调试编译器。现有的程序简化技术要么是通用的，可以适用于多种编程语言（如Perses和Vulcan），要么是专门为某一特定语言优化的，通过利用特定语言的知识（例如，C-Reduce）。然而，如何在程序简化中将跨语言的通用性与针对">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2024-10-10T12:56:10.000Z">
<meta property="article:modified_time" content="2024-10-10T12:57:34.348Z">
<meta property="article:author" content="Lin Li">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LPR: Large Language Models-Aided Program Reduction',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-10 20:57:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="LinLi's Blog"><span class="site-name">LinLi's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LPR: Large Language Models-Aided Program Reduction</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-10T12:56:10.000Z" title="发表于 2024-10-10 20:56:10">2024-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-10T12:57:34.348Z" title="更新于 2024-10-10 20:57:34">2024-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LPR: Large Language Models-Aided Program Reduction"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LPR-Large-Language-Models-Aided-Program-Reduction"><a href="#LPR-Large-Language-Models-Aided-Program-Reduction" class="headerlink" title="LPR: Large Language Models-Aided Program Reduction"></a><strong>LPR: Large Language Models-Aided Program Reduction</strong></h1><p>程序简化是一种广泛使用的技术，用于通过自动最小化触发编译器错误的程序来促进调试编译器。现有的程序简化技术要么是通用的，可以适用于多种编程语言（如Perses和Vulcan），要么是专门为某一特定语言优化的，通过利用特定语言的知识（例如，C-Reduce）。然而，如何在程序简化中将跨语言的通用性与针对特定语言的最佳性有机结合，仍然是未被探索的领域。</p>
<p>本文提出了LPR，这是一种首次利用大语言模型（LLMs）进行多语言特定程序简化的技术。关键思想在于利用像Perses这样的通用程序简化器的语言通用性，以及LLMs所学到的特定语言语义。具体而言，通用程序简化器可以将程序有效地简化为LLMs能够处理的小尺寸程序；LLMs可以通过学到的语义有效地转换程序，创建新的简化机会，以便通用程序简化器进一步简化程序。</p>
<p>我们在三种编程语言（即C、Rust和JavaScript）上的50个基准测试的详细评估表明，LPR在实用性和优越性上超过了Vulcan，这是当前最先进的通用程序简化工具。在有效性方面，LPR在C、Rust和JavaScript基准上分别比Vulcan生成了24.93%、4.47%和11.71%更小的程序。此外，LPR和Vulcan有潜力互为补充。对于C-Reduce所优化的C语言，通过结合Vulcan和LPR的输出，我们可以达到与C-Reduce相当的程序尺寸。在处理大型复杂程序时，LPR比Vulcan更加高效，在C、Rust和JavaScript基准测试中分别节省了10.77%、34.88%和36.96%的时间。</p>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a><strong>INTRODUCTION</strong></h2><p><font color="red">Background</font></p>
<p><strong>程序简化技术的目的</strong>：</p>
<ul>
<li>程序简化旨在通过最小化触发错误的程序来促进编译器调试，强调效果和效率。</li>
<li>给定一个程序 P 和其保持的属性 ψ，程序简化器会生成一个仍能保持 ψ 的最小化程序 Pmin。</li>
</ul>
<p><strong>程序简化的应用</strong>：</p>
<ul>
<li>广泛应用于软件工程任务中，尤其在编译器测试和调试中。</li>
</ul>
<p><font color="red">Related work</font></p>
<p><strong>程序简化中的挑战</strong>：</p>
<ul>
<li>核心挑战在于<font color="red">通用性与特定语言的平衡</font>：如何在跨语言的通用性和针对某种语言的最佳性之间做出取舍。</li>
</ul>
<p><strong>现有程序简化技术的分类</strong>：</p>
<ul>
<li><strong>语言特定简化</strong>：利用特定语言的语义对程序进行简化，通常效果更好，但需要对语言特性有深刻理解，耗时费力。适用于少数语言（如C、Java、SMT-LIBv2）。</li>
<li><strong>语言通用简化</strong>：适用于多种语言，但缺乏对语言特性和语义的深刻理解，因此无法执行语言特定的转换（如函数内联），也无法利用语言的特殊特性生成最优简化程序。</li>
</ul>
<p><strong>当前的问题</strong>：</p>
<ul>
<li>语言通用简化器虽然适用范围广，但由于缺乏特定语言的语义知识，无法完成特定的简化任务，从而无法生成最优的简化程序。</li>
</ul>
<p><font color="red">Motivation</font></p>
<p><strong>研究目标</strong>：</p>
<ul>
<li>研究旨在找到一种平衡点，结合通用性与针对特定语言的最佳性，通过融合两类程序简化技术的优势，提升程序简化的效果。</li>
</ul>
<p><strong>通用简化器的局限</strong>：</p>
<ul>
<li>通用简化器（如Perses）的主要局限在于无法执行语言特定的转换，缺乏对特定语义信息的理解，限制了进一步的简化效果。</li>
</ul>
<p><font color="red">Potential solution</font></p>
<p><strong>大语言模型（LLMs）的潜力</strong>：</p>
<ul>
<li>研究指出，LLMs能够作为强大的助手，帮助执行语言特定的转换。LLMs已经展示出分析和转换主流语言程序的能力，这为程序简化提供了新的可能性。</li>
</ul>
<p><strong>LLMs的应用前景</strong>：</p>
<ul>
<li>通过利用LLMs的能力，可以使通用简化器更好地理解不同语言的语义，简化器的定制和扩展也会变得更加高效，减少手动实现语言特定简化器或添加额外功能的时间和成本。</li>
</ul>
<p><font color="red">Challenges of Using LLM</font></p>
<p><strong>LLMs的局限性</strong>：</p>
<ul>
<li>LLMs 并不是解决程序简化的万能工具。</li>
<li>由于LLMs的固有限制，LLMs可能无法理解代码中的细微差异，并且容易被不相关的上下文干扰。</li>
<li>程序简化任务中的输入代码通常包含数万行，这超出了LLMs的输入限制。</li>
</ul>
<p><strong>缺乏明确指导</strong>：</p>
<ul>
<li>在没有有效指导的情况下，LLMs不清楚需要执行哪些转换操作。</li>
</ul>
<p><font color="red">Approach</font></p>
<p>**LLMs辅助的程序简化 (LPR)**：</p>
<ul>
<li>LPR 是首个将大语言模型（LLMs）引入程序简化任务的方法。该方法结合了语言通用简化器和LLMs的优势。<ul>
<li><font color="green">不是为了用LLM而用LLM，overclaim</font></li>
</ul>
</li>
<li>LPR通过交替调用通用简化器（实验中使用了Perses）和LLM来简化程序。</li>
<li>首先，Perses将大型程序简化到适合LLM处理的大小，随后LLM根据用户定义的提示进一步转换程序。</li>
<li>之后，Perses再次被调用，因为LLM的转换通常会带来新的简化机会，直到程序无法再被最小化。</li>
</ul>
<p><strong>五种简化转换技术</strong>：</p>
<ul>
<li>LPR使用了五种通用转换技术来进一步减少程序：函数内联、循环展开、数据类型消除、数据类型简化、变量消除。</li>
</ul>
<p><strong>多层次提示方法</strong>：</p>
<ul>
<li>为了应对LLMs的挑战，LPR设计了多层次提示的方法，首先让LLM确定可转换的目标，之后逐步应用转换。</li>
<li>这种方法有助于集中指导LLM，排除不相关的上下文和可能分散LLM注意力的其他目标。<ul>
<li><font color="green">Prompt engineering, 上下文干扰</font></li>
</ul>
</li>
</ul>
<p><font color="red">Evaluation</font></p>
<p><strong>LPR的评估结果</strong>：</p>
<ul>
<li>进行了广泛的评估，显示LPR优于当前最先进的通用算法Vulcan。</li>
<li>在三个基准测试集（C、Rust、JavaScript）上，LPR生成的程序比Vulcan小得多，分别减少了24.93%、4.47%和11.71%。</li>
</ul>
<p><strong>LPR与Vulcan的互补性</strong>：</p>
<ul>
<li>LPR和Vulcan可以互相补充，特别是在针对C-Reduce优化的C语言上，通过结合Vulcan和LPR的输出，达到与C-Reduce相同的程序尺寸。</li>
</ul>
<p><strong>效率与性能</strong>：</p>
<ul>
<li>在程序简化效率上，LPR与Vulcan相当；在用户感知的执行时间上，LPR在简化复杂程序时比Vulcan更高效。</li>
<li>每种简化转换在程序简化过程中都起着至关重要的作用。</li>
</ul>
<p><strong>多层次提示方法的效果</strong>：</p>
<ul>
<li>评估还表明，LPR的多层次提示方法比不使用提示时表现更优，展示了该方法的有效性。</li>
</ul>
<p><font color="red">Contributions</font></p>
<p><strong>引入LPR</strong>：</p>
<ul>
<li>首次尝试使用大语言模型（LLMs）进行程序简化，通过结合语言通用简化器和LLMs的能力，LPR展示了跨语言的通用性和对特定语言语义的理解。LPR还具有通过设计新提示轻松扩展新转换的灵活性。</li>
</ul>
<p><strong>提出多层次提示方法</strong>：</p>
<ul>
<li>提出了一种多层次提示方法来引导LLMs执行程序转换，展示了其实践中的有效性。我们提出了五种通用转换，为LLMs简化程序或创造更多简化机会提供了支持。</li>
</ul>
<p><strong>全面评估LPR</strong>：</p>
<ul>
<li>在C、Rust和JavaScript三种常用语言的50个基准上对LPR进行了全面评估，结果展示了LPR的高效性和通用性。</li>
</ul>
<h2 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a><strong>BACKGROUND</strong></h2><h3 id="2-1-Program-Reduction"><a href="#2-1-Program-Reduction" class="headerlink" title="2.1 Program Reduction"></a><strong>2.1 Program Reduction</strong></h3><p><strong>程序简化的目标</strong>：</p>
<ul>
<li>给定一个程序 P 和某种属性（如触发编译器错误），程序简化的目标是找到一个最小化的程序 Pmin ，它仍然保留该属性。</li>
<li>程序简化通过去除与错误无关的代码，极大地提高了调试的效率。</li>
</ul>
<p><strong>语言通用简化器</strong>：</p>
<ul>
<li><p>一些简化器可以适用于多种语言，如HDD和Perses。这些算法通过解析语法树来删除不必要的节点，生成更小的程序变体。</p>
</li>
<li><p>Vulcan进一步扩展了Perses，引入新的方法来寻找更小的变体，例如替换标识符或子树，删除局部组合。</p>
</li>
<li><p>通用简化器虽然有效，但不能利用特定语言的语义来进一步简化程序，缺乏进行函数内联等转换的能力，无法利用特定程序的特征来进行更深层次的优化。</p>
</li>
</ul>
<p><strong>语言特定简化器</strong>：</p>
<ul>
<li>语言特定简化器针对某些特定语言进行定制，如C-Reduce是C语言最有效的简化工具。它通过多次转换，根据语言特性进行简化。</li>
<li>开发语言特定简化器是复杂的过程，涉及大量的时间和人力成本。设计新的简化器或对现有简化器添加新功能，通常都很耗时耗力。</li>
</ul>
<h3 id="2-2-Large-Language-Models"><a href="#2-2-Large-Language-Models" class="headerlink" title="2.2 Large Language Models"></a><strong>2.2 Large Language Models</strong></h3><p><strong>大语言模型的定义与能力</strong>：</p>
<ul>
<li>大语言模型（LLMs）是基于大规模数据集进行训练的深度学习模型，适用于多种任务。</li>
<li>LLMs不仅擅长处理自然语言，还在理解和处理编程语言方面展现出显著的能力，预示了其在软件工程领域中的广阔前景。</li>
</ul>
<p><strong>LLMs的应用</strong>：</p>
<ul>
<li>LLMs最近被应用于多项软件工程任务，如自动程序修复和程序生成，展示了其在这些领域中的潜力。</li>
</ul>
<p><strong>LLMs的局限性</strong>：</p>
<ul>
<li>尽管LLMs用途广泛，但一些研究表明，当前的LLMs在区分程序中的细微差异上存在不足。</li>
<li>随着输入规模的增加，LLMs的记忆和处理能力会下降，因此无法期望LLMs自动完成复杂任务，必须有针对性地进行引导。</li>
<li>在程序简化过程中，直接让LLMs处理成千上万行代码是不切实际的。</li>
</ul>
<h2 id="APPROACH"><a href="#APPROACH" class="headerlink" title="APPROACH"></a><strong>APPROACH</strong></h2><h3 id="3-1-Motivation"><a href="#3-1-Motivation" class="headerlink" title="3.1 Motivation"></a><strong>3.1 Motivation</strong></h3><p>这个例子展示了LPR（利用大语言模型的程序简化）的优势。具体步骤如下：</p>
<ol>
<li><strong>原始代码的复杂性</strong>：<ul>
<li>原始代码包含高度嵌套的循环结构，如图1a所示。嵌套循环通常难以进行有效的简化。</li>
</ul>
</li>
<li><strong>循环展开</strong>：<ul>
<li>通过<strong>LLM的语义转换</strong>，嵌套的循环在图1b至1c中被完全展开，形成了数百行代码。这一过程称为<strong>循环展开（Loop Unrolling）</strong>。</li>
</ul>
</li>
<li><strong>进一步简化</strong>：<ul>
<li>尽管在展开循环时代码暂时变得更长，但Perses接下来有效地消除了除了与错误相关的行之外的所有其他行。最终结果就是图1d所示的LPR简化后的代码。</li>
</ul>
</li>
<li><strong>对比其他简化器</strong>：<ul>
<li><strong>Vulcan</strong>：图1e显示，Vulcan通过替换标识符和树节点来尝试简化，但无法避免陷入局部最小值，效果不如LPR。</li>
<li><strong>C-Reduce</strong>：虽然C-Reduce也能简化代码，但它无法完全展开循环结构，因为它没有与循环展开技术集成。</li>
</ul>
</li>
<li><strong>对比手动实现</strong>：<ul>
<li>尽管可以在未来版本的C-Reduce中加入循环展开技术，但实现这种特定转换的代价非常高，相比之下，通过自然语言提示定义转换的方式更加省时省力。</li>
</ul>
</li>
</ol>
<p><img src="/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/image-20241010191650171.png" alt="image-20241010191650171"></p>
<h3 id="3-2-Workflow"><a href="#3-2-Workflow" class="headerlink" title="3.2 Workflow"></a><strong>3.2 Workflow</strong></h3><p><strong>工作流程概述</strong>：</p>
<ul>
<li>LPR接收一个触发错误的程序作为输入，通过交替调用语言通用简化器和LLM，直到程序无法进一步简化为止。</li>
<li>每次迭代中，语言通用简化器首先将程序简化到最小，而LLM基于语言的语义知识，进一步对简化后的程序进行转换。这个过程由用户定义的提示引导，以暴露更多简化的机会。</li>
</ul>
<p><strong>预定义提示的组成</strong>：</p>
<ul>
<li>预定义的提示包括  primaryQuestion 和 followupQuestion ：<ul>
<li><code>primaryQuestion</code>让LLM确定要转换的目标。</li>
<li><code>followupQuestion</code>引导LLM对每个目标逐一进行转换。</li>
</ul>
</li>
</ul>
<p><strong>示例</strong>：</p>
<ul>
<li>比如在循环展开的例子中，LLM被要求识别要展开的循环，并生成目标列表，随后进行展开并简化代码。</li>
</ul>
<p><strong>过程细节</strong>：</p>
<ul>
<li>在每个转换过程中，LLM可以生成多个经过转换的程序，LPR会保留最小的那个，同时仍然通过了属性测试的程序，其他则被丢弃。</li>
<li>如果没有合适的转换程序，LPR会保留之前的版本。</li>
<li>最后，LPR通过语言通用简化器（如Perses）进一步简化，直到程序无法再简化。</li>
</ul>
<p><img src="/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/image-20241010192158378.png" alt="image-20241010192158378"></p>
<h3 id="3-3-Transformations"><a href="#3-3-Transformations" class="headerlink" title="3.3 Transformations"></a><strong>3.3 Transformations</strong></h3><p><strong>提出的五种通用转换</strong>：</p>
<ul>
<li>为了通过LLM进一步简化程序，本文提出了五种通用转换方法：<ul>
<li><strong>函数内联（Function Inlining）</strong>：将函数的所有调用点替换为相应的函数体，消除多余的函数调用。</li>
<li><strong>循环展开（Loop Unrolling）</strong>：识别并展开循环，将循环变成重复的代码片段，减少复杂性。</li>
<li><strong>数据类型消除（Data Type Elimination）</strong>：消除与错误无关的数据类型，如C中的<code>typedef</code>或Rust中的<code>type</code>别名。</li>
<li><strong>数据类型简化（Data Type Simplification）</strong>：将复杂的数据类型转换为更简单的原始数据类型。</li>
<li><strong>变量消除（Variable Elimination）</strong>：优化程序中的中间变量和未使用的变量，简化代码。</li>
<li><font color="green">类似于SQLess中的精简策略</font></li>
</ul>
</li>
</ul>
<p><strong>转换应用范围广泛</strong>：</p>
<ul>
<li>这些转换可以广泛应用于多种编程语言，减少了设计和实现针对特定语言简化器的工作量。</li>
<li>通过LLM进行这些转换，简化了过程，同时扩展了这些方法在不同编程语言中的应用。</li>
</ul>
<p><strong>对比现有简化器</strong>：</p>
<ul>
<li>某些语言特定简化器（如C-Reduce）已经包含了一些转换技术（如函数内联和变量消除），但创建新的转换仍然是一项复杂的任务。本文的方法简化了这个过程，并扩展了其跨多种编程语言的适用性。</li>
</ul>
<h3 id="3-4-Multi-level-Prompts"><a href="#3-4-Multi-level-Prompts" class="headerlink" title="3.4 Multi-level Prompts"></a><strong>3.4 Multi-level Prompts</strong></h3><p><img src="/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/image-20241010201657600.png" alt="image-20241010201657600"></p>
<p><strong>多层次提示的作用</strong>：</p>
<ul>
<li>多层次提示使LLMs能够逐步应用前述的转换方法，如函数内联，而不是让LLMs一次性执行所有的转换，这样可以避免过载处理能力。</li>
</ul>
<p><strong>函数内联示例</strong>：</p>
<ul>
<li>以函数内联为例，首先提出一个初步问题：给定程序，识别所有可以内联的函数（第1步）。</li>
<li>基于LLM返回的函数列表，接着提出后续问题，如“给定指定函数，进行内联优化”（第3步和第5步）。</li>
<li>LLM根据提示执行相应的转换（第4步和第6步）。</li>
</ul>
<p><strong>提示的效果</strong>：</p>
<ul>
<li>这种策略能够排除无关上下文，使查询更加精准，从而提高LLM生成高质量结果的可能性。</li>
<li>其他转换也可以遵循类似的模板，先让LLM识别目标，再引导它优化每个目标。</li>
</ul>
<h2 id="EVALUATION"><a href="#EVALUATION" class="headerlink" title="EVALUATION"></a><strong>EVALUATION</strong></h2><p><strong>RQ1</strong>：LPR在程序简化中的有效性如何？</p>
<p><strong>RQ2</strong>：LPR在程序简化中的效率在用户看来达到何种程度？</p>
<p><strong>RQ3</strong>：LPR中每种转换的有效性如何？</p>
<h3 id="4-1-Experimental-Setup"><a href="#4-1-Experimental-Setup" class="headerlink" title="4.1 Experimental Setup"></a><strong>4.1 Experimental Setup</strong></h3><p><strong>实验工具和环境</strong>：</p>
<ul>
<li>实验中使用了Perses作为语言无关的简化器，并与OpenAI的GPT-3.5-turbo结合。实验还设计了LPR+Vulcan的变体，进一步简化LPR的输出。</li>
<li>实验在Ubuntu 22服务器上运行，配置为Intel Xeon CPU（2.60GHz，512GB RAM），所有算法都在单线程、单进程的环境下执行。</li>
</ul>
<p><strong>基准测试</strong>：</p>
<ul>
<li>使用了三个基准测试集：Benchmark-C、Benchmark-Rust和Benchmark-JS，分别测试C语言、Rust语言和JavaScript的程序。</li>
<li>Benchmark-C包含20个复杂的真实世界错误，Benchmark-Rust包含20个Rust程序错误，Benchmark-JS通过FuzzJIT生成10个导致JIT编译器错误的JavaScript程序。</li>
</ul>
<p><strong>基线比较</strong>：</p>
<ul>
<li>实验采用了Perses和Vulcan作为基线，Vulcan基于Perses，并进一步减少了程序的token数量，但运行时间有所增加。还包含C-Reduce（v2.9.0）作为对比，它在C语言中表现最优，也可应用于其他语言。</li>
</ul>
<p><strong>配置和设置</strong>：</p>
<ul>
<li>默认使用OpenAI API进行实验，LLM的<code>temperature</code>设置为1.0，以鼓励生成更多样的结果。每个查询生成5个结果，选择符合条件的最小程序。</li>
</ul>
<h3 id="4-2-RQ1-Reduction-Effectiveness"><a href="#4-2-RQ1-Reduction-Effectiveness" class="headerlink" title="4.2  RQ1: Reduction Effectiveness"></a><strong>4.2  RQ1: Reduction Effectiveness</strong></h3><ol>
<li><strong>评估方法</strong>：<ul>
<li>通过最终程序的token数量来衡量LPR、LPR+Vulcan以及基线算法的简化效果。程序越小，表示移除了更多无关的错误代码，从而减少开发人员的手动工作。</li>
</ul>
</li>
<li><strong>Benchmark-C</strong>：<ul>
<li>Perses将程序简化到平均247.8个token，Vulcan进一步压缩到157.4个token，LPR将程序缩小到105.8个token，减少了24.93%。</li>
<li>C-Reduce表现最优，生成了85.7个token的最小程序。然而，LPR+Vulcan的表现仍然出色，在20个基准测试中，LPR+Vulcan在13个中超过了C-Reduce。</li>
</ul>
</li>
<li><strong>Benchmark-Rust</strong>：<ul>
<li>Perses和Vulcan分别将程序简化到212.5和184.2个token，LPR和LPR+Vulcan进一步缩小到147.5和135.5个token。</li>
<li>C-Reduce再次表现最优，但LPR和Vulcan在对较大的程序执行语义转换和局部穷举搜索时表现突出。</li>
</ul>
</li>
<li><strong>Benchmark-JS</strong>：<ul>
<li>JavaScript程序相对较小，Perses将程序简化到55.5个token，LPR和LPR+Vulcan分别缩小到38.2和27.5个token。</li>
<li>C-Reduce在JavaScript中的表现不如其他算法，因为它没有使用JavaScript的特定优化。</li>
</ul>
</li>
</ol>
<p><strong>总结</strong>：LPR在不同基准测试中的表现证明了其在程序简化中的有效性，尤其是在与Vulcan配合使用时，能够在多个基准上产生最佳结果。</p>
<p><img src="/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/image-20241010203733356.png" alt="image-20241010203733356"></p>
<h3 id="4-3-RQ2-Reduction-Efficiency"><a href="#4-3-RQ2-Reduction-Efficiency" class="headerlink" title="4.3 RQ2: Reduction Efficiency"></a><strong>4.3 RQ2: Reduction Efficiency</strong></h3><ol>
<li><strong>评估方法</strong>：<ul>
<li>通过程序简化所需的时间来衡量LPR、Vulcan和C-Reduce的效率。较短的时间表示更高的效率。</li>
</ul>
</li>
<li><strong>Benchmark-C</strong>：<ul>
<li>LPR的平均运行时间为1小时54分钟54秒，比Vulcan的2小时8分钟46秒短10.77%。但是，与Vulcan相比，LPR在Benchmark-C中总共需要多出45.83%的时间。LPR在处理大且复杂的程序时效率更高，而Vulcan在简化较小且简单的程序时更快。</li>
</ul>
</li>
<li><strong>Benchmark-Rust</strong>：<ul>
<li>类似的趋势表明，LPR在处理复杂程序时更有效率，而Vulcan在处理较小且简单的程序时表现更好。对于超过一小时的基准，LPR比Vulcan节省了4.15%和21.69%的时间。</li>
</ul>
</li>
<li><strong>Benchmark-JS</strong>：<ul>
<li>在Benchmark-JS中，LPR的平均执行时间为1.915小时、1.839小时和0.174小时。LPR消耗的时间大部分用于等待LLM响应，分别占32.26%、28.11%和72.99%。</li>
</ul>
</li>
<li><strong>效率成本</strong>：<ul>
<li>以Benchmark-C为例，每个基准的平均成本为0.42美元，每个基准测试平均需要41次查询，每次查询大约耗时39秒。</li>
</ul>
</li>
</ol>
<p><strong>结论</strong>：尽管LPR的效率较高，但受限于LLM的响应时间。然而，随着LLM技术的进步，未来LPR的效率有望显著提高。</p>
<h3 id="4-4-RQ3-Effectiveness-of-Each-Transformation"><a href="#4-4-RQ3-Effectiveness-of-Each-Transformation" class="headerlink" title="4.4 RQ3: Effectiveness of Each Transformation"></a><strong>4.4 RQ3: Effectiveness of Each Transformation</strong></h3><ol>
<li><strong>评估方法</strong>：<ul>
<li>研究分析了每种转换对程序大小的影响，并探讨了它们逃避局部最小化并解锁新简化机会的潜力。主要通过Benchmark-C来进行评估。</li>
</ul>
</li>
<li><strong>转换的效果</strong>：<ul>
<li>每次转换后，程序大小的变化通过箱线图和点图进行展示。</li>
<li>部分转换能够立即减少程序大小：<ul>
<li><strong>函数内联</strong>：平均减少132个token。</li>
<li><strong>数据类型消除</strong>：减少4.7个token。</li>
<li><strong>变量消除</strong>：减少4.3个token。</li>
</ul>
</li>
<li><strong>循环展开</strong>和<strong>数据类型简化</strong>在初次应用时通常会增加程序的大小，但随后通过Perses执行的后续简化步骤能够减少最终程序大小。</li>
</ul>
</li>
<li><strong>对整体影响的分析</strong>：<ul>
<li>计算每个转换在所有基准测试中的平均贡献。函数内联是影响最大的转换，平均减少88.2个token，占总减少量的62.12%。循环展开的贡献较小，仅减少了4.8个token，占总减少量的3.35%。</li>
</ul>
</li>
<li><strong>不同转换的普及率</strong>：<ul>
<li>数据类型消除影响了所有基准，原因是<code>typedef</code>在所有基准中广泛存在。</li>
<li>循环展开是最少应用的转换，仅影响了20个基准中的6个。</li>
</ul>
</li>
</ol>
<p><strong>总结</strong>：每种转换在程序简化中的贡献程度各不相同，函数内联和数据类型消除的影响最大，而循环展开的效果较小。部分转换在实际应用中遇到了挑战，尤其是复杂结构的转换。</p>
<h2 id="DISCUSSION"><a href="#DISCUSSION" class="headerlink" title="DISCUSSION"></a><strong>DISCUSSION</strong></h2><h3 id="5-1-The-Effectiveness-of-Multi-level-Prompt"><a href="#5-1-The-Effectiveness-of-Multi-level-Prompt" class="headerlink" title="5.1 The Effectiveness of Multi-level Prompt"></a><strong>5.1 The Effectiveness of Multi-level Prompt</strong></h3><ol>
<li><strong>目的</strong>：<ul>
<li>为验证多层次提示的有效性，设计了单层次提示作为对比，并评估其与多层次提示的效果差异。</li>
<li>单层次提示将<code>primaryQuestion</code>和<code>followupQuestion</code>合并为一个问题，例如“给定程序，识别并内联一个可以内联的函数”。</li>
</ul>
</li>
<li><strong>实验结果</strong>：<ul>
<li>在Benchmark-C上进行的5次实验中，单层次提示的平均结果是155.0 ± 8.7个token，远高于多层次提示的105.8 ± 4.4个token，表明单层次提示效果较差。</li>
<li>原因可能是单层次提示虽然更简洁，但使得LLM在目标上的集中度下降，从而可能遗漏一些简化机会。</li>
</ul>
</li>
</ol>
<p><strong>结论</strong>：多层次提示在引导LLMs进行转换时更加有效，因为它能够使LLM更加专注于特定目标。</p>
<h3 id="5-2-The-Impact-of-Temperature"><a href="#5-2-The-Impact-of-Temperature" class="headerlink" title="5.2 The Impact of Temperature"></a><strong>5.2 The Impact of Temperature</strong></h3><ol>
<li><strong>实验设置</strong>：<ul>
<li>实验中，LLM的温度参数通常设置为1.0。为了衡量该参数对性能的影响，进行了多次实验，测试了温度值为1.0、0.75、0.5、0.25和0的情况。</li>
<li>高温度会指示LLM生成更多创造性和多样化的结果。</li>
</ul>
</li>
<li><strong>实验结果</strong>：<ul>
<li>表3展示了不同温度下的简化结果，温度为0的效果最差，生成的程序比其他温度更大。</li>
<li>其余温度下，性能相似，但较高的温度（如1.0和0.75）能生成更多样化的结果，帮助LPR更好地探索简化机会。</li>
</ul>
</li>
<li><strong>温度的影响分析</strong>：<ul>
<li>较低的温度（如0）限制了输出的多样性，阻碍了LPR探索局部最小值，从而影响了程序简化的效果。</li>
<li>随机性有利于帮助LPR在一些基准测试中生成比C-Reduce更小的程序，但也会引入不确定性，使得结果可能有较大的波动，特别是在复杂的程序中。</li>
</ul>
</li>
</ol>
<p><strong>总结</strong>：较高的温度参数能够更有效地帮助LPR探索不同的简化路径，提高简化效果，而较低的温度限制了输出的多样性，表现较差。</p>
<h3 id="5-3-Failures-in-LPR"><a href="#5-3-Failures-in-LPR" class="headerlink" title="5.3 Failures in LPR"></a><strong>5.3 Failures in</strong> <strong>LPR</strong></h3><p><strong>失败的常见性</strong>：</p>
<ul>
<li>在LPR中，生成无法通过属性测试的变体是常见且可以接受的现象。主要有两种场景导致LPR未能生成通过测试的变体：<ul>
<li><strong>非确定性</strong>：LLM通常是非确定性的，可能无法在复杂程序中执行正确的转换。因此，为了减少潜在的失败，每次LLM查询都会请求多个响应。</li>
<li><strong>属性测试失败</strong>：有时转换消除了触发错误的模式（例如，编译器在函数调用处崩溃，但内联了该函数），即使语义正确，属性测试仍然可能失败。</li>
</ul>
</li>
</ul>
<p><strong>失败的处理</strong>：</p>
<ul>
<li>当转换后的程序无法通过测试时，LPR会继续使用原始程序进行下一步尝试。失败虽然增加了测试次数，但这是程序简化中的一种不可避免的“试错”过程。</li>
<li>即使在没有LLM的情况下，像Perses和Vulcan这样的简化器也会有大约90%的失败率。</li>
</ul>
<p><strong>结论</strong>：</p>
<ul>
<li>LPR通过多个响应来减轻失败的影响，表现依然高效和有效，如表1和表2所示。</li>
</ul>
<h3 id="5-4-Threats-to-Validity"><a href="#5-4-Threats-to-Validity" class="headerlink" title="5.4 Threats to Validity"></a><strong>5.4 Threats to Validity</strong></h3><ol>
<li><strong>内部有效性威胁</strong>：<ul>
<li>主要威胁来自数据泄露问题，即LLMs是否通过逐步分析提供了合理的转换，还是简单地记忆了公开可用的最小化程序。</li>
<li>该威胁通过多个角度进行缓解：<ul>
<li>程序简化涉及触发编译器错误的随机生成程序，通常规模大且无特定用途，因此LLMs不太可能记住这些无序内容，降低了数据泄露的风险。</li>
<li>Benchmark-JS基准测试集由作者通过JIT模糊测试工具创建，且不公开，这确保了LLMs无法通过记忆解决问题。</li>
</ul>
</li>
</ul>
</li>
<li><strong>外部有效性威胁</strong>：<ul>
<li>主要威胁在于LPR跨语言的泛化能力。虽然LPR方法是语言无关的，但LLMs可能对某些语言的知识有限，影响其表现。对此，研究通过在C、Rust和JavaScript上评估LPR，证明了其在多种流行编程语言上的泛化能力。</li>
<li>另一威胁是LPR在不同LLMs上的适用性。实验也在CodeLlama上进行了评估，结果与ChatGPT相似，表明了不同LLMs的适用性。</li>
<li><img src="/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/image-20241010205127581.png" alt="image-20241010205127581"></li>
</ul>
</li>
</ol>
<p><strong>总结</strong>：通过多方面的缓解策略，数据泄露和泛化能力的威胁得以减少，确保了LPR的有效性。</p>
<h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a><strong>RELATED WORK</strong></h2><h3 id="6-1-Program-Reduction"><a href="#6-1-Program-Reduction" class="headerlink" title="6.1 Program Reduction"></a><strong>6.1 Program Reduction</strong></h3><p><strong>程序简化的历史背景</strong>：</p>
<ul>
<li>DDMIN引入了程序简化研究，通过递归分割输入列表，逐步减少其规模。随后，HDD（层次化的增量调试）通过将输入解析为语法树并在每一层执行DDMIN进一步提升了简化效果。</li>
<li>Perses通过形式语法转换避免生成语法无效的程序变体，而Vulcan则进一步通过替换标识符&#x2F;子树和局部穷举搜索来推动简化。</li>
<li>RCC采用缓存刷新机制加快简化进程，但这些工具不针对特定语言进行优化，缺乏语言语义知识。</li>
</ul>
<p><strong>语言特定工具</strong>：</p>
<ul>
<li>C-Reduce是针对C语言的最有效简化工具，结合了多种转换方法。</li>
<li>J-Reduce则是用于Java字节码的简化工具，ddSMT针对SMT-LIBv2格式的程序进行简化，这些工具都利用了语言特定特性来进行有效的简化。</li>
</ul>
<p><strong>LPR的独特性</strong>：</p>
<ul>
<li>LPR将LLMs与语言通用简化工具结合起来，充分利用了两者的优势。语言通用简化工具因其在多语言间的广泛适用性而突出，而LLMs利用了从大规模训练集学习的语言领域知识，进一步简化程序。</li>
<li>与需要大量手动设计和实现的语言特定工具相比，LPR通过自然语言提示大幅降低了简化过程中的人力成本。</li>
</ul>
<h3 id="6-2-LLMs-for-Software-Engineering"><a href="#6-2-LLMs-for-Software-Engineering" class="headerlink" title="6.2 LLMs for Software Engineering"></a><strong>6.2 LLMs for Software Engineering</strong></h3><ol>
<li><strong>LLMs在软件工程中的应用</strong>：<ul>
<li>大语言模型（LLMs）展示了在处理文本相关任务中的显著能力，包括源代码相关的任务。近期的研究着重于应用LLMs来支持软件工程任务，评估LLMs在软件开发和维护中的有效性、潜力和局限性。</li>
</ul>
</li>
<li><strong>自动程序修复（APR）</strong>：<ul>
<li>许多研究集中于通过实验证明LLMs在自动程序修复中的作用，这些研究展示了LLMs在软件开发任务中的广泛应用。</li>
</ul>
</li>
<li><strong>LPR方法的独特性</strong>：<ul>
<li>与传统研究不同，LPR方法利用LLMs对随机、无特定目标的程序进行简化，而其他研究通常关注逻辑明确、有特定目标的程序。因此，LPR提供了对LLMs处理无明显目标程序表现的新见解。</li>
</ul>
</li>
</ol>
<p>总结来说，LLMs在软件工程领域展现了广泛的应用潜力，尤其是在代码生成、修复和简化方面，相关研究也在持续拓展LLMs的使用场景。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Lin Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/">http://example.com/2024/10/10/LPR-Large-Language-Models-Aided-Program-Reduction/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LinLi's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/12/Domain-Adaptation-for-Code-Model-Based-Unit-Test-Case-Generation/" title="Domain Adaptation for Code Model-Based Unit Test Case Generation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Domain Adaptation for Code Model-Based Unit Test Case Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/04/Understanding-and-Detecting-SQL-Function-Bugs/" title="Understanding and Detecting SQL Function Bugs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Understanding and Detecting SQL Function Bugs</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/04/PLUMBER/" title="PLUMBER"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-04</div><div class="title">PLUMBER</div></div></a></div><div><a href="/2023/03/08/%E9%80%9A%E8%BF%87NPM%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BE%9D%E8%B5%96%E6%A0%91%E6%8F%AD%E5%BC%80%E8%84%86%E5%BC%B1%E6%80%A7%E4%BC%A0%E6%92%AD%E5%8F%8A%E5%85%B6%E6%BC%94%E5%8C%96%E7%9A%84%E7%A5%9E%E7%A7%98%E9%9D%A2%E7%BA%B1/" title="通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-08</div><div class="title">通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱</div></div></a></div><div><a href="/2023/04/03/Flexible-and-Optimal-Dependency-Management-via-Max-SMT/" title="Flexible and Optimal Dependency Management via Max-SMT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-03</div><div class="title">Flexible and Optimal Dependency Management via Max-SMT</div></div></a></div><div><a href="/2023/04/11/What-the-Fork-Finding-Hidden-Code-Clones-in-npm/" title="What the Fork Finding Hidden Code Clones in npm"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">What the Fork Finding Hidden Code Clones in npm</div></div></a></div><div><a href="/2023/04/12/Static-Type-Inference-for-Foreign-Functions-of-Python/" title="Static Type Inference for Foreign Functions of Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="title">Static Type Inference for Foreign Functions of Python</div></div></a></div><div><a href="/2023/04/13/Where-to-Start-Studying-Type-Annotation-Practices-in-Python/" title="Where to Start Studying Type Annotation Practices in Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-13</div><div class="title">Where to Start Studying Type Annotation Practices in Python</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lin Li</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LPR-Large-Language-Models-Aided-Program-Reduction"><span class="toc-number">1.</span> <span class="toc-text">LPR: Large Language Models-Aided Program Reduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#INTRODUCTION"><span class="toc-number">1.1.</span> <span class="toc-text">INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BACKGROUND"><span class="toc-number">1.2.</span> <span class="toc-text">BACKGROUND</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Program-Reduction"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 Program Reduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Large-Language-Models"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 Large Language Models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#APPROACH"><span class="toc-number">1.3.</span> <span class="toc-text">APPROACH</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Motivation"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Workflow"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 Workflow</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Transformations"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 Transformations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Multi-level-Prompts"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 Multi-level Prompts</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EVALUATION"><span class="toc-number">1.4.</span> <span class="toc-text">EVALUATION</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Experimental-Setup"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 Experimental Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-RQ1-Reduction-Effectiveness"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2  RQ1: Reduction Effectiveness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-RQ2-Reduction-Efficiency"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 RQ2: Reduction Efficiency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-RQ3-Effectiveness-of-Each-Transformation"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 RQ3: Effectiveness of Each Transformation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DISCUSSION"><span class="toc-number">1.5.</span> <span class="toc-text">DISCUSSION</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-The-Effectiveness-of-Multi-level-Prompt"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 The Effectiveness of Multi-level Prompt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-The-Impact-of-Temperature"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 The Impact of Temperature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Failures-in-LPR"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 Failures in LPR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Threats-to-Validity"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 Threats to Validity</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RELATED-WORK"><span class="toc-number">1.6.</span> <span class="toc-text">RELATED WORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Program-Reduction"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 Program Reduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-LLMs-for-Software-Engineering"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 LLMs for Software Engineering</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/18/AGENTLESS-Demystifying-LLM-based-Software-Engineering-Agents/" title="AGENTLESS: Demystifying LLM-based Software Engineering Agents">AGENTLESS: Demystifying LLM-based Software Engineering Agents</a><time datetime="2025-02-18T12:16:29.000Z" title="发表于 2025-02-18 20:16:29">2025-02-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning/" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a><time datetime="2025-02-01T11:11:48.000Z" title="发表于 2025-02-01 19:11:48">2025-02-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-Coder-When-the-Large-Language-Model-Meets-Programming-The-Rise-of-Code-Intelligence/" title="DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence">DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence</a><time datetime="2025-02-01T02:43:19.000Z" title="发表于 2025-02-01 10:43:19">2025-02-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/" title="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation">LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</a><time datetime="2025-01-30T12:14:09.000Z" title="发表于 2025-01-30 20:14:09">2025-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/24/Benchmarking-Bias-in-Large-Language-Models-during-Role-Playing/" title="Benchmarking Bias in Large Language Models during Role-Playing">Benchmarking Bias in Large Language Models during Role-Playing</a><time datetime="2025-01-24T14:55:55.000Z" title="发表于 2025-01-24 22:55:55">2025-01-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Lin Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>