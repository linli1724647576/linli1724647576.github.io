<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>METAL Metamorphic Testing Framework for Analyzing Large-Language Model Qualities | LinLi's Blog</title><meta name="author" content="Lin Li"><meta name="copyright" content="Lin Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="METAL: Metamorphic Testing Framework for Analyzing Large-Language Model QualitiesAbstract大型语言模型（LLMs）已经改变了自然语言数据处理的范式。然而，其黑箱性和概率特性可能导致多种LLM应用中输出质量的潜在风险。最近的研究通过生成对抗输入文本，测试了LLMs的质量属性（QAs），例如稳健性或公平性。然而，现">
<meta property="og:type" content="article">
<meta property="og:title" content="METAL Metamorphic Testing Framework for Analyzing Large-Language Model Qualities">
<meta property="og:url" content="http://example.com/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/index.html">
<meta property="og:site_name" content="LinLi&#39;s Blog">
<meta property="og:description" content="METAL: Metamorphic Testing Framework for Analyzing Large-Language Model QualitiesAbstract大型语言模型（LLMs）已经改变了自然语言数据处理的范式。然而，其黑箱性和概率特性可能导致多种LLM应用中输出质量的潜在风险。最近的研究通过生成对抗输入文本，测试了LLMs的质量属性（QAs），例如稳健性或公平性。然而，现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2024-11-19T02:47:44.000Z">
<meta property="article:modified_time" content="2024-11-19T02:48:34.776Z">
<meta property="article:author" content="Lin Li">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'METAL Metamorphic Testing Framework for Analyzing Large-Language Model Qualities',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-19 10:48:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">122</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="LinLi's Blog"><span class="site-name">LinLi's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">METAL Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-19T02:47:44.000Z" title="发表于 2024-11-19 10:47:44">2024-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-19T02:48:34.776Z" title="更新于 2024-11-19 10:48:34">2024-11-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="METAL Metamorphic Testing Framework for Analyzing Large-Language Model Qualities"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities"><a href="#METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities" class="headerlink" title="METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities"></a>METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>大型语言模型（LLMs）已经改变了自然语言数据处理的范式。然而，其黑箱性和概率特性可能导致多种LLM应用中输出质量的潜在风险。最近的研究通过生成对抗输入文本，测试了LLMs的质量属性（QAs），例如稳健性或公平性。然而，现有研究在覆盖LLMs的QAs和任务方面存在局限性，并且难以扩展。此外，这些研究通常仅使用一个评估指标——攻击成功率（ASR）来评估其方法的有效性。为了解决这些问题，我们提出了一种用于分析LLMs的变形测试（METAL）框架，通过应用变形测试（MT）技术来系统化地测试LLMs的质量。</p>
<p>这种方法通过定义变形关系（MRs），为评估提供模块化的指标，从而促进LLMs质量的系统测试。METAL框架可以从覆盖各种QAs和任务的模板中自动生成数百个MRs。此外，我们引入了结合ASR方法的新颖指标，将其整合到文本的语义质量中，以准确评估MRs的有效性。通过对三个主流LLMs的实验，我们证实了METAL框架能够有效评估LLMs在主要任务上的重要QAs，并揭示LLMs的质量风险。此外，提出的新指标能够指导每项任务的最优MRs生成方式，并为生成MRs的最有效方法提供建议。</p>
<p><strong>关键词</strong>：大型语言模型，变形测试，质量属性，文本扰动</p>
<h2 id="INTRODUCTDION"><a href="#INTRODUCTDION" class="headerlink" title="INTRODUCTDION"></a>INTRODUCTDION</h2><p><font color="red">Background</font></p>
<ul>
<li><p>大型语言模型（LLMs）改变了自然语言数据检索和分析的格局。</p>
</li>
<li><p>工业界已推出多个LLMs，如Google PaLM、ChatGPT、Llama2等。</p>
</li>
<li><p>LLMs被广泛应用于多种领域，需关注其潜在风险对输出质量的影响。</p>
</li>
<li><p>高扩展性、黑箱特性、概率性导致无法保证意外场景下的高质量输出（如稳健性、公平性）。</p>
</li>
<li><p>近期研究通过生成对抗性输入和提示测试LLMs的质量属性（如稳健性）。</p>
</li>
<li><p>对抗性输入可能导致LLMs输出结果的显著差异。</p>
</li>
</ul>
<p><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241118184635602.png" alt="image-20241118184635602"></p>
<p><font color="red">Motivation and limitations</font></p>
<p><strong>对抗性输入测试的分析</strong>：</p>
<ul>
<li>提供了两种对抗性输入来评估LLMs的稳健性：<ul>
<li>示例1（Fig. 1(a)）：通过字符交换的输入，情感分析任务中LLMs输出稳定。</li>
<li>示例2（Fig. 1(b)）：通过同义词替换的输入，毒性检测任务中LLMs的稳健性受到影响。</li>
</ul>
</li>
</ul>
<p>**现有测试方法的局限性 [4-13] **：</p>
<ul>
<li>仅覆盖少量关键质量属性（QAs）。</li>
<li>测试范围局限于特定任务，难以扩展至多样化的应用场景。</li>
<li>主要依赖攻击成功率（ASR）作为评估指标。</li>
</ul>
<p>[5] B. Liu, B. Xiao, X. Jiang, S. Cen, X. He, W. Dou et al., “Adversarial attacks on large language model-based system and mitigating strategies: A case study on chatgpt,” Security and Communication Networks, vol. 2023, 2023.</p>
<p>[9] F. Perez and I. Ribeiro, “Ignore previous prompt: Attack techniques for language models,” arXiv preprint arXiv:2211.09527, 2022.</p>
<p>[12] X. Huang, W. Ruan, W. Huang, G. Jin, Y. Dong, C. Wu, S. Bensalem, R. Mu, Y. Qi, X. Zhao et al., “A survey of safety and trustworthiness of large language models through the lens of verification and validation,” arXiv preprint arXiv:2305.11391, 2023.</p>
<p>[13]  J. Wang, Z. Liu, K. H. Park, M. Chen, and C. Xiao, “Adversarial demonstration attacks on large language models,” arXiv preprint arXiv:2305.14950, 2023. </p>
<p><font color="red">Initution</font></p>
<p><strong>变形测试（MT）的引入</strong>：</p>
<ul>
<li>通过定义变形关系（MRs），对LLMs的QAs进行系统化测试。</li>
<li>变形关系的示例包括：<ul>
<li>MR1：原始输入与字符交换输入的输出应一致。</li>
<li>MR2：原始输入与同义词替换输入的输出应一致。</li>
</ul>
</li>
</ul>
<p><strong>变形关系的定义与优势</strong>：</p>
<ul>
<li>定义由目标模型、输入文本、提示、关系运算符（如&#x3D;、≠）和扰动函数（如字符交换）组成。</li>
<li>优势：<ul>
<li>无需依赖有限的测试数据库，也无需标注数据。</li>
<li>高扩展性，可适用于LLMs的多种任务。</li>
<li>作为模块化工具，评估LLMs的QAs和任务表现。</li>
</ul>
</li>
</ul>
<p><font color="red">Approach</font></p>
<p>构建用于分析LLMs的变形测试框架（METAL），以解决现有测试方法的局限性。</p>
<p><strong>框架设计</strong>：</p>
<ul>
<li>定义覆盖主要质量属性（如稳健性、公平性、非确定性和效率）的变形关系（MR）模板。</li>
<li>基于模板开发自动化MR生成流程，并引入多种文本扰动类型。</li>
<li>利用LLMs及框架功能，从自检和交叉检查角度优化MR生成流程。</li>
<li><font color="green">提出结合ASR方法的新指标，整合文本语义与结构相似性，用于精确评估MR的有效性。</font></li>
</ul>
<p><strong>研究问题</strong>：</p>
<ul>
<li><strong>RQ1</strong>：框架能否评估LLMs并揭示实现多样QAs的质量风险？</li>
<li>RQ2：哪些MR最适合评估LLMs的特定任务？<ul>
<li><strong>RQ2-1</strong>：哪个MR最有效？</li>
<li><strong>RQ2-2</strong>：哪个MR对特定任务最优化？</li>
</ul>
</li>
<li><strong>RQ3</strong>：哪种MR生成方法在LLMs的自检&#x2F;交叉检查中表现最佳？</li>
</ul>
<p><strong>实验结果</strong>：</p>
<ul>
<li>在GooglePaLM、ChatGPT和Llama2上应用框架。</li>
<li>结果显示：<ul>
<li>GooglePaLM在大多数QAs和任务上表现优于其他模型。</li>
<li>新提出的MR有效性指标能引导最优MR的选择。</li>
<li>验证了LLMs自检&#x2F;交叉检查的可行性，其中ChatGPT在MR生成中表现出高效性。</li>
</ul>
</li>
</ul>
<h2 id="RELATED-WORKS"><a href="#RELATED-WORKS" class="headerlink" title="RELATED WORKS"></a>RELATED WORKS</h2><h4 id="Adversarial-test-datasets"><a href="#Adversarial-test-datasets" class="headerlink" title="Adversarial test datasets."></a><strong>Adversarial test datasets.</strong></h4><p><strong>AdvGLUE</strong> [16]：</p>
<ul>
<li>用于评估NLP模型在多个任务上的准确性（Accuracy）和稳健性（Robustness），包括情感分析和自然语言推理（NLI）。</li>
<li>数据集包含人为设计的对抗性样本（如拼写错误或关键字压力测试）。</li>
</ul>
<p><strong>AdversarialSQuAD</strong> [17]：</p>
<ul>
<li>从SQuAD扩展而来，通过在上下文段落末尾添加干扰句子设计的对抗性数据集。</li>
<li>专注于问答（Q&amp;A）任务的攻击性能评估。</li>
</ul>
<p><strong>ANLI</strong> [18]：</p>
<ul>
<li>提供人工精心设计的复杂句子，用于评估NLI任务的对抗性性能。</li>
</ul>
<p><strong>SQuAD 2.0</strong> [20]：</p>
<ul>
<li>包含不可回答的问题，这些问题与SQuAD 1.0中的可回答问题相似。</li>
<li>适用于问答任务的对抗性评估。</li>
</ul>
<p><strong>SNLI</strong> [19] 和 <strong>HELLASWAG</strong> [21]：</p>
<ul>
<li>提供用于NLI和Q&amp;A任务的准确性测试数据。</li>
</ul>
<p><strong>发现和局限性</strong>：</p>
<ul>
<li>数据集仅覆盖LLMs应用场景中的有限任务。</li>
<li>大多数数据集只关注特定功能需求（如准确性），仅AdvGLUE评估了稳健性。</li>
<li>数据集的测试案例数量有限，主要由于分析和标注成本高，难以适应LLMs在不同领域中的广泛使用需求。</li>
</ul>
<h4 id="Adversarial-attack-generators"><a href="#Adversarial-attack-generators" class="headerlink" title="Adversarial attack generators."></a><strong>Adversarial attack generators.</strong></h4><p><strong>对抗攻击生成器的研究</strong>：</p>
<ul>
<li>为解决有限测试数据集的多样性和可迁移性问题，提出了基于预训练NLP模型的对抗攻击生成器 [4-13]。</li>
<li>攻击生成器将原始文本转换为扰动文本，用于验证LLMs任务的稳健性（Robustness）。</li>
</ul>
<p><strong>具体研究工作</strong>：</p>
<ul>
<li><strong>Liu et al.</strong> [6]：提出一个预训练模型，通过追加嵌入空间生成扰动文本。</li>
<li><strong>Jin et al.</strong> [7] 和 <strong>Chiang et al.</strong> [11]：提出同义词替换攻击模型，生成语义相同但句法不同的句子。</li>
<li><strong>Wang et al.</strong> [8]：为情感分析任务提供了语义扰动功能。</li>
</ul>
<p><strong>生成修改提示的研究</strong>：</p>
<ul>
<li><strong>Guo et al.</strong> [22]：提出数据完整性和隐私攻击方法，贯穿预训练NLP模型的构建和微调流程。</li>
<li><strong>Perez et al.</strong> [9] 和 <strong>Zhou et al.</strong> [25]：提出了目标劫持和提示泄露攻击方法，通过添加特定命令和后缀进行攻击。</li>
<li><strong>Li et al.</strong> [23] 和 <strong>Shen et al.</strong> [24]：提出越狱攻击方法，诱导隐私和安全泄露。</li>
</ul>
<p><strong>现有攻击模型的局限性</strong>：</p>
<ul>
<li>仅生成特定类型的文本扰动，扩展性较低，难以覆盖各种质量属性（QAs）和任务。</li>
<li>提示攻击需要分析整个ML流水线，以缓解安全和隐私问题。</li>
<li>研究仅基于攻击成功率（ASR）评估方法，未考虑扰动的质量。</li>
</ul>
<h4 id="QA-analysis-on-NLP-models"><a href="#QA-analysis-on-NLP-models" class="headerlink" title="QA analysis on NLP models."></a><strong>QA analysis on NLP models.</strong></h4><p>**HELM框架 [27]**：</p>
<ul>
<li>由Liang等人提出，用于综合评估NLP模型的质量属性（QAs）。</li>
<li>提供了多样的测试数据集和评估指标，但主要是对现有基准测试的概览，而非可执行的评估框架。</li>
</ul>
<p>**对抗性攻击的研究 [28]**：</p>
<ul>
<li>Qiu等人研究了多种对抗攻击，分类为四个级别，并分析了相应的对抗训练方法和防御策略。</li>
<li>重点在于提出对抗攻击的替代防御策略，而非评估LLMs的QAs。</li>
</ul>
<p>**基于变形测试的垃圾检测评估 [29]**：</p>
<ul>
<li>Wang等人提出了一个基于变形测试（MT）的评估框架，专注于文本垃圾检测。</li>
<li>分类了11种扰动类型，并定义了变形关系（MRs），通过真实垃圾文本（英文和中文）的分析评估LLMs在垃圾检测任务中的稳健性（Robustness）。</li>
</ul>
<h4 id="METAL-framwork"><a href="#METAL-framwork" class="headerlink" title="METAL framwork"></a>METAL framwork</h4><p><strong>我们的METAL框架的特点</strong>：</p>
<ul>
<li>提供一次性评估，覆盖四个主要QAs和六个任务。</li>
<li>支持从无限制来源输入文本，采用MT技术。</li>
<li>系统定义了五个MR模板，涵盖主要QAs和任务。</li>
<li>开发了自动MR生成流程，支持13种扰动类型的输入文本生成，并执行模块化评估。</li>
<li>结合ASR和文本相似度度量，精准评估MR的有效性。</li>
</ul>
<h2 id="APPROACH-METAL-FRAMEWORK"><a href="#APPROACH-METAL-FRAMEWORK" class="headerlink" title="APPROACH: METAL FRAMEWORK"></a>APPROACH: METAL FRAMEWORK</h2><h3 id="A-Quality-Attributes-and-Tasks-in-LLMs"><a href="#A-Quality-Attributes-and-Tasks-in-LLMs" class="headerlink" title="A. Quality Attributes and Tasks in LLMs"></a><em>A. Quality Attributes and Tasks in LLMs</em></h3><p><strong>变形关系（MRs）的定义</strong>：</p>
<ul>
<li>MT框架需要定义一组MRs作为目标系统的评估指标和测试Oracle [14]。</li>
<li>本研究采用自上而下的方法，确定覆盖关键QAs和任务的MR模板，然后基于模板生成MRs。</li>
</ul>
<p><strong>关键质量属性（QAs）</strong>：</p>
<ul>
<li>通过对通用机器学习和生成式AI的评估研究 [15, 22, 27, 30-38]，确定以下四个关键QAs：<ul>
<li>稳健性（Robustness）</li>
<li>公平性（Fairness）</li>
<li>非确定性（Non-determinism）</li>
<li>效率（Efficiency）</li>
</ul>
</li>
<li>其他QAs如可解释性（Explainability）、安全性（Security）和隐私性（Privacy）需要外部数据或整条ML管道的分析，未包含在本研究中 [30, 34, 35]。</li>
</ul>
<p><strong>QAs的具体定义</strong>：</p>
<ul>
<li><strong>稳健性</strong>：LLMs在扰动输入下的输出应保持一致 [41]。</li>
<li><strong>公平性</strong>：不同人群生成的相同输入结果应一致 [27, 42-44]。</li>
<li><strong>非确定性</strong>：相同输入应产生一致的输出 [45-47]。</li>
<li><strong>效率</strong>：原始与扰动输入的处理时间差应低于阈值。</li>
</ul>
<p><strong>任务划分与分类</strong>：</p>
<ul>
<li>将六个任务分为两类 [27, 28, 48]：<ul>
<li><strong>分类任务</strong>：单标签分类，包括情感分析、毒性检测、新闻分类。</li>
<li><strong>生成任务</strong>：多标签任务，包括文本摘要、问答和信息检索。</li>
</ul>
</li>
</ul>
<p><strong>QAs与任务的相关性</strong>：</p>
<ul>
<li><strong>稳健性、非确定性、效率</strong>：与六种任务高度相关。</li>
<li><strong>公平性</strong>：主要与情感分析、毒性检测、问答任务相关 [27]。对新闻分类等任务的公平性分析受限于区域和数据集的定义。</li>
</ul>
<p><strong>研究重点</strong>：</p>
<ul>
<li>针对公平性问题，主要分析情感分析、毒性检测和问答任务。</li>
<li>通过MR模板，评估这些任务的QAs表现。</li>
</ul>
<h3 id="B-Metamorphic-Relation-Templates-for-LLM-Evaluation"><a href="#B-Metamorphic-Relation-Templates-for-LLM-Evaluation" class="headerlink" title="B. Metamorphic Relation Templates for LLM Evaluation"></a><em>B. Metamorphic Relation Templates for LLM Evaluation</em></h3><p><strong>变形关系模板（MRTs）的定义</strong>：</p>
<ul>
<li>MRTs系统性定义了MRs，用于评估LLMs的功能需求或质量属性（QAs）[49, 50]。</li>
<li>MRT由以下组件组成：<ul>
<li><strong>LLMs</strong>：可执行的模型或API。</li>
<li><strong>Input</strong>：输入样本文本。</li>
<li><strong>Prompt</strong>：任务相关的指令文本。</li>
<li><strong>REL_OP</strong>：关系运算符（如&#x3D;, ≠, &gt;, &lt;）。</li>
<li><strong>Perturb</strong>：对抗性扰动函数。</li>
<li><strong>Dist</strong>：计算文本间距离的函数。</li>
</ul>
</li>
</ul>
<p><strong>主要MRT类型及公式</strong>：</p>
<ul>
<li>Equivalence_MRT (公式7)：<ul>
<li>比较原始输入与扰动输入的输出是否相等，用于稳健性（Robustness）评估。</li>
<li>适用于短文本任务（如分类任务）。</li>
</ul>
</li>
<li>Discrepancy_MRT (公式8)：<ul>
<li>检查原始输入和扰动输入的输出是否不同，用于评估对语义改变的稳健性。</li>
<li>适用于生成扰动（如反义词替换、代词替换）的场景。</li>
</ul>
</li>
<li>Set_Equivalence_MRT (公式9)：<ul>
<li>评估公平性（Fairness）和非确定性（Non-determinism），比较原始输入与一组扰动生成的输出集合。</li>
<li>可用于验证相同输入是否对不同群体（如性别、地区）输出一致结果。</li>
</ul>
</li>
<li>Distance_MRT (公式10)：<ul>
<li>通过距离函数量化原始与扰动输入输出的差异，用于稳健性和生成任务的效率（Efficiency）评估。</li>
</ul>
</li>
<li>Set_Distance_MRT (公式11)：<ul>
<li>检查生成任务的非确定性，通过多次运行相同输入结果的输出一致性。</li>
</ul>
</li>
</ul>
<p><strong>模板适用性</strong>：</p>
<ul>
<li>MRTs提供全面的QAs和任务覆盖，结合公式(7)-(11)，首次正式定义了评估LLMs质量属性的方法。</li>
<li>可用于分类和生成任务，确保LLMs的稳健性、公平性、非确定性和效率的评估。</li>
</ul>
<h3 id="C-Generating-Metamorphic-Relations-using-Templates"><a href="#C-Generating-Metamorphic-Relations-using-Templates" class="headerlink" title="C. Generating Metamorphic Relations using Templates"></a><em>C. Generating Metamorphic Relations using Templates</em></h3><p><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241118192929548.png" alt="image-20241118192929548"></p>
<p><strong>MR模板与扰动函数的生成</strong>：</p>
<ul>
<li>MR模板根据评估目标、质量属性（QAs）和目标任务定义评估范围。</li>
<li>通过指定扰动函数（如<code>P ∈ Perturb</code>），自动生成继承自模板的变形关系（MR）。</li>
</ul>
<p>**13种扰动函数分类 [26-28]**：</p>
<ul>
<li><strong>字符级扰动</strong>（7种）：包括替换、删除、添加、交换、打乱字符顺序，以及将文本转换为l33t格式（如“apple” → “@ppl3”）和随机插入空格。</li>
<li><strong>词和句子级扰动</strong>（5种）：包括语义保留与改变，例如反义词替换（语义改变）和随机替换虚拟句子（语义改变）。</li>
<li><strong>语句级分组</strong>：通过添加背景信息句子（如“此文本由[性别]&#x2F;[年龄]&#x2F;[种族]&#x2F;[取向]群体提供”），为公平性（Fairness）评估提供支持。包含21种分组选项（3性别、3年龄、10种族、5取向）。</li>
</ul>
<p><strong>扰动函数生成与LLMs结合</strong>：</p>
<ul>
<li>使用LLMs生成扰动文本，针对自检和交叉检验需求，设计特定提示（如随机替换同义词）。</li>
<li>在实验中，发现部分生成文本低质或不符合语义逻辑，例如无意义的替换或上下文不符的结果。</li>
</ul>
<p><strong>解决低质量问题</strong>：</p>
<ul>
<li>为文本扰动设计新的质量指标，确保生成文本在语义和上下文上的适用性（详见Section IV-B）。</li>
<li>提出改进的方法，用于提高扰动生成的合理性和准确性。</li>
</ul>
<h3 id="D-Framework-Implementation"><a href="#D-Framework-Implementation" class="headerlink" title="D. Framework Implementation"></a><em>D. Framework Implementation</em></h3><p><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241118193759696.png" alt="image-20241118193759696"></p>
<p><strong>METAL框架的目标</strong>：</p>
<ul>
<li>用于评估LLMs在六种任务中四个关键质量属性（QAs）的表现。</li>
</ul>
<p><strong>框架模块</strong>：</p>
<ul>
<li>执行模块（Execution Module）：<ul>
<li>包含扰动生成器，用于根据函数和LLM提示生成扰动文本。</li>
<li>包含LLM日志生成器，记录原始和扰动输入文本及其生成的输出。</li>
<li>输出包含输入ID、文本、扰动ID和相应输出。</li>
</ul>
</li>
<li>评估模块（Evaluation Module）：<ul>
<li>使用从MRTs生成的MRs验证执行结果。</li>
<li>使用二值结果表示评估结果：1表示MR满足，0表示不满足。</li>
<li>每个原始输入应用10种扰动，每种扰动产生一列结果。</li>
</ul>
</li>
</ul>
<p><strong>框架的优势</strong>：</p>
<ul>
<li>支持从多种来源获取的输入数据，无需标注，减少分析成本。</li>
<li>提供基于API的动态LLM评估功能，适用于不同LLMs。</li>
<li>提供了详细的手册和框架说明，托管在GitHub上供进一步研究。</li>
</ul>
<p><strong>具体实现与输出</strong>：</p>
<ul>
<li>通过Fig. 2的实例，展示了框架结构和示例输出。</li>
<li>包括手动和自动的评估示例，支持具体的MR基准测试。</li>
</ul>
<h2 id="EXPERIMENT"><a href="#EXPERIMENT" class="headerlink" title="EXPERIMENT"></a>EXPERIMENT</h2><h3 id="A-Experiment-Design"><a href="#A-Experiment-Design" class="headerlink" title="A. Experiment Design"></a><em>A. Experiment Design</em></h3><p><strong>实验目标</strong>：</p>
<ul>
<li>在LLMs（Google、OpenAI、Meta）上评估四个关键质量属性（QAs）与六个任务的21种组合。</li>
<li>任务包括毒性检测（TD）、情感分析（SA）、新闻分类（NC）、问答（Q&amp;A）、文本摘要（TS）和信息检索（IR）。</li>
</ul>
<p><strong>MR与扰动设置</strong>：</p>
<ul>
<li>总计生成273个变形关系（MRs），其中：<ul>
<li>240个MR用于稳健性（Robustness）评估，每个任务对应10种扰动（字符级、词句级等）。</li>
<li>21个MR用于公平性（Fairness），基于21个人口学分组选项。</li>
<li>6个MR用于非确定性（Non-determinism）和效率（Efficiency）评估。</li>
</ul>
</li>
<li>任务文本特点：<ul>
<li>分类和问答任务：通常由1-2个句子组成，难以删除或替换。</li>
<li>其他任务：包含10-20个句子，字符级扰动影响有限。</li>
</ul>
</li>
</ul>
<p><strong>输入数据</strong>：</p>
<ul>
<li>样本总数为900，来源于多种网络资源（如Amazon评论、新闻文章等）。</li>
<li>输入文本长度从15到4000字不等。</li>
</ul>
<p><strong>实验规模</strong>：</p>
<ul>
<li>每个LLM约接受42,000次请求，总计处理19,150,000个标记。</li>
<li>包括LLM的五次重复执行，详细内容见Section IV-C。</li>
</ul>
<h3 id="RQ1-Quality-evaluation-results-on-LLMs"><a href="#RQ1-Quality-evaluation-results-on-LLMs" class="headerlink" title="RQ1. Quality evaluation results on LLMs."></a><strong>RQ1. Quality evaluation results on LLMs.</strong></h3><p><strong>目标</strong>：</p>
<p>评估LLMs在任务中的质量属性（QAs），包括稳健性（Robustness）、公平性（Fairness）、非确定性（Non-determinism）和效率（Efficiency），并揭示潜在的质量风险。</p>
<hr>
<p><strong>方法与指标</strong>：</p>
<ol>
<li>ASR（Attack Success Rate）：<ul>
<li>通过MR不满足的比例评估稳健性，结合文本相似度指标。</li>
<li>举例：若10个MR在100次输入中有200次不满足，ASR &#x3D; 0.2。</li>
</ul>
</li>
<li>生成任务中的文本相似性：<ul>
<li>使用Google的Universal Sentence Encoder（USE）计算Q&amp;A任务的语义相似度（STS）。</li>
<li>为文本摘要任务定义A-STS（平均语义相似度），阈值设为0.6。</li>
</ul>
</li>
<li>IR任务的排名距离：<ul>
<li>定义最大STS排名距离（MSRD），通过扩展Kendall tau距离计算平均排名差异，阈值为2。</li>
</ul>
</li>
<li>非确定性分析：<ul>
<li>基于多次输入相同文本的输出差异，计算1-STS的平均值。</li>
</ul>
</li>
</ol>
<hr>
<p><strong>结果分析</strong>：</p>
<ol>
<li>稳健性（Robustness）：<ul>
<li>分类任务中LLMs的ASR值较低，生成任务中ASR值较高。</li>
<li>GooglePaLM和ChatGPT的ASR值接近，而Llama2的ASR值最高（最易受扰动影响）。</li>
<li>在文本摘要任务中，Llama2的ASR达0.99，表明大多数扰动导致输出显著不同。</li>
<li><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119102830986.png" alt="image-20241119102830986"></li>
</ul>
</li>
<li>公平性（Fairness）：<ul>
<li>GooglePaLM在毒性检测任务中ASR最低，而ChatGPT在情感分析中ASR最低。</li>
<li>问答任务中，三个模型的输出相似性较高，平均ASR约为0.9。</li>
<li><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119102840147.png" alt="image-20241119102840147"></li>
</ul>
</li>
<li>非确定性（Non-determinism）：<ul>
<li>GooglePaLM的输出差异在所有任务中较小，但在信息检索（IR）任务中例外。</li>
<li>Llama2的输出波动最大，频繁返回推荐文本或手动提示，解释了其在稳健性和IR任务中的表现。</li>
<li><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119102849428.png" alt="image-20241119102849428"></li>
</ul>
</li>
<li>效率（Efficiency）：<ul>
<li>GooglePaLM表现最稳定，输入扰动对推理时间的影响最小。</li>
<li>ChatGPT在生成任务中推理时间波动较大。</li>
<li>Llama2的推理时间差异范围更大，从-2500秒到2000秒不等。</li>
<li><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119102816342.png" alt="image-20241119102816342"></li>
</ul>
</li>
</ol>
<hr>
<p><strong>总结</strong>：</p>
<ul>
<li>GooglePaLM在稳健性和效率上表现出色。</li>
<li>ChatGPT在公平性和生成任务中较为均衡。</li>
<li>Llama2在多个质量属性上表现不稳定，尤其在IR和生成任务中易受扰动影响。</li>
</ul>
<p><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119101246952.png" alt="image-20241119101246952"></p>
<p><img src="/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/image-20241119101302720.png" alt="image-20241119101302720"></p>
<h3 id="RQ2-The-effectiveness-of-MRs"><a href="#RQ2-The-effectiveness-of-MRs" class="headerlink" title="RQ2. The effectiveness of MRs."></a><strong>RQ2. The effectiveness of MRs.</strong></h3><p><strong>目标：</strong></p>
<p>评估和优化每个任务中变形关系（MR）的有效性，衡量MR在评估LLMs稳健性（Robustness）中的表现，并确定每个任务的最优MR。</p>
<hr>
<p><strong>方法与指标：</strong></p>
<ol>
<li><strong>EFM（Effectiveness of MRs）公式</strong>：<ul>
<li>定义为： EFM&#x3D;M-ASR×PerturbQualityEFM &#x3D; M\text{-}ASR \times PerturbQualityEFM&#x3D;M-ASR×PerturbQuality</li>
<li><strong>M-ASR</strong>：MR的不满足率（即ASR），表示模型对扰动的脆弱性。</li>
<li><strong>PerturbQuality</strong>：通过A-STS度量扰动文本的语义质量，确保扰动在保留上下文一致性的同时具有攻击性。</li>
<li>结合两者，EFM反映了MR在保证扰动质量的前提下对LLMs有效性的综合指标。</li>
</ul>
</li>
<li><strong>PerturbQuality计算（算法1）</strong>：<ul>
<li>通过比较原始和扰动文本的上下文相似度（ContextSim），计算文本的语义质量。</li>
<li>阈值设定为0.98，高于此值则视为未扰动。</li>
<li>若被扰动，则考虑句子的语义相似度和长度比例。</li>
</ul>
</li>
<li><strong>Shapley Value分析</strong>：<ul>
<li>用于确定每个MR在组合中的边际贡献。</li>
<li>通过多个MR组合的EFM差异，计算单个MR的贡献值。</li>
</ul>
</li>
</ol>
<hr>
<p><strong>实验结果：</strong></p>
<ol>
<li>MR有效性分析（EFM）：<ul>
<li>高效的MR类型因任务而异：<ul>
<li><strong>毒性检测（TD）、新闻分类（NC）、文本摘要（TS）</strong>：字符级扰动（如ConvertToL33tFormat）最有效。</li>
<li><strong>情感分析（SA）</strong>：字符级扰动（ShuffleCharacter和SwapCharacter）最有效。</li>
<li><strong>问答（Q&amp;A）和信息检索（IR）</strong>：词级扰动（ReplaceSynonym和AddRandomWord）更有效。</li>
<li><strong>句子级扰动</strong>（如ReplaceRandomSentence）：在IR任务中表现出高效性。</li>
</ul>
</li>
</ul>
</li>
<li>优化MR分析（RQ2-2）：<ul>
<li>字符级扰动在分类任务中表现最佳。</li>
<li>词级扰动在生成任务中更具优势。</li>
<li>Shapley Value分析揭示，字符和词级扰动在所有任务中的有效性高于句子级扰动。</li>
</ul>
</li>
</ol>
<hr>
<p><strong>总结：</strong></p>
<ul>
<li>字符级和词级扰动在稳健性评估中表现出高EFM值，适用于不同任务。</li>
<li>MR的优化需要根据任务类型（分类任务或生成任务）调整扰动级别。</li>
</ul>
<h3 id="RQ3-Self-and-Cross-examination-of-LLMs"><a href="#RQ3-Self-and-Cross-examination-of-LLMs" class="headerlink" title="RQ3. Self and Cross-examination of LLMs."></a><strong>RQ3. Self and Cross-examination of LLMs.</strong></h3><p><strong>目标：</strong></p>
<p>验证变形关系（MR）在LLMs的自检（self-examination）和交叉检验（cross-examination）中的有效性，并找出最适合评估特定LLM质量的MR生成方法。</p>
<hr>
<p><strong>方法：</strong></p>
<ul>
<li>对比不同LLMs目标下的平均MR有效性（EFM），分析各生成方法在不同任务中的表现。</li>
<li>使用表格数据（Table V）呈现平均EFM值，结合特定生成方法（如CreatedFunctions、GooglePaLM等）。</li>
</ul>
<hr>
<p><strong>结果：</strong></p>
<ol>
<li><strong>对GooglePaLM的分析</strong>：<ul>
<li>除GooglePaLM方法本身外，其他MR生成方法（如CreatedFunctions和ChatGPT）表现出相似的EFM结果。</li>
<li>GooglePaLM的平均EFM值最低，仅为0.09。</li>
</ul>
</li>
<li><strong>对ChatGPT的分析</strong>：<ul>
<li>CreatedFunctions方法生成的MR在ChatGPT上表现良好，EFM为0.22。</li>
<li>ChatGPT方法生成的MR效果更佳，EFM达0.28，是对ChatGPT最有效的MR生成方法。</li>
</ul>
</li>
<li><strong>对Llama2的分析</strong>：<ul>
<li>GooglePaLM和ChatGPT方法生成的MR表现更优，EFM值分别为0.35和0.30。</li>
<li>CreatedFunctions方法的EFM值为0.26，相对稍弱。</li>
</ul>
</li>
<li><strong>对所有LLMs的总体观察</strong>：<ul>
<li>ChatGPT方法在所有目标LLMs中生成了更高效的MR，表现最为稳定和有效。</li>
</ul>
</li>
</ol>
<hr>
<p><strong>结论：</strong></p>
<ul>
<li>不同LLMs适合不同的MR生成方法，需根据具体LLM选择最佳方法。</li>
<li>ChatGPT方法在生成高效MR上表现最为突出，适用于所有目标LLMs，尤其在稳健性（Robustness）评估中效果显著。</li>
</ul>
<h3 id="C-Threats-to-Validity"><a href="#C-Threats-to-Validity" class="headerlink" title="C. Threats to Validity"></a><em>C. Threats to Validity</em></h3><p><strong>1. 内部有效性（Internal Validity）：</strong></p>
<ul>
<li><strong>问题</strong>：LLMs可能对上下文具有记忆能力，影响扰动文本和原始文本的独立性。</li>
<li>对策：<ul>
<li>每次请求通过API重新开启新会话，避免上下文污染。</li>
<li>随机调整原始和扰动输入的顺序，避免固定顺序对模型输出的影响。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>2. 外部有效性（External Validity）：</strong></p>
<ul>
<li><strong>问题</strong>：实验中涉及的提示词和任务可能无法完全涵盖所有LLMs的真实使用场景。</li>
<li>对策：<ul>
<li>使用现有研究中的提示词工程方法，例如：<ul>
<li>文本摘要任务的提示：<code>&quot;Please summarize the given text in 5 sentences&quot;。</code></li>
<li>字符扰动任务的提示：<code>&quot;Please randomly swap characters a maximum of 3 times in each sentence&quot;。</code></li>
</ul>
</li>
<li>实验中使用的提示词已公开，以便未来扩展到更多扰动。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>3. 结论有效性（Conclusion Validity）：</strong></p>
<ul>
<li>问题：<ul>
<li>实验中模型参数和硬件资源的限制可能影响结论的推广性。</li>
<li>例如：<ul>
<li>使用的是7B模型，而非70B模型（后者需要至少30GB GPU内存）。</li>
</ul>
</li>
<li>GPT-API的收费策略限制了重复实验的次数。</li>
</ul>
</li>
<li>对策：<ul>
<li>在所有LLMs上对每个QA和任务重复5次实验，以确保结果的一致性。</li>
<li>每次测试所有输入数据以评估非确定性，执行了20次实验结果（5次重复 × 4种MR生成方法）。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>总结：</strong></p>
<p>通过控制上下文污染、使用开放的提示词策略以及合理分配实验资源，最大程度确保实验的内部、外部和结论有效性，同时认识到模型规模和资源限制带来的局限性。</p>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p><strong>1. 框架总结：</strong></p>
<ul>
<li>本研究提出了<strong>METAL框架</strong>，利用变形测试（MT）技术系统评估LLMs的质量。</li>
<li>框架特点：<ul>
<li>提供了全面的MR模板，用于涵盖LLMs的关键质量属性（QAs）和任务。</li>
<li>设计了自动化的MR生成模块。</li>
<li>引入了新的指标（如ASR结合语义和结构相似度度量）评估LLMs的输出质量。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>2. 实验结果：</strong></p>
<ul>
<li>对3种主流LLMs（GooglePaLM、ChatGPT、Llama2）验证了框架的有效性。<ul>
<li>GooglePaLM在多个任务上的表现优于其他模型。</li>
<li>新指标成功指导了MR的优先级，帮助确定每个任务中最有效的MR。</li>
<li>验证了LLMs自检和交叉检验的可行性，其中ChatGPT生成的MR效果最佳。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>3. 框架价值：</strong></p>
<ul>
<li>对ML工程师：<ul>
<li>支持使用无标签数据测试LLMs，提高测试效率。</li>
<li>揭示LLMs在极端场景下的弱点，帮助改进模型。</li>
</ul>
</li>
<li>对行业用户：<ul>
<li>作为微调LLMs的公开评估平台，适用多个领域。</li>
</ul>
</li>
<li>对小企业：<ul>
<li>帮助解决评估微调模型质量的难题。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Lin Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/">http://example.com/2024/11/19/METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LinLi's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/19/How-Effective-Are-They-Exploring-Large-Language-Model-Based-Fuzz-Driver-Generation/" title="How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/19/SMT-Solver-Validation-Empowered-by-Large-Pre-trained-Language-Models/" title="SMT Solver Validation Empowered by Large Pre-trained Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SMT Solver Validation Empowered by Large Pre-trained Language Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/04/PLUMBER/" title="PLUMBER"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-04</div><div class="title">PLUMBER</div></div></a></div><div><a href="/2023/03/08/%E9%80%9A%E8%BF%87NPM%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BE%9D%E8%B5%96%E6%A0%91%E6%8F%AD%E5%BC%80%E8%84%86%E5%BC%B1%E6%80%A7%E4%BC%A0%E6%92%AD%E5%8F%8A%E5%85%B6%E6%BC%94%E5%8C%96%E7%9A%84%E7%A5%9E%E7%A7%98%E9%9D%A2%E7%BA%B1/" title="通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-08</div><div class="title">通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱</div></div></a></div><div><a href="/2023/04/03/Flexible-and-Optimal-Dependency-Management-via-Max-SMT/" title="Flexible and Optimal Dependency Management via Max-SMT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-03</div><div class="title">Flexible and Optimal Dependency Management via Max-SMT</div></div></a></div><div><a href="/2023/04/11/What-the-Fork-Finding-Hidden-Code-Clones-in-npm/" title="What the Fork Finding Hidden Code Clones in npm"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">What the Fork Finding Hidden Code Clones in npm</div></div></a></div><div><a href="/2023/04/12/Static-Type-Inference-for-Foreign-Functions-of-Python/" title="Static Type Inference for Foreign Functions of Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="title">Static Type Inference for Foreign Functions of Python</div></div></a></div><div><a href="/2023/04/13/Where-to-Start-Studying-Type-Annotation-Practices-in-Python/" title="Where to Start Studying Type Annotation Practices in Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-13</div><div class="title">Where to Start Studying Type Annotation Practices in Python</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lin Li</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">122</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#METAL-Metamorphic-Testing-Framework-for-Analyzing-Large-Language-Model-Qualities"><span class="toc-number">1.</span> <span class="toc-text">METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#INTRODUCTDION"><span class="toc-number">1.2.</span> <span class="toc-text">INTRODUCTDION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RELATED-WORKS"><span class="toc-number">1.3.</span> <span class="toc-text">RELATED WORKS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Adversarial-test-datasets"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">Adversarial test datasets.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adversarial-attack-generators"><span class="toc-number">1.3.0.2.</span> <span class="toc-text">Adversarial attack generators.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#QA-analysis-on-NLP-models"><span class="toc-number">1.3.0.3.</span> <span class="toc-text">QA analysis on NLP models.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#METAL-framwork"><span class="toc-number">1.3.0.4.</span> <span class="toc-text">METAL framwork</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#APPROACH-METAL-FRAMEWORK"><span class="toc-number">1.4.</span> <span class="toc-text">APPROACH: METAL FRAMEWORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Quality-Attributes-and-Tasks-in-LLMs"><span class="toc-number">1.4.1.</span> <span class="toc-text">A. Quality Attributes and Tasks in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Metamorphic-Relation-Templates-for-LLM-Evaluation"><span class="toc-number">1.4.2.</span> <span class="toc-text">B. Metamorphic Relation Templates for LLM Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Generating-Metamorphic-Relations-using-Templates"><span class="toc-number">1.4.3.</span> <span class="toc-text">C. Generating Metamorphic Relations using Templates</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-Framework-Implementation"><span class="toc-number">1.4.4.</span> <span class="toc-text">D. Framework Implementation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EXPERIMENT"><span class="toc-number">1.5.</span> <span class="toc-text">EXPERIMENT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Experiment-Design"><span class="toc-number">1.5.1.</span> <span class="toc-text">A. Experiment Design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RQ1-Quality-evaluation-results-on-LLMs"><span class="toc-number">1.5.2.</span> <span class="toc-text">RQ1. Quality evaluation results on LLMs.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RQ2-The-effectiveness-of-MRs"><span class="toc-number">1.5.3.</span> <span class="toc-text">RQ2. The effectiveness of MRs.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RQ3-Self-and-Cross-examination-of-LLMs"><span class="toc-number">1.5.4.</span> <span class="toc-text">RQ3. Self and Cross-examination of LLMs.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Threats-to-Validity"><span class="toc-number">1.5.5.</span> <span class="toc-text">C. Threats to Validity</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CONCLUSION"><span class="toc-number">1.6.</span> <span class="toc-text">CONCLUSION</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/08/%E8%A7%A3%E5%86%B3Camera-ready-%E2%9C%98-Problem-Not-all-fonts-are-embedded-%E9%97%AE%E9%A2%98/" title="解决Camera ready ✘ Problem: Not all fonts are embedded. 问题">解决Camera ready ✘ Problem: Not all fonts are embedded. 问题</a><time datetime="2025-04-08T14:19:21.000Z" title="发表于 2025-04-08 22:19:21">2025-04-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/21/Sequence-Oriented-DBMS-Fuzzing/" title="Sequence-Oriented DBMS Fuzzing">Sequence-Oriented DBMS Fuzzing</a><time datetime="2025-02-21T09:35:08.000Z" title="发表于 2025-02-21 17:35:08">2025-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/18/AGENTLESS-Demystifying-LLM-based-Software-Engineering-Agents/" title="AGENTLESS: Demystifying LLM-based Software Engineering Agents">AGENTLESS: Demystifying LLM-based Software Engineering Agents</a><time datetime="2025-02-18T12:16:29.000Z" title="发表于 2025-02-18 20:16:29">2025-02-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning/" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a><time datetime="2025-02-01T11:11:48.000Z" title="发表于 2025-02-01 19:11:48">2025-02-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-Coder-When-the-Large-Language-Model-Meets-Programming-The-Rise-of-Code-Intelligence/" title="DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence">DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence</a><time datetime="2025-02-01T02:43:19.000Z" title="发表于 2025-02-01 10:43:19">2025-02-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Lin Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>