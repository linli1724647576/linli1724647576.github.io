<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>预训练语言模型的前世今生 - 从Word Embedding到BERT | LinLi's Blog</title><meta name="author" content="Lin Li"><meta name="copyright" content="Lin Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="预训练语言模型的前世今生 - 从Word Embedding到BERT一、预训练1.1 图像领域的预训练在介绍图像领域的预训练之前，我们首先介绍下卷积神经网络（CNN），CNN 一般用于图片分类任务，并且CNN 由多个层级结构组成，不同层学到的图像特征也不同，越浅的层学到的特征越通用（横竖撇捺），越深的层学到的特征和具体任务的关联性越强（人脸-人脸轮廓、汽车-汽车轮廓），如下图所示：  由此，当领">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练语言模型的前世今生 - 从Word Embedding到BERT">
<meta property="og:url" content="http://example.com/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/index.html">
<meta property="og:site_name" content="LinLi&#39;s Blog">
<meta property="og:description" content="预训练语言模型的前世今生 - 从Word Embedding到BERT一、预训练1.1 图像领域的预训练在介绍图像领域的预训练之前，我们首先介绍下卷积神经网络（CNN），CNN 一般用于图片分类任务，并且CNN 由多个层级结构组成，不同层学到的图像特征也不同，越浅的层学到的特征越通用（横竖撇捺），越深的层学到的特征和具体任务的关联性越强（人脸-人脸轮廓、汽车-汽车轮廓），如下图所示：  由此，当领">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-06-02T01:34:10.000Z">
<meta property="article:modified_time" content="2023-06-02T01:35:05.472Z">
<meta property="article:author" content="Lin Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '预训练语言模型的前世今生 - 从Word Embedding到BERT',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-02 09:35:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="LinLi's Blog"><span class="site-name">LinLi's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">预训练语言模型的前世今生 - 从Word Embedding到BERT</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-02T01:34:10.000Z" title="发表于 2023-06-02 09:34:10">2023-06-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-02T01:35:05.472Z" title="更新于 2023-06-02 09:35:05">2023-06-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="预训练语言模型的前世今生 - 从Word Embedding到BERT"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="预训练语言模型的前世今生-从Word-Embedding到BERT"><a href="#预训练语言模型的前世今生-从Word-Embedding到BERT" class="headerlink" title="预训练语言模型的前世今生 - 从Word Embedding到BERT"></a>预训练语言模型的前世今生 - 从Word Embedding到BERT</h2><h3 id="一、预训练"><a href="#一、预训练" class="headerlink" title="一、预训练"></a>一、预训练</h3><h4 id="1-1-图像领域的预训练"><a href="#1-1-图像领域的预训练" class="headerlink" title="1.1 图像领域的预训练"></a>1.1 图像领域的预训练</h4><p>在介绍图像领域的预训练之前，我们首先介绍下卷积神经网络（CNN），CNN 一般用于图片分类任务，并且CNN 由多个层级结构组成，不同层学到的图像特征也不同，<strong>越浅的层学到的特征越通用（横竖撇捺），越深的层学到的特征和具体任务的关联性越强（人脸-人脸轮廓、汽车-汽车轮廓）</strong>，如下图所示：</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526100402245.png" alt="image-20230526100402245"></p>
<p>由此，当领导给我们一个任务：阿猫、阿狗、阿虎的图片各十张，然后让我们设计一个深度神经网络，通过该网络把它们三者的图片进行分类。</p>
<p>对于上述任务，如果我们亲手设计一个深度神经网络基本是不可能的，<strong>因为深度学习一个弱项就是在训练阶段对于数据量的需求特别大</strong>，而领导只给我们合计三十张图片，显然这是不够的。</p>
<p>虽然领导给我们的数据量很少，<strong>但是我们是否可以利用网上现有的大量已做好分类标注的图片</strong>。比如 ImageNet 中有 1400 万张图片，并且这些图片都已经做好了分类标注。</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526100423090.png" alt="image-20230526100423090"></p>
<p>上述利用网络上现有图片的思想就是预训练的思想，具体做法就是：</p>
<ol>
<li>通过 ImageNet 数据集我们训练出一个模型 A</li>
<li>由于上面提到 CNN 的浅层学到的特征通用性特别强，我们可以对模型 A 做出一部分改进得到模型 B（两种方法）：<ol>
<li>冻结：浅层参数使用模型 A 的参数，高层参数随机初始化，<strong>浅层参数一直不变</strong>，然后利用领导给出的 30 张图片训练参数</li>
<li>微调：浅层参数使用模型 A 的参数，高层参数随机初始化，然后利用领导给出的 30 张图片训练参数，<strong>但是在这里浅层参数会随着任务的训练不断发生变化</strong></li>
</ol>
</li>
</ol>
<p><font color="red">通过上述的讲解，对图像预训练做个总结（可参照上图）：对于一个具有少量数据的任务 A，首先通过一个现有的大量数据搭建一个 CNN 模型 A，由于 CNN的浅层学到的特征通用性特别强，因此在搭建一个 CNN 模型 B，其中模型 B 的浅层参数使用模型 A 的浅层参数，模型 B 的高层参数随机初始化，然后通过冻结或微调的方式利用任务 A 的数据训练模型 B，模型 B 就是对应任务 A 的模型。</font></p>
<h4 id="1-2-预训练的思想"><a href="#1-2-预训练的思想" class="headerlink" title="1.2 预训练的思想"></a>1.2 预训练的思想</h4><p>有了图像领域预训练的引入，我们在此给出预训练的思想：任务 A 对应的模型 A 的参数不再是随机初始化的，而是通过任务 B 进行预先训练得到模型 B，然后利用模型 B 的参数对模型 A 进行初始化，再通过任务 A 的数据对模型 A 进行训练。注：模型 B 的参数是随机初始化的。</p>
<h3 id="二、语言模型"><a href="#二、语言模型" class="headerlink" title="二、语言模型"></a>二、语言模型</h3><p>想了解预训练语言模型，首先得了解什么是语言模型。</p>
<p>语言模型通俗点讲就是<strong>计算一个句子的概率。</strong>也就是说，对于语言序列 $w_1,w_2,\cdots,w_n$，语言模型就是计算该序列的概率，即 $P(w_1,w_2,\cdots,w_n)$。</p>
<p>下面通过两个实例具体了解上述所描述的意思：</p>
<ol>
<li>假设给定两句话 “判断这个词的磁性” 和 “判断这个词的词性”，语言模型会认为后者更自然。转化成数学语言也就是：$P(判断，这个，词，的，词性) \gt P(判断，这个，词，的，磁性)$</li>
<li>假设给定一句话做填空 “判断这个词的____”，则问题就变成了给定前面的词，找出后面的一个词是什么，转化成数学语言就是：$P(词性|判断，这个，词，的) \gt P(磁性|判断，这个，词，的)$</li>
</ol>
<p>通过上述两个实例，可以给出语言模型更加具体的描述：给定一句由 $n$ 个词组成的句子 $W&#x3D;w_1,w_2,\cdots,w_n$，计算这个句子的概率 $P(w_1,w_2,\cdots,w_n)$，或者计算根据上文计算下一个词的概率 $P(w_n|w_1,w_2,\cdots,w_{n-1})$。</p>
<p>下面将介绍语言模型的两个分支，统计语言模型和神经网络语言模型。</p>
<h4 id="2-1-统计语言模型"><a href="#2-1-统计语言模型" class="headerlink" title="2.1 统计语言模型"></a>2.1 统计语言模型</h4><p>统计语言模型的基本思想就是<strong>计算条件概率</strong>。</p>
<p>给定一句由 $n$ 个词组成的句子 $W&#x3D;w_1,w_2,\cdots,w_n$，计算这个句子的概率 $P(w_1,w_2,\cdots,w_n)$ 的公式如下（条件概率乘法公式的推广，链式法则）：<br>$$<br>\begin{align*}<br>P(w_1,w_2,\cdots,w_n)<br>&amp; &#x3D;  P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots p(w_n|w_1,w_2,\cdots,w_{n-1}) \<br>&amp; &#x3D; \prod_i P(w_i|w1,w_2,\cdots,w_{i-1})<br>\end{align*}<br>$$<br>对于上一节提到的 “判断这个词的词性” 这句话，利用上述的公式，可以得到：<br>$$<br>\begin{align*}<br>&amp; P(判断，这个，词，的，词性) &#x3D; \<br>&amp; P(判断)P(这个|判断)P(词|判断，这个) \<br>&amp; P(的|判断，这个，词)P(词性|判断，这个，词，的)P(判断，这个，词，的，词性)<br>\end{align*}<br>$$<br>对于上一节提到的另外一个问题，当给定前面词的序列 “判断，这个，词，的” 时，想要知道下一个词是什么，可以直接计算如下概率：<br>$$<br>P(w_{next}|判断，这个，词，的)\quad\text{公式(1)}<br>$$<br>其中，$w_{next} \in V$ 表示词序列的下一个词，$V$ 是一个具有 $|V|$ 个词的词典（词集合）。</p>
<p>对于公式（1），可以展开成如下形式：<br>$$<br>P(w_{next}|判断，这个，词，的) &#x3D; \frac{count(w_{next}，判断，这个，词，的)}{count(判断，这个，词，的)} \quad\text{公式(2)}<br>$$<br>对于公式（2），可以把字典 $V$ 中的多有单词，逐一作为 $w_{next}$，带入计算，最后取最大概率的词作为 $w_{next}$ 的候选词。</p>
<p>如果 $|V|$ 特别大，公式（2）的计算将会非常困难，但是我们可以引入马尔科夫链的概念（当然，在这里只是简单讲讲如何做，关于马尔科夫链的数学理论知识可以自行查看其他参考资料）。</p>
<p>假设字典 $V$ 中有 “火星” 一词，可以明显发现 “火星” 不可能出现在 “判断这个词的” 后面，因此（火星，判断，这个，词，的）这个组合是不存在的，并且词典中会存在很多类似于 “火星” 这样的词。</p>
<p>进一步，可以发现我们把（火星，判断，这个，词，的）这个组合判断为不存在，是因为 “火星” 不可能出现在 “词的” 后面，也就是说我们可以考虑是否把公式（1）转化为<br>$$<br>P(w_{next}|判断，这个，词，的) \approx P(w_{next}|词，的)\quad\text{公式(3)}<br>$$<br>公式（3）就是马尔科夫链的思想：假设 $w_{next}$ 只和它之前的 <strong>$k$ 个词有相关性</strong>，$k&#x3D;1$ 时称作一个单元语言模型，$k&#x3D;2$ 时称为二元语言模型。</p>
<p>可以发现通过马尔科夫链后改写的公式计算起来将会简单很多，下面我们举个简单的例子介绍下如何计算一个二元语言模型的概率。</p>
<p>其中二元语言模型的公式为：<br>$$<br>P(w_i|w_{i-1})&#x3D;\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\quad\text{公式(4)}<br>$$</p>
<p>假设有一个文本集合：</p>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">“词性是动词”</span><br><span class="line">“判断单词的词性”</span><br><span class="line">“磁性很强的磁铁”</span><br><span class="line">“北京的词性是名词”</span><br></pre></td></tr></table></figure>

<p>对于上述文本，如果要计算 $P(词性|的)$ 的概率，通过公式（4），需要统计 “的，词性” 同时按序出现的次数，再除以 “的” 出现的次数：<br>$$<br>P(词性|的) &#x3D; \frac{count(的，词性)}{count(的)} &#x3D; \frac{2}{3}\quad\text{公式(5)}<br>$$<br>上述文本集合是我们自定制的，然而对于绝大多数具有现实意义的文本，会出现数据稀疏的情况，例如<strong>训练时未出现，测试时出现了的未登录单词</strong>。</p>
<p>由于数据稀疏问题，则会出现概率值为 0 的情况（填空题将无法从词典中选择一个词填入），为了避免 0 值的出现，会使用一种<strong>平滑的策略</strong>——分子和分母都加入一个非 0 正数，例如可以把公式（4）改为：<br>$$<br>P(w_i|w_{i-1}) &#x3D; \frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+|V|}\quad\text{公式(6)}<br>$$</p>
<h4 id="2-2-神经网络语言模型"><a href="#2-2-神经网络语言模型" class="headerlink" title="2.2 神经网络语言模型"></a>2.2 神经网络语言模型</h4><p>上一节简单的介绍了统计语言模型，并且在结尾处讲到统计语言模型存在数据稀疏问题，针对该问题，我们也提出了平滑方法来应对这个问题。</p>
<p>神经网络语言模型则引入神经网络架构来估计单词的分布，<strong>并且通过词向量的距离衡量单词之间的相似度，因此，对于未登录单词，也可以通过相似词进行估计，进而避免出现数据稀疏问题</strong>。</p>
<p>神经网络+语言模型就是用神经网络的方法去完成以下两个和人说的话相关的任务。</p>
<p>这第二个任务：“判断”，“一个”，“词”，“的”，“<code>___</code>”</p>
<p>假设词库里有“词性”和“火星”</p>
<p>P(<code>__</code>|“判断”，“一个”，“词”，“的”)</p>
<p>词性</p>
<p>w1,w2,w3,w4（上述 4 个单词的独热编码）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w1*Q=c1,</span><br><span class="line">w2*Q=c2,</span><br><span class="line">w3*Q=c3,</span><br><span class="line">w4*Q=c4,</span><br><span class="line"></span><br><span class="line">C=[c1,c2,c3,c4]</span><br><span class="line">Q就是一个随机矩阵，是一个参数（可学习）</span><br></pre></td></tr></table></figure>

<p>“判断”，“这个”，“词”，“的”，“词性”</p>
<p>softmax（U[tanh(WC+b1)]+b2）&#x3D;&#x3D; [0.1, 0.1, 0.2, 0.2, 0.4] $\in[1,V_L]$</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526102300297.png" alt="image-20230526102300297"></p>
<p>如图所示的神经网络语言模型分为三层，接下来我们详细讲解这三层的作用：</p>
<ol>
<li>神经网络语言模型的第一层，为输入层。首先将前 $n-1$ 个单词用 Onehot 编码（例如：0001000）作为原始单词输入，之后乘以一个随机初始化的矩阵 Q 后获得词向量 $C(w_i)$，对这 $n-1$ 个词向量处理后得到输入 $x$，记作 $x&#x3D;(C(w_1),C(w_2),\cdots,C(w_{t-1}))$</li>
<li>神经网络语言模型的第二层，为隐层，包含 $h$ 个隐变量，$H$ 代表权重矩阵，因此隐层的输出为 $Hx+d$，其中 $d$ 为偏置项。并且在此之后使用 $tanh$ 作为激活函数。</li>
<li>神经网络语言模型的第三层，为输出层，一共有 $|V|$  个输出节点（字典大小），直观上讲，每个输出节点 $y_i$ 是词典中每一个单词概率值。最终得到的计算公式为：$y &#x3D; softmax(b+Wx+U\tanh(d+Hx))$，其中 $W$ 是直接从输入层到输出层的权重矩阵，$U$ 是隐层到输出层的参数矩阵。</li>
</ol>
<h3 id="三、词向量"><a href="#三、词向量" class="headerlink" title="三、词向量"></a>三、词向量</h3><p>在描述神经网络语言模型的时候，提到 Onehot 编码和词向量 $C(w_i)$，但是并没有具体提及他们到底是什么玩意。</p>
<p>由于他们对于未来 BERT 的讲解非常重要，所以在这里重开一章来描述词向量到底是什么，如何表示。</p>
<h4 id="3-1-独热（Onehot）编码"><a href="#3-1-独热（Onehot）编码" class="headerlink" title="3.1 独热（Onehot）编码"></a>3.1 独热（Onehot）编码</h4><p>独热编码：让计算机认识单词</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526102333330.png" alt="image-20230526102333330"></p>
<p>对于上图的解释，假设有一个包含 8 个次的字典 $V$，“time” 位于字典的第 1 个位置，“banana” 位于字典的第 8 个位置，因此，采用独热表示方法，对于 “time” 的向量来说，除了第 1 个位置为 1，其余位置为 0；对于 “banana” 的向量来说，除了第 8 个位置为 1，其余位置为 0。</p>
<p>但是，对于独热表示的向量，如果采用余弦相似度计算向量间的相似度，<strong>可以明显的发现任意两者向量的相似度结果都为 0</strong>，即任意二者都不相关，也就是说独热表示无法解决词之间的相似性问题。</p>
<h4 id="3-2-Word-Embedding"><a href="#3-2-Word-Embedding" class="headerlink" title="3.2 Word Embedding"></a>3.2 Word Embedding</h4><p>由于独热表示无法解决词之间相似性问题，这种表示很快就被词向量表示给替代了，这个时候聪明的你可能想到了在神经网络语言模型中出现的一个词向量 $C(w_i)$，对的，<strong>这个 $C(w_i)$ 其实就是单词对应的 Word Embedding 值，也就是我们这节的核心——词向量。</strong></p>
<p>在神经网络语言模型中，我们并没有详细解释词向量是如何计算的，现在让我们重看神经网络语言模型的架构图：</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526102449217.png" alt="image-20230526102449217"></p>
<p>上图所示有一个 $V×m$ 的矩阵 $Q$，这个矩阵 $Q$ 包含 $V$ 行，$V$ 代表词典大小，每一行的内容代表对应单词的 Word Embedding 值。</p>
<p>只不过 $Q$ 的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵 $Q$，当这个网络训练好之后，矩阵 $Q$ 的内容被正确赋值，每一行代表一个单词对应的 Word embedding 值。</p>
<p>但是这个词向量有没有解决词之间的相似度问题呢？为了回答这个问题，我们可以看看词向量的计算过程：<br>$$<br>\begin{bmatrix}<br>0&amp;0&amp;1&amp;0&amp;0<br>\end{bmatrix}</p>
<p>\begin{bmatrix}<br>17&amp;24&amp;1\<br>23&amp;5&amp;7\<br>4&amp;6&amp;13\<br>10&amp;12&amp;19\<br>11&amp;18&amp;25<br>\end{bmatrix}</p>
<p>&#x3D;</p>
<p>\begin{bmatrix}<br>4&amp;6&amp;13<br>\end{bmatrix}<br>\quad\text{公式(8)}<br>$$<br>通过上述词向量的计算，可以发现第 4 个词的词向量表示为 $[10,12,19]$。<br>$$<br>\begin{bmatrix}<br>0&amp;0&amp;0&amp;1&amp;0<br>\end{bmatrix}</p>
<p>\begin{bmatrix}<br>17&amp;24&amp;1\<br>23&amp;5&amp;7\<br>4&amp;6&amp;13\<br>10&amp;12&amp;19\<br>11&amp;18&amp;25<br>\end{bmatrix}</p>
<p>&#x3D;</p>
<p>\begin{bmatrix}<br>10&amp;12&amp;19<br>\end{bmatrix}<br>\quad\text{公式(8)}<br>$$<br>如果再次采用余弦相似度计算两个词之间的相似度，结果不再是 0 ，既可以一定程度上描述两个词之间的相似度。</p>
<p>下图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成 Word Embedding 后，很容易找出语义相近的其它词汇。</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527135024892.png" alt="image-20230527135024892"></p>
<p>神经网络语言模型：通过神经网络解决两个人说的话的问题</p>
<p>有一个副产品：Q 矩阵–》新的词向量（词向量可以选择词向量的维度，可以求两个词之间的相似程度）</p>
<p>下游任务</p>
<h3 id="四、Word2Vec-模型"><a href="#四、Word2Vec-模型" class="headerlink" title="四、Word2Vec 模型"></a>四、Word2Vec 模型</h3><p><font color="red">NNLM（）–&gt;  预测下一个词</font></p>
<p>神经网络+语言模型：用神经网络去解决和人说话有关的两个任务的一个东西</p>
<p>softmax(w2(tanh(（w1x+b1）))+b2)</p>
<p>得到一个副产品（词向量）</p>
<p>Q 矩阵，对于任何一个独热编码的词向量都可以通过 Q 矩阵得到新的词向量</p>
<ol>
<li>可以转换维度</li>
<li>相似词之间的词向量之间也有了关系</li>
</ol>
<p><font color="red">Word2Vec() –&gt; 为了得到词向量</font></p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526103322056.png" alt="image-20230526103322056"></p>
<p>NNLM 和 Word2Vec 基本一致（一模一样），不考虑细节，网络架构就是一模一样</p>
<p><strong>CBOW</strong></p>
<p>给出一个词的&#x3D;&#x3D;上下文&#x3D;&#x3D;，得到这个词</p>
<p>“我是最<code>_</code>的Nick”</p>
<p>“帅” $w_t$</p>
<p><strong>Skip-gram</strong></p>
<p>给出一个词，得到这个词的上下文</p>
<p>“帅”</p>
<p>“我是<code>_</code>的Nick”</p>
<p><font color="red">NNLM和Word2Vec的区别</font></p>
<p>NNNL –》 重点是预测下一词，双层感知机softmax(w2(tanh(（w1(xQ)+b1）))+b2)</p>
<p>Word2Vec –》 CBOW 和 Skip-gram 的两种架构的重点都是得到一个 Q 矩阵，softmax(w1 (xQ) +b1)</p>
<ol>
<li>CBOW：一个老师告诉多个学生，Q 矩阵怎么变</li>
<li>Skip-gram：多个老师告诉一个学生，Q 矩阵怎么变</li>
</ol>
<p><font color="red">Word2Vec的缺点</font></p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526104400931.png" alt="image-20230526104400931"></p>
<p>00010 代表 apple × Q &#x3D; 10，12，19</p>
<p>apple（苹果，）</p>
<p>假设数据集里面的 apple 只有苹果这个意思，没有这个意思（训练）</p>
<p>（测试，应用）10，12，19 apple, 无法表示这个意思</p>
<p>词向量不能进行多意 —&gt; ELMO</p>
<h3 id="五、自然语言处理的预训练模型"><a href="#五、自然语言处理的预训练模型" class="headerlink" title="五、自然语言处理的预训练模型"></a>五、自然语言处理的预训练模型</h3><p>突然在文章中插入这一段，其实就是给出一个问题：Word Embedding 这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会 Word Embedding 后下游任务是怎么使用它的。</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526105139515.png" alt="image-20230526105139515"></p>
<p>假设如上图所示，我们有个NLP的下游任务，比如 QA，就是问答问题，所谓问答问题，指的是给定一个问题 X，给定另外一个句子 Y，要判断句子 Y 是否是问题 X 的正确答案。</p>
<p>预训练语言模型终于出来（给出一句话，我们先使用独热编码（一一对应的一种表查询），再使用Word2Vec 预训练好的 Q 矩阵直接得到词向量，然后进行接下来的任务）</p>
<ol>
<li>冻结：可以不改变 Q 矩阵</li>
<li>微调：随着任务的改变，改变 Q 矩阵</li>
</ol>
<p>下游NLP任务在使用 Word Embedding 的时候也类似图像有两种做法，一种是 Frozen，就是 Word Embedding 那层网络参数固定不动；另外一种是 Fine-Tuning，就是 Word Embedding 这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p>
<h3 id="六、RNN-和-LSTM"><a href="#六、RNN-和-LSTM" class="headerlink" title="六、RNN 和 LSTM"></a>六、RNN 和 LSTM</h3><p>为什么要在这里穿插一个 RNN（Recurrent Neural Network） 和 LSTM（Long Short-Term Memory） 呢？</p>
<p>因为接下来要介绍的 ELMo（Embeddings from Language Models） 模型在训练过程中使用了双向长短期记忆网络（Bi-LSTM）。</p>
<p>当然，这里只是简单地介绍，想要详细了解的可以去查看网上铺天盖地的参考资料。</p>
<h4 id="6-1-RNN"><a href="#6-1-RNN" class="headerlink" title="6.1 RNN"></a>6.1 RNN</h4><p>传统的神经网络无法获取时序信息，然而<strong>时序信息在自然语言处理任务中非常重要</strong>。</p>
<p>例如对于这一句话 “我吃了一个苹果”，“苹果” 的词性和意思，在这里取决于前面词的信息，如果没有 “我吃了一个” 这些词，“苹果” 也可以翻译为乔布斯搞出来的那个被咬了一口的苹果。</p>
<p>也就是说，RNN 的出现，让处理时序信息变为可能。</p>
<p>RNN 的基本单元结构如下图所示：</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526105951323.png" alt="image-20230526105951323"></p>
<p>上图左边部分称作 RNN 的一个 timestep，在这个 timestep 中可以看到，在 $t$ 时刻，输入变量 $x_t$，通过 RNN 的一个基础模块 A，输出变量 $h_t$，而 $t$ 时刻的信息，将会传递到下一个时刻 $t+1$。</p>
<p>如果把模块按照时序展开，则会如上图右边部分所示，<strong>由此可以看到 RNN 为多个基础模块 A 的互连，每一个模块都会把当前信息传递给下一个模块</strong>。</p>
<p>RNN 解决了时序依赖问题，但这里的时序一般指的是短距离的，首先我们先介绍下短距离依赖和长距离依赖的区别：</p>
<ul>
<li>短距离依赖：对于这个填空题 “我想看一场篮球____”，我们很容易就判断出 “篮球” 后面跟的是 “比赛”，这种短距离依赖问题非常适合 RNN。</li>
<li>长距离依赖：对于这个填空题 “我出生在中国的瓷都景德镇，小学和中学离家都很近，……，我的母语是____”，对于短距离依赖，“我的母语是” 后面可以紧跟着 “汉语”、“英语”、“法语”，但是如果我们想精确答案，则必须回到上文中很长距离之前的表述 “我出生在中国的瓷都景德镇”，进而判断答案为 “汉语”，而 RNN 是很难学习到这些信息的。</li>
</ul>
<h4 id="6-2-RNN-的梯度消失问题"><a href="#6-2-RNN-的梯度消失问题" class="headerlink" title="6.2 RNN 的梯度消失问题"></a>6.2 RNN 的梯度消失问题</h4><p>在这里我简单讲解下 RNN 为什么不适合长距离依赖问题。</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526110220055.png" alt="image-20230526110220055"></p>
<p>在这里我简单讲解下 RNN 为什么不适合长距离依赖问题。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/RNN模型结构.jpg" style="zoom:50%;">

<p>如上图所示，为RNN模型结构，前向传播过程包括：</p>
<ul>
<li><strong>隐藏状态：</strong>$h^{(t)} &#x3D; \sigma (z^{(t)}) &#x3D; \sigma(Ux^{(t)} + Wh^{(t-1)} + b)$ ，此处激活函数一般为 $tanh$ 。</li>
<li><strong>模型输出：</strong>$o^{(t)} &#x3D; Vh^{(t)} + c$</li>
<li><strong>预测输出：</strong>$\hat{y}^{(t)} &#x3D; \sigma(o^{(t)})$ ，此处激活函数一般为softmax。</li>
<li><strong>模型损失：</strong>$L &#x3D; \sum_{t &#x3D; 1}^{T} L^{(t)}$</li>
</ul>
<p>RNN 所有的 timestep 共享一套参数 $U,V,W$，在 RNN 反向传播过程中，需要计算 $U,V,W$ 等参数的梯度，以 $W$ 的梯度表达式为例（假设 RNN 模型的损失函数为 $L$）：<br>$$<br>\frac{\partial L}{\partial W}<br>&#x3D; \sum_{t &#x3D; 1}^{T} \frac{\partial L}{\partial y^{(T)}} \frac{\partial y^{(T)}}{\partial o^{(T)}} \frac{\partial o^{(T)}}{\partial h^{(T)}} \left( \prod_{k&#x3D;t + 1}^{T} \frac{\partial h^{(k)}}{\partial h^{(k - 1)}} \right) \frac{\partial h^{(t)}}{\partial W} \  \<br>&#x3D; \sum_{t &#x3D; 1}^{T} \frac{\partial L}{\partial y^{(T)}} \frac{\partial y^{(T)}}{\partial o^{(T)}} \frac{\partial o^{(T)}}{\partial h^{(T)}} \left( \prod_{k&#x3D;t+1}^{T} tanh^{‘}(z^{(k)}) W \right) \frac{\partial h^{(t)}}{\partial W} \  \<br>\quad\text{公式(9)}<br>$$<br>对于公式（9）中的 $\left( \prod_{k&#x3D;t + 1}^{T} \frac{\partial h^{(k)}}{\partial h^{(k - 1)}} \right) &#x3D; \left( \prod_{k&#x3D;t+1}^{T} tanh^{‘}(z^{(k)}) W \right)$，$\tanh$ 的导数总是小于 1 的，由于是 $T-(t+1)$ 个 timestep 参数的连乘，<strong>如果 $W$ 的主特征值小于 1，梯度便会消失；如果 $W$ 的特征值大于 1，梯度便会爆炸。</strong></p>
<p>需要注意的是，RNN和DNN梯度消失和梯度爆炸含义并不相同。</p>
<p>RNN中权重在各时间步内共享，最终的梯度是各个时间步的梯度和，梯度和会越来越大。因此，RNN中总的梯度是不会消失的，即使梯度越传越弱，也只是远距离的梯度消失。 从公式（9）中的 $\left( \prod_{k&#x3D;t+1}^{T} tanh^{‘}(z^{(k)}) W \right)$ 可以看到，<strong>RNN所谓梯度消失的真正含义是，梯度被近距离（$t+1 趋向于 T$）梯度主导，远距离（$t+1 远离 T$）梯度很小，导致模型难以学到远距离的信息。</strong> </p>
<h4 id="6-3-LSTM"><a href="#6-3-LSTM" class="headerlink" title="6.3 LSTM"></a>6.3 LSTM</h4><p>为了解决 RNN 缺乏的序列长距离依赖问题，LSTM 被提了出来，首先我们来看看 LSTM 相对于 RNN 做了哪些改进：</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230526110342022.png" alt="image-20230526110342022"></p>
<p>如上图所示，为 LSTM 的 RNN 门控结构（LSTM 的 timestep），LSTM 前向传播过程包括：</p>
<ul>
<li><strong>遗忘门：</strong>决定了丢弃哪些信息，遗忘门接收 $t-1$ 时刻的状态 $h_{t-1}$，以及当前的输入 $x_t$，经过 Sigmoid 函数后输出一个 0 到 1 之间的值 $f_t$<ul>
<li>输出： $f_{t} &#x3D; \sigma(W_fh_{t-1} + U_fx_{t} + b_f)$</li>
</ul>
</li>
<li><strong>输入门：</strong>决定了哪些新信息被保留，并更新细胞状态，输入们的取值由 $h_{t-1}$ 和 $x_t$ 决定，通过 Sigmoid 函数得到一个 0 到 1 之间的值 $i_t$，而 $\tanh$ 函数则创造了一个当前细胞状态的候选 $a_t$<ul>
<li>输出：$i_{t} &#x3D; \sigma(W_ih_{t-1} + U_ix_{t} + b_i)$ , $\tilde{C_{t} }&#x3D; tanhW_ah_{t-1} + U_ax_{t} + b_a$</li>
</ul>
</li>
<li><strong>细胞状态：</strong>旧细胞状态 $C_{t-1}$ 被更新到新的细胞状态 $C_t$ 上，<ul>
<li>输出：$C_{t} &#x3D; C_{t-1}\odot f_{t} + i_{t}\odot \tilde{C_{t} }$</li>
</ul>
</li>
<li><strong>输出门：</strong>决定了最后输出的信息，输出门取值由 $h_{t-1}$ 和 $x_t$ 决定，通过 Sigmoid 函数得到一个 0 到 1 之间的值 $o_t$，最后通过 $\tanh$ 函数决定最后输出的信息<ul>
<li>输出：$o_{t} &#x3D; \sigma(W_oh_{t-1} + U_ox_{t} + b_o)$ , $h_{t} &#x3D; o_{t}\odot tanhC_{t}$</li>
</ul>
</li>
<li><strong>预测输出：</strong>$\hat{y}<em>{t} &#x3D; \sigma(Vh</em>{t}+c)$</li>
</ul>
<h4 id="6-4-LSTM-解决-RNN-的梯度消失问题"><a href="#6-4-LSTM-解决-RNN-的梯度消失问题" class="headerlink" title="6.4 LSTM 解决 RNN 的梯度消失问题"></a>6.4 LSTM 解决 RNN 的梯度消失问题</h4><p>明白了 RNN 梯度消失的原因之后，我们看 LSTM 如何解决问题的呢？</p>
<p>RNN 梯度消失的原因是，随着梯度的传导，梯度被近距离梯度主导，模型难以学习到远距离的信息。具体原因也就是 $\prod_{k&#x3D;t+1}^{T}\frac{\partial h_{k}}{\partial h_{k - 1}}$ 部分，在迭代过程中，每一步 $\frac{\partial h_{k}}{\partial h_{k - 1}}$ <strong>始终在 [0,1) 之间或者始终大于 1。</strong></p>
<p>而对于 LSTM 模型而言，针对 $\prod <em>{k&#x3D;t+1}^{T} \frac{\partial C</em>{k}}{\partial C_{k-1}}$ 求得：<br>$$<br>\begin{align}<br>&amp; \frac{\partial C_{k}}{\partial C_{k-1}} &#x3D; f_k + other \<br>&amp; \prod <em>{k&#x3D;t+1}^{T} \frac{\partial C</em>{k}}{\partial C_{k-1}} &#x3D; f_{k}f_{k+1}…f_{T} + other  \<br>\end{align}<br>$$<br>在 LSTM 迭代过程中，针对 $\prod_{k&#x3D;t+1}^{T} \frac{\partial C_{k}}{\partial C_{k-1}}$ 而言，每一步$\frac{\partial C_{k}}{\partial C_{k-1}}$ <strong>可以自主的选择在 [0,1] 之间，或者大于1</strong>，因为 $f_{k}$ 是可训练学习的。那么整体 $\prod <em>{k&#x3D;t+1}^{T} \frac{\partial C</em>{k}}{\partial C_{k-1}}$ 也就不会一直减小，远距离梯度不至于完全消失，也就能够解决 RNN 中存在的梯度消失问题。</p>
<p>LSTM 遗忘门值 $f_t$ 可以选择在 [0,1] 之间，让 LSTM 来改善梯度消失的情况。也可以选择接近 1，让遗忘门饱和，此时远距离信息梯度不消失；也可以选择接近 0，此时模型是故意阻断梯度流，遗忘之前信息。</p>
<p>另外需要强调的是LSTM搞的这么复杂，<strong>除了在结构上天然地克服了梯度消失的问题，更重要的是具有更多的参数来控制模型</strong>；通过四倍于RNN的参数量，可以更加精细地预测时间序列变量。</p>
<p>此外，我记得有一篇文章讲到，<strong>LSTM 在 200左右长度的文本上，就已经捉襟见肘了</strong>。</p>
<h3 id="七、ELMo-模型"><a href="#七、ELMo-模型" class="headerlink" title="七、ELMo 模型"></a>七、ELMo 模型</h3><h4 id="7-1-ELMo-的预训练"><a href="#7-1-ELMo-的预训练" class="headerlink" title="7.1 ELMo 的预训练"></a>7.1 ELMo 的预训练</h4><p>在讲解 Word Embedding 时，细心地读者一定已经发现，这些词表示方法本质上是静态的，每一个词都有一个唯一确定的词向量，不能根据句子的不同而改变，无法处理自然语言处理任务中的多义词问题。</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527140845836.png" alt="image-20230527140845836"></p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527141021360.png" alt="image-20230527141021360"></p>
<p>如上图所示，例如多义词 Bank，有两个常用含义，但是 Word Embedding 在对 bank 这个单词进行编码的时候，是区分不开这两个含义的。</p>
<p><font color="red">ELMo（专门做词向量，通过预训练）</font></p>
<p>不只是训练一个 Q 矩阵，我还可以把这个词的上下文信息融入到这个 Q 矩阵中</p>
<p>左边的 LSTM 获取 E2 的上文信息，右边就是下文信息</p>
<p>x1,x2, x4,x5 –&gt; Word2Vec x1+x2+x4+x5 —&gt; 预测那一个词</p>
<p>获取上下文信息后，把三层的信息进行一个叠加</p>
<p>E1+E2+E3 &#x3D; K1 一个新的词向量 $\approx$ E1</p>
<p>E2,E3 相当于两个上下文信息</p>
<p>E1+E2+E3+E4</p>
<p>K1 包含了第一个词的词向量包含单词特征、句法特征、语义特征</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527141117398.png" alt="image-20230527141117398"></p>
<p>上图展示的是其第一阶段预训练过程，它的网络结构采用了双层双向 LSTM，目前语言模型训练的任务目标是根据单词 $w_i$ 的上下文去正确预测单词 $w_i$，$w_i$ 之前的单词序列 Context-before 称为上文，之后的单词序列 Context-after 称为下文。</p>
<p>图中左端的前向双层 LSTM 代表正方向编码器，输入的是从左到右顺序的除了预测单词外 $W_i$ 的上文 Context-before；右端的逆向双层 LSTM 代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层 LSTM 叠加。</p>
<h4 id="7-2-ELMo-的-Feature-based-Pre-Training"><a href="#7-2-ELMo-的-Feature-based-Pre-Training" class="headerlink" title="7.2 ELMo 的 Feature-based Pre-Training"></a>7.2 ELMo 的 Feature-based Pre-Training</h4><p>上面介绍的是 ELMo 的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527142530867.png" alt="image-20230527142530867"></p>
<p>上图展示了下游任务的使用过程，比如我们的下游任务仍然是 QA 问题，此时对于问句 X：</p>
<ol>
<li>我们可以先将句子 X 作为预训练好的 ELMo 网络的输入，这样句子 X 中每个单词在 ELMO 网络中都能获得对应的三个 Embedding；</li>
<li>之后给予这三个 Embedding 中的每一个 Embedding 一个权重 a，这个权重可以学习得来，根据各自权重累加求和，将三个 Embedding 整合成一个；</li>
<li>然后将整合后的这个 Embedding 作为 X 句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。</li>
<li>对于上图所示下游任务 QA 中的回答句子 Y 来说也是如此处理。</li>
</ol>
<p><strong>因为 ELMo 给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为 “Feature-based Pre-Training”。</strong></p>
<p>至于为何这么做能够达到区分多义词的效果，原因在于在训练好 ELMo 后，<strong>在特征提取的时候，每个单词在两层 LSTM 上都会有对应的节点，这两个节点会编码单词的一些句法特征和语义特征，并且它们的 Embedding 编码是动态改变的</strong>，会受到上下文单词的影响，周围单词的上下文不同应该会强化某种语义，弱化其它语义，进而就解决了多义词的问题。</p>
<h3 id="八、Attention"><a href="#八、Attention" class="headerlink" title="八、Attention"></a>八、Attention</h3><p>上面巴拉巴拉了一堆，都在为 BERT 的讲解做铺垫，而接下来要叙述的 Attention 和 Transformer 同样如此，它们都只是 BERT 构成的一部分。</p>
<h4 id="8-1-人类的视觉注意力"><a href="#8-1-人类的视觉注意力" class="headerlink" title="8.1 人类的视觉注意力"></a>8.1 人类的视觉注意力</h4><p>Attention 是注意力的意思，从它的命名方式看，很明显借鉴了人类的注意力机制，因此，我们首先介绍人类的视觉注意力。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/人类的视觉注意力.jpg" style="zoom:50%;">

<p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p>
<p>这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p>
<p>上图形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，<strong>其中红色区域表明视觉系统更关注的目标</strong>，很明显对于上图所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p>
<p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，<strong>核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</strong></p>
<h4 id="8-2-Attention-的本质思想"><a href="#8-2-Attention-的本质思想" class="headerlink" title="8.2 Attention 的本质思想"></a>8.2 Attention 的本质思想</h4><p>从人类的视觉注意力可以看出，注意力模型 Attention 的本质思想为：从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略不重要的信息。</p>
<p>在详细讲解 Attention之前，我们在讲讲 Attention的其他作用。之前我们讲解 LSTM 的时候说到，虽然 LSTM 解决了序列长距离依赖问题，但是单词超过 200 的时候就会失效。<strong>而 Attention 机制可以更加好的解决序列长距离依赖问题，并且具有并行计算能力</strong>。现在不明白这点不重要，随着我们对 Attention 的慢慢深入，相信你会明白。</p>
<p>首先我们得明确一个点，注意力模型从大量信息 Values 中筛选出少量重要信息，这些重要信息一定是相对于另外一个信息 Query 而言是重要的，例如对于上面那张婴儿图，Query 就是观察者。也就是说，我们要搭建一个注意力模型，我们必须得要有一个 Query 和一个 Values，然后通过 Query 这个信息从 Values 中筛选出重要信息。</p>
<p>通过 Query 这个信息从 Values 中筛选出重要信息，简单点说，<strong>就是计算 Query 和 Values 中每个信息的相关程度。</strong></p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/attention-计算图.png" style="zoom:50%;">

<p>再具体点，通过上图，Attention 通常可以进行如下描述，表示为将 Query(Q) 和 key-value pairs（<strong>把 Values 拆分成了键值对的形式</strong>） 映射到输出上，其中 query、每个 key、每个 value 都是向量，输出是 V 中所有 values 的加权，其中权重是由 Query 和每个 key 计算出来的，计算方法分为三步：</p>
<ol>
<li>第一步：计算比较 Q 和 K 的相似度，用 f 来表示：$f(Q,K_i)\quad i&#x3D;1,2,\cdots,m$，一般第一步计算方法包括四种<ol>
<li>点乘（<strong>Transformer 使用</strong>）：$f(Q,K_i) &#x3D; Q^T K_i$</li>
<li>权重：$f(Q,K_i) &#x3D; Q^TWK_i$</li>
<li>拼接权重：$f(Q,K_i) &#x3D; W[Q^T;K_i]$</li>
<li>感知器：$f(Q,K_i)&#x3D;V^T \tanh(WQ+UK_i)$</li>
</ol>
</li>
<li>第二步：将得到的相似度进行 softmax 操作，进行归一化：$\alpha_i &#x3D; softmax(\frac{f(Q,K_i)}{\sqrt d_k})$<ol>
<li>这里简单讲解除以 $\sqrt d_k$ 的作用：假设 $Q$ , $K$ 里的元素的均值为0，方差为 1，那么 $A^T&#x3D;Q^TK$ 中元素的均值为 0，方差为 d。当 d 变得很大时， $A$ 中的元素的方差也会变得很大，如果 $A$ 中的元素方差很大(分布的方差大，分布集中在绝对值大的区域)，<strong>在数量级较大时， softmax 将几乎全部的概率分布都分配给了最大值对应的标签</strong>，由于某一维度的数量级较大，进而会导致 softmax 未来求梯度时会消失。总结一下就是 $\operatorname{softmax}\left(A\right)$ 的分布会和d有关。因此 $A$ 中每一个元素乘上 $\frac{1}{\sqrt{d_k}}$ 后，方差又变为 1，并且 $A$ 的数量级也将会变小。</li>
</ol>
</li>
<li>第三步：针对计算出来的权重 $\alpha_i$，对 $V$ 中的所有 values 进行加权求和计算，得到 Attention 向量：$Attention &#x3D; \sum_{i&#x3D;1}^m \alpha_i V_i$</li>
</ol>
<blockquote>
<p>我（查询对象 Q），这张图（被查询对象 V）</p>
<p>我看这张图，第一眼，我就会去判断哪些东西对我而言更重要，哪些对我而言又更不重要（去计算 Q 和 V 里的事物的重要度）</p>
<p>重要度计算，其实是不是就是相似度计算（更接近），点乘其实是求内积（不要关心为什么可以）</p>
<p>V被分成了Key和value </p>
<p>Q，$K &#x3D;k_1,k_2,\cdots,k_n$ ，我们一般使用点乘的方式</p>
<p>通过点乘的方法计算Q 和 K 里的每一个事物的相似度，就可以拿到 Q 和$k_1$的相似值$s_1$，Q 和$k_2$的相似值$s_2$，Q 和$k_n$的相似值 $s_n$</p>
<p>做一层 $softmax(s_1,s_2,\cdots,s_n)$ 就可以得到概率$(a_1,a_2,\cdots,a_n)$</p>
<p>进而就可以找出哪个对Q 而言更重要了</p>
</blockquote>
<blockquote>
<p>我们还得进行一个汇总，当你使用 Q 查询结束了后，Q 已经失去了它的使用价值了，我们最终还是要拿到这张图片的，只不过现在的这张图片，它多了一些信息（多了于我而言更重要，更不重要的信息在这里）</p>
<p>V &#x3D; $(v_1,v_2,\cdots,v_n)$</p>
<p>$(a_1,a_2,\cdots,a_n)<em>+(v_1,v_2,\cdots,v_n)&#x3D;(a_1</em>v_1+a_2<em>v_2+\cdots+a_n</em>v_n)$ &#x3D; V’ </p>
<p>这样的话，就得到了一个新的 V’，这个新的 V’ 就包含了，哪些更重要，哪些不重要的信息在里面，然后用 V’ 代替 V </p>
<p>一般 K&#x3D;V，在 Transformer 里，K!&#x3D;V 可不可以，可以的，但是 K 和 V 之间一定具有某种联系，这样的 QK 点乘才能知道V 哪些重要，哪些不重要</p>
</blockquote>
<h4 id="8-3-Self-Attention-模型"><a href="#8-3-Self-Attention-模型" class="headerlink" title="8.3 Self Attention 模型"></a>8.3 Self Attention 模型</h4><p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230527172420496.png" alt="image-20230527172420496"></p>
<blockquote>
<p>QK 相乘求相似度，做一个 scale（未来做 softmax 的时候避免出现极端情况）</p>
<p>然后做 Softmax 得到概率</p>
<p>新的向量表示了K 和 V（K&#x3D;&#x3D;V），然后这种表示还暗含了 Q 的信息（于 Q 而言，K 里面重要的信息），也就是说，挑出了 K 里面的关键点</p>
</blockquote>
<blockquote>
<p>Self-Attention 的关键点再于，不仅仅是 K$\approx$V$\approx$Q 来源于同一个 X，这三者是同源的</p>
<p>通过 X 找到 X 里面的关键点</p>
<p>并不是 K&#x3D;V&#x3D;Q&#x3D;X，而是通过三个参数 $W_Q,W_K,W_V$</p>
<p>接下来的步骤和注意力机制一模一样</p>
</blockquote>
<p>接下来为了表示的方便，我们先通过向量的计算叙述 Self Attention 计算的流程，然后再描述 Self Attention 的矩阵计算过程</p>
<ol>
<li><p>第一步，Q、K、V 的获取</p>
<p>1. </p>
   <img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/qkv.jpg" style="zoom:50%;">

<p>   上图操作：两个单词 Thinking 和 Machines。通过线性变换，即 $x_i$ 和 $x_2$ 两个向量分别与$W_q,W_k,W_v$ 三个矩阵点乘得到 ${q_1,q_2},{k_1,k_2},{v_1,v_2} $ 共 6 个向量。矩阵 Q 则是向量 $q_1,q_2$ 的拼接，K、V 同理。</p>
</li>
<li><p>第二步，MatMul</p>
<ol>
<li><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/Q-K乘积.jpg" style="zoom:50%;">

上图操作：向量 ${q_1,k_1}$ 做点乘得到得分 112， ${q_1,k_2}$ 做点乘得到得分96。注意：<strong>这里是通过 $q_1$ 这个信息找到 $x_1,x_2$ 中的重要信息。</strong></li>
</ol>
</li>
<li><p>第三步和第四步，Scale + Softmax</p>
<ol>
<li><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/qk-scale.jpg" style="zoom:50%;"></li>
</ol>
<p>上图操作：对该得分进行规范，除以 $\sqrt {d_k} &#x3D; 8$</p>
</li>
<li><p>第五步，MatMul</p>
<ol>
<li><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/qk-softmax.jpg" style="zoom:50%;"></li>
</ol>
</li>
</ol>
<blockquote>
<p>$z_1$表示的就是 thinking 的新的向量表示</p>
<p>对于 thinking，初始词向量为$x_1$</p>
<p>现在我通过 thinking  machines 这句话去查询这句话里的每一个单词和 thinking 之间的相似度</p>
<p>新的$z_1$依然是 thinking 的词向量表示，只不过这个词向量的表示蕴含了 thinking machines 这句话对于 thinking 而言哪个更重要的信息</p>
</blockquote>
<p>用得分比例 [0.88，0.12] 乘以 $[v_1,v_2]$ 值得到一个加权后的值，将这些值加起来得到 $z_1$。</p>
<p>上述所说就是 Self Attention 模型所做的事，仔细感受一下，用 $q_1$、$K&#x3D;[k_1,k_2]$ 去计算一个 Thinking 相对于 Thinking 和 Machine 的权重，再用权重乘以 Thinking 和 Machine 的 $V&#x3D;[v_1,v_2]$ 得到加权后的 Thinking 和 Machine 的 $V&#x3D;[v_1,v_2]$，最后求和得到针对各单词的输出 $z_1$。</p>
<p>同理可以计算出 Machine 相对于 Thinking 和 Machine 的加权输出 $z_2$，拼接 $z_1$ 和 $z_2$ 即可得到 Attention 值 $Z&#x3D;[z_1,z_2]$，这就是 Self Attention 的矩阵计算，如下所示。</p>
<p>之前的例子是单个向量的运算例子。这张图展示的是矩阵运算的例子，输入是一个 [2x4] 的矩阵（句子中每个单词的词向量的拼接），每个运算是 [4x3] 的矩阵，求得 Q、K、V。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/QKV-矩阵表示.jpg" style="zoom:50%;">

<p>Q 对 K 转制做点乘，除以 $\sqrt d_k$，做一个 softmax 得到合为 1 的比例，对 V 做点乘得到输出 Z。那么这个 Z 就是一个考虑过 Thinking 周围单词 Machine 的输出。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/QKVZ-结果.jpg" style="zoom:50%;">

<p>注意看这个公式，**$QK^T$ 其实就会组成一个 word2word 的 attention map！**（加了 softmax 之后就是一个合为 1 的权重了）。比如说你的输入是一句话 “i have a dream” 总共 4 个单词，这里就会形成一张 4x4 的注意力机制的图：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/注意力机制矩阵图.jpg" style="zoom:50%;">

<p>这样一来，每一个单词对应每一个单词都会有一个权重，<strong>这也是 Self Attention 名字的来源，即 Attention 的计算来源于 Source（源句） 和 Source 本身，通俗点讲就是 Q、K、V 都来源于输入 X 本身。</strong></p>
<blockquote>
<p>注意力机制是一个很宽泛的概念，QKV相乘就是注意力，但是他没有规定QKV是怎么来的。</p>
<p>通过一个查询变量Q，去找到V里面比较重要的东西。</p>
<p>假设K&#x3D;V，然后QK相乘求相似度，然后AV相乘得到注意力值Z，这个Z就是V的另外一种形式表示。</p>
<p>Q可以是任何一个东西，V也可以是任何一个东西，K往往等同于V（同源）</p>
</blockquote>
<h4 id="8-4-Self-Attention-和-RNN、LSTM-的区别"><a href="#8-4-Self-Attention-和-RNN、LSTM-的区别" class="headerlink" title="8.4 Self Attention 和 RNN、LSTM 的区别"></a>8.4 Self Attention 和 RNN、LSTM 的区别</h4><blockquote>
<p>Word2Vec无法将所有单词关联起来</p>
<p>RNN 无法做长序列，当一段话达到50个字，效果很差</p>
<p>LSTM通过各种门，遗忘门，选择性的可以记忆之前的信息（200个词）</p>
<p>Elmo通过LSTM，可以解决一词多义的问题。</p>
<p>self-attention:得到的词向量具有句法特征和语义特征（表征更完善），通过每个词和每个词之间的计算相似度。</p>
</blockquote>
<p>引入 Self Attention 有什么好处呢？或者说通过 Self Attention 到底学到了哪些规律或者抽取出了哪些特征呢？我们可以通过下述两幅图来讲解：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/self-attention-好处1.jpg" style="zoom:50%;">

<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/self-attention-好处2.jpg" style="zoom:50%;">

<p>从上述两张图可以看出，Self Attention 可以捕获同一个句子中单词之间的一些句法特征（例如第一张图展示的有一定距离的短语结构）或者语义特征（例如第二张图展示的 its 的指代对象为 Law）。</p>
<p>有了上述的讲解，我们现在可以来看看 Self Attention 和 RNN、LSTM 的区别：</p>
<ul>
<li>RNN、LSTM：如果是 RNN 或者 LSTM，需要依次序列计算，对于远距离的相互依赖的特征，<strong>要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小</strong>。</li>
<li>Self Attention：<ul>
<li>通过上述两幅图，很明显的可以看出，引入 Self Attention 后会更容易捕获句子中长距离的相互依赖的特征，<strong>因为 Self Attention 在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征</strong>；</li>
<li>除此之外，Self<br>Attention 对于<strong>一句话中的每个单词都可以单独的进行 Attention 值的计算</strong>，也就是说 Self Attention 对计算的并行性也有直接帮助作用，而对于必须得依次序列计算的 RNN 而言，是无法做到并行计算的。</li>
</ul>
</li>
</ul>
<p>从上面的计算步骤和图片可以看出，<strong>无论句子序列多长，都可以充分捕获近距离上往下问中的任何依赖关系，进而可以很好的提取句法特征还可以提取语义特征</strong>；而且对于一个句子而言，<strong>每个单词的计算是可以并行处理的</strong>。</p>
<p>理论上 Self-Attention （Transformer 50 个左右的单词效果最好）解决了 RNN 模型的长序列依赖问题，但是由于文本长度增加时，训练时间也将会呈指数增长，因此在处理长文本任务时可能不一定比 LSTM（200 个左右的单词效果最好） 等传统的 RNN 模型的效果好。</p>
<p>上述所说的，则是为何 Self Attention 逐渐替代 RNN、LSTM 被广泛使用的原因所在。</p>
<h4 id="8-5-Masked-Self-Attention-模型"><a href="#8-5-Masked-Self-Attention-模型" class="headerlink" title="8.5 Masked Self Attention 模型"></a>8.5 Masked Self Attention 模型</h4><p>趁热打铁，我们讲讲 Transformer 未来会用到的 Masked Self Attention 模型，这里的 Masked 就是要在做语言模型（或者像翻译）的时候，不给模型看到未来的信息，它的结构如下图所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/masked-attention.jpg" style="zoom:50%;">

<p>上图中和 Self Attention 重复的部分此处就不讲了，主要讲讲 Mask 这一块。</p>
<p>self-attention:</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230528210630351.png" alt="image-20230528210630351"></p>
<p><font color="red">Masked在注意力机制上做了一个改进：</font></p>
<blockquote>
<p>Masked在注意力机制上做了一个改进：</p>
<p>为什么要做这个改进：生成模型，生成单词，一个一个生成的</p>
<p>当我们做生成任务的时候，我们也想对生成的这个单词做注意力计算，但是，生成的句子是一个一个单词生成的</p>
</blockquote>
<blockquote>
<p>I have a dream</p>
<ol>
<li><p>I  第一次注意力计算，只有 I</p>
</li>
<li><p>I have 第二次，只有 I 和 have</p>
</li>
<li><p>I have a</p>
</li>
<li><p>I have a dream</p>
</li>
<li><p>I have a dream &lt;eos&gt;</p>
</li>
</ol>
</blockquote>
<p>假设在此之前我们已经通过 scale 之前的步骤得到了一个 attention map，<strong>而 mask 就是沿着对角线把灰色的区域用0覆盖掉，不给模型看到未来的信息</strong>，如下图所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/mask-attention-map.jpg" style="zoom:50%;">

<p>详细来说：</p>
<ol>
<li>“i” 作为第一个单词，只能有和 “i” 自己的 attention；</li>
<li>“have” 作为第二个单词，有和 “i、have” 前面两个单词的 attention；</li>
<li>“a” 作为第三个单词，有和 “i、have、a” 前面三个单词的 attention；</li>
<li>“dream” 作为最后一个单词，才有对整个句子 4 个单词的 attention。</li>
</ol>
<p>并且在做完 softmax 之后，横轴结果合为 1。如下图所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/mask-attention-map-softmax.jpg" style="zoom:50%;">

<p>具体为什么要 mask，未来再讲解 Transformer 的时候我们会详细解释。</p>
<h4 id="8-6-Multi-head-Self-Attention-模型"><a href="#8-6-Multi-head-Self-Attention-模型" class="headerlink" title="8.6 Multi-head Self Attention 模型"></a>8.6 Multi-head Self Attention 模型</h4><p>Attention</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/masked-attention-168527994618133.jpg" alt="img" style="zoom:50%;">

<p>Self-Attention</p>
<blockquote>
<p>Self-Attention 其实是 Attention 的一个具体做法</p>
<p>给定一个 X，通过自注意力模型，得到一个 Z，这个 Z 就是对 X 的新的表征（词向量），Z 这个词向量相比较 X 拥有了句法特征和语义特征</p>
</blockquote>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/注意力机制矩阵图-168527994618235.jpg" alt="img" style="zoom:50%;">

<p>Multi-Head Self-Attention（多头自注意力）</p>
<blockquote>
<p>Z 相比较 X 有了提升，通过 Multi-Head Self-Attention，得到的 $Z{‘}$ 相比较 Z 又有了进一步提升</p>
<p>多头自注意力，问题来了，多头是什么，多头的个数用 h 表示，一般$h&#x3D;8$，我们通常使用的是 8 头自注意力</p>
<p>如何多头？</p>
<p>对于 X，我们不是说，直接拿 X 去得到 Z，而是把 X 分成了 8 块（8 头），得到 Z0-Z7</p>
<p>然后把 Z0-Z7 拼接起来，再做一次线性变换（改变维度）得到 Z</p>
</blockquote>
<p>Multi-Head Attention 就是把 Self Attention 得到的注意力值 $Z$ 切分成 n 个 $Z_1,Z_2,\cdots,Z_n$，然后通过全连接层获得新的 $Z’$.</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/multi-head-attention.png" style="zoom:50%;">

<p>我们还是以上面的形式来解释，我们对 $Z$ 进行 8 等份的切分得到 8 个 $Z_i$ 矩阵：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/8-head-attention.jpg" style="zoom:50%;">

<p>为了使得输出与输入结构相同，拼接矩阵 $Z_i$ 后乘以一个线性 $W_0$ 得到最终的Z：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/8-z-拼接.jpg" style="zoom:50%;">

<p>可以通过下图看看 multi-head attention 的整个流程：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/multi-head-拼接.jpg" style="zoom:50%;">

<p>上述操作有什么好处呢？<strong>多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息</strong>。其实本质上是论文原作者发现这样效果确实好。</p>
<blockquote>
<p>有什么作用？</p>
<p>机器学习的本质是什么：y&#x3D;$\sigma$(wx+b)，在做一件什么事情，非线性变换（把一个看起来不合理的东西，通过某个手段（训练模型），让这个东西变得合理）</p>
<p>非线性变换的本质又是什么？改变空间上的位置坐标，任何一个点都可以在维度空间上找到，通过某个手段，让一个不合理的点（位置不合理），变得合理</p>
<p>这就是词向量的本质</p>
<p>one-hot 编码（0101010）</p>
<p>word2vec（11，222，33）</p>
<p>emlo（15，3，2）</p>
<p>attention（124，2，32）</p>
<p>multi-head attention（1231，23，3），把 X 切分成 8 块（8 个子空间），这样一个原先在一个位置上的 X，去了空间上 8 个位置，通过对 8 个点进行寻找，找到更合适的位置</p>
<p>词向量的大小是 512</p>
<p>假设你的任务，视频向量是 5120，80</p>
<p>对计算机的性能提出了要求</p>
</blockquote>
<h3 id="九、Position-Embedding"><a href="#九、Position-Embedding" class="headerlink" title="九、Position Embedding"></a>九、Position Embedding</h3><p>attention：</p>
<blockquote>
<p>优点：</p>
<ol>
<li>解决了长序列依赖问题</li>
<li>可以并行</li>
</ol>
<p>缺点：</p>
<ol>
<li><p>开销变大了</p>
</li>
<li><p>既然可以并行，也就是说，词与词之间不存在顺序关系（打乱一句话，这句话里的每个词的词向量依然不会变），即无位置关系（既然没有，我就加一个，通过位置编码的形式加）</p>
</li>
</ol>
<p>位置编码的问题</p>
<p>为什么需要位置编码？</p>
<p>RNN和LSTM包括位置编码，因为是一个位置一个位置传过来的。</p>
</blockquote>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230528214058094.png" alt="image-20230528214058094"></p>
<p>如上图所示，为了解决 Attention 丢失的序列顺序信息，Transformer 的提出者提出了 Position Embedding，也就是对于输入 $X$ 进行 Attention 计算之前，在 $X$ 的词向量中加上位置信息，也就是说 $X$ 的词向量为 $X_{final_embedding} &#x3D; Embedding + Positional, Embedding$</p>
<p>但是如何得到 $X$ 的位置向量呢？</p>
<p>其中位置编码公式如下图所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/位置编码公式.png" style="zoom:33%;">

<p>其中 pos 表示位置、i 表示维度、$d_{model}$表示位置向量的向量维度 、$2i、2i+1$ 表示的是奇偶数（奇偶维度），上图所示就是偶数位置使用 $\sin$ 函数，奇数位置使用 $\cos$ 函数。</p>
<p>有了位置编码，我们再来看看位置编码是如何嵌入单词编码的（其中 512 表示编码维度），<strong>通过把单词的词向量和位置向量进行叠加，这种方式就称作位置嵌入</strong>，如下图所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/位置编码和词向量之和.png" style="zoom:50%;">

<p><strong>Position Embedding 本身是一个绝对位置的信息</strong>，但在语言模型中，相对位置也很重要。那么为什么位置嵌入机制有用呢？</p>
<p>我们不要去关心三角函数公式，可以看看下图公式（3）中的第一行，我们做如下的解释，对于 “我爱吃苹果” 这一句话，有 5 个单词，假设序号分别为 1、2、3、4、5。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/位置嵌入解释.png" style="zoom:50%;">

<p>假设 $pos&#x3D;1&#x3D;我、k&#x3D;2&#x3D;爱、pos+k&#x3D;3&#x3D;吃$，也就是说 $pos+k&#x3D;3$ 位置的位置向量的某一维可以通过 $pos&#x3D;1$ 位置的位置向量的某一维线性组合加以线性表示，通过该线性表示可以得出 “吃” 的位置编码信息蕴含了相对于前两个字 “我” 的位置编码信息。</p>
<p>总而言之就是，<strong>某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。</strong></p>
<blockquote>
<p>sin(pos+k) &#x3D; sin(pos)*cos(k) + cos(pos) *sin(k)  #sin表示的是偶数维度</p>
<p>cos(pos+k) &#x3D; cos(pos) cos(k) - sin(pos)sin(k)  #cos表示的是奇数维度</p>
<p>他特别在pos+k是pos和k的线性组合</p>
<p>我爱你，现在我做第三个词”你“的位置编码</p>
<p>pos &#x3D; 3 &#x3D; 1+2</p>
<p>pos+k &#x3D; 3 &#x3D; 1+2 &#x3D; 1*2+ 1 *2</p>
<p>这句话变成”你爱我“，现在我们仍然做第三个词”我“的位置编码</p>
<p>pos &#x3D; 3 &#x3D;1+2</p>
<p>pos+k &#x3D; 3 &#x3D; 1+2 &#x3D; 1*2+ 1 *2</p>
</blockquote>
<h3 id="十、Transformer"><a href="#十、Transformer" class="headerlink" title="十、Transformer"></a>十、Transformer</h3><blockquote>
<p>预训练–》NNLM–》word2Vec–》ELMo–》Attention</p>
<p>NLP 中预训练的目的，其实就是为了生成词向量</p>
<p>顺水推舟，transformer 其实就是 attention 的一个堆叠</p>
<p>transformer其实就是一个seq2seq模型，序列（编码器）到序列（解码器）</p>
</blockquote>
<h4 id="10-1-Transformer-的结构"><a href="#10-1-Transformer-的结构" class="headerlink" title="10.1 Transformer 的结构"></a>10.1 Transformer 的结构</h4><p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230529182407125.png" alt="image-20230529182407125"></p>
<p>上图所示的整体框架乍一眼一看非常复杂，由于 Transformer 起初是作为翻译模型，因此我们以翻译举例，简化一下上述的整体框架：</p>
<blockquote>
<p>通过机器翻译来做解释</p>
<p>给一个输入，给出一个输出（输出是输入的翻译的结果）</p>
<p>“我是一个学生”  –》（通过 Transformer） I am a student</p>
</blockquote>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-框架简化.jpg" style="zoom:50%;">

<blockquote>
<p>编码器和解码器</p>
<p>编码器：把输入变成一个词向量（Self-Attetion）</p>
<p>解码器：得到编码器输出的词向量后，生成翻译的结果</p>
<p>Nx 的意思是，编码器里面又有 N 个小编码器（默认 N&#x3D;6）</p>
<p>通过 6 个编码器，对词向量一步又一步的强化（增强）</p>
</blockquote>
<p>从上图可以看出 Transformer 相当于一个黑箱，左边输入 “Je suis etudiant”，右边会得到一个翻译结果 “I am a student”。</p>
<p>再往细里讲，Transformer 也是一个 Seq2Seq 模型（Encoder-Decoder 框架的模型），左边一个 Encoders 把输入读进去，右边一个 Decoders 得到输出，如下所示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-ed-框架.jpg" style="zoom:50%;">

<blockquote>
<p>在这里，我们穿插描述下 Encoder-Decoder 框架的模型是如何进行文本翻译的：</p>
<ol>
<li>将序列 $(x_1,x_2,\cdots,x_n)$ 作为 Encoders 的输入，得到输出序列 $(z_1,z_2,\cdots,z_n)$</li>
<li>把 Encoders 的输出序列 $(z_1,z_2,\cdots,z_n)$ 作为 Decoders 的输入，生成一个输出序列 $(y_1,y_2,\cdots,y_m)$。注：<strong>Decoders 每个时刻输出一个结果</strong></li>
</ol>
</blockquote>
<p>第一眼看到上述的 Encodes-Decoders 框架图，随之产生问题就是 Transformer 中 左边 Encoders 的输出是怎么和右边 Decoders 结合的。因为decoders 里面是有N层的，再画张图直观的看就是这样：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-ed-复杂.jpg" style="zoom:50%;">

<p>也就是说，Encoders 的输出，会和<strong>每一层的 Decoder 进行结合</strong>。</p>
<p>现在我们取其中一层进行详细的展示：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/ed-细分.jpg" style="zoom:50%;">

<blockquote>
<p>FFN（Feed Forward）：w2(（w1x+b1）)+b2</p>
</blockquote>
<p>通过上述分析，发现我们想要详细了解 Transformer，只要了解 Transformer 中的 Encoder 和 Decoder 单元即可，接下来我们将详细阐述这两个单元。</p>
<h4 id="10-2-Encoder"><a href="#10-2-Encoder" class="headerlink" title="10.2 Encoder"></a>10.2 Encoder</h4><blockquote>
<p>编码器包括两个子层，Self-Attention、Feed Forward</p>
<p>每一个子层的传输过程中都会有一个（残差网络+归一化）</p>
</blockquote>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/ed-细分-16853561510629.jpg" style="zoom:50%;">

<p>有了上述那么多知识的铺垫，我们知道 Eecoders 是 N&#x3D;6 层，通过上图我们可以看到每层 Encoder 包括两个 sub-layers：</p>
<ul>
<li>第一个 sub-layer 是 multi-head self-attention，用来计算输入的 self-attention；</li>
<li>第二个 sub-layer 是简单的前馈神经网络层 Feed Forward；</li>
</ul>
<p>注意：在每个 sub-layer 我们都模拟了残差网络（在下面的数据流示意图中会细讲），每个sub-layer的输出都是 $LayerNorm(x+Sub_layer(x))$，其中 $sub_layer$ 表示的是该层的上一层的输出</p>
<p>现在我们给出 Encoder 的数据流示意图，一步一步去剖析</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/encoder-详细图.png" style="zoom:50%;">

<blockquote>
<p>Thinking</p>
<p>–》得到绿色的 x1（词向量，可以通过 one-hot、word2vec 得到）+ 叠加位置编码（给 x1 赋予位置属性）得到黄色的 x1</p>
<p>–》输入到 Self-Attention 子层中，做注意力机制（x1、x2 拼接起来的一句话做），得到 z1（x1 与 x1，x2拼接起来的句子做了自注意力机制的词向量，表征的仍然是 thinking），也就是说 z1 拥有了位置特征、句法特征、语义特征的词向量</p>
<p>–》残差网络（避免梯度消失，w3(w2(w1x+b1)+b2)+b3，如果 w1，w2，w3 特别小，0.0000000000000000……1，x 就没了，【w3(w2(w1x+b1)+b2)+b3+x】），归一化（LayerNorm），做标准化（避免梯度爆炸），得到了深粉色的 z1</p>
<p>–》Feed Forward，Relu（w2(w1x+b1)+b2），（前面每一步都在做线性变换，wx+b，线性变化的叠加永远都是线性变化（线性变化就是空间中平移和扩大缩小），通过 Feed Forward中的 Relu 做一次非线性变换，这样的空间变换可以无限拟合任何一种状态了），得到 r1（是 thinking 的新的表征）</p>
<p>总结下（这是重点，上面听不懂都没关系）：做词向量，只不过这个词向量更加优秀，让这个词向量能够更加精准的表示这个单词、这句话</p>
</blockquote>
<ol>
<li>深绿色的 $x_1$ 表示 Embedding 层的输出，加上代表 Positional Embedding 的向量之后，得到最后输入 Encoder 中的特征向量，也就是浅绿色向量 $x_1$；</li>
<li>浅绿色向量 $x_1$ 表示单词 “Thinking” 的特征向量，其中 $x_1$ 经过 Self-Attention 层，变成浅粉色向量 $z_1$；</li>
<li>$x_1$ 作为残差结构的直连向量，直接和 $z_1$ 相加，之后进行 Layer Norm 操作，得到粉色向量 $z_1$；<ol>
<li>残差结构的作用：避免出现梯度消失的情况</li>
<li>Layer Norm 的作用：为了保证数据特征分布的稳定性，并且可以加速模型的收敛</li>
</ol>
</li>
<li>$z_1$ 经过前馈神经网络（Feed Forward）层，经过残差结构与自身相加，之后经过 LN 层，得到一个输出向量 $r_1$；<ol>
<li>该前馈神经网络包括两个线性变换和一个ReLU激活函数：$FFN(x) &#x3D; max(0,xW_1+b_1)W_2+b2$</li>
</ol>
</li>
<li>由于 Transformer 的 Encoders 具有 6 个 Encoder，**$r_1$ 也将会作为下一层 Encoder 的输入，代替 $x_1$ 的角色**，如此循环，直至最后一层 Encoder。</li>
</ol>
<p>需要注意的是，<strong>上述的 $x、z、r$ 都具有相同的维数</strong>，论文中为 512 维。</p>
<h4 id="10-3-Decoder"><a href="#10-3-Decoder" class="headerlink" title="10.3 Decoder"></a>10.3 Decoder</h4><blockquote>
<p>解码器会接收编码器生成的词向量，然后通过这个词向量去生成翻译的结果。</p>
<p>解码器的 Self-Attention 在编码已经生成的单词</p>
<p>假如目标词“我是一个学生”—》masked Self-Attention</p>
<p>训练阶段：目标词“我是一个学生”是已知的，然后 Self-Attention 是对“我是一个学生” 做计算</p>
<p>如果不做 masked，每次训练阶段，都会获得全部的信息</p>
<p>如果做 masked，Self-Attention 第一次对“我”做计算</p>
<p>Self-Attention 第二次对“我是”做计算</p>
<p>……</p>
<p>测试阶段：</p>
<ol>
<li>目标词未知，假设目标词是“我是一个学生”（未知），Self-Attention 第一次对“我”做计算</li>
<li>第二次对“我是”做计算</li>
<li>……</li>
</ol>
<p>而测试阶段，没生成一点，获得一点</p>
</blockquote>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/ed-细分-168535653724714.jpg" style="zoom:50%;">

<p>Decoders 也是 N&#x3D;6 层，通过上图我们可以看到每层 Decoder 包括 3 个 sub-layers：</p>
<ul>
<li>第一个 sub-layer 是 Masked multi-head self-attention，也是计算输入的 self-attention；<ul>
<li>在这里，先不解释为什么要做 Masked，后面在 “Transformer 动态流程展示” 这一小节会解释</li>
</ul>
</li>
<li>第二个 sub-layer 是 Encoder-Decoder Attention 计算，对 Encoder 的输入和 Decoder 的Masked multi-head self-attention 的输出进行 attention 计算；<ul>
<li>在这里，同样不解释为什么要对 Encoder 和 Decoder 的输出一同做 attention 计算，后面在 “Transformer 动态流程展示” 这一小节会解释</li>
</ul>
</li>
<li>第三个 sub-layer 是前馈神经网络层，与 Encoder 相同。</li>
</ul>
<h4 id="10-4-Transformer-输出结果"><a href="#10-4-Transformer-输出结果" class="headerlink" title="10.4 Transformer 输出结果"></a>10.4 Transformer 输出结果</h4><p>以上，就讲完了 Transformer 编码和解码两大模块，那么我们回归最初的问题，将 “机器学习” 翻译成 “machine learing”，解码器的输出是一个浮点型的向量，怎么转化成 “machine learing” 这两个词呢？让我们来看看 Encoders 和 Decoders 交互的过程寻找答案：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/ed-交互.jpg" style="zoom:50%;">

<blockquote>
<p>Linear 层转换成词表的维度</p>
<p>softmax 得到最大词的概率</p>
</blockquote>
<p>从上图可以看出，Transformer 最后的工作是让解码器的输出通过线性层 Linear 后接上一个 softmax</p>
<ul>
<li>其中线性层是一个简单的全连接神经网络，它将解码器产生的向量 A 投影到一个更高维度的向量 B 上，假设我们模型的词汇表是10000个词，那么向量 B 就有10000个维度，每个维度对应一个惟一的词的得分。</li>
<li>之后的softmax层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出啦！</li>
</ul>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-最后输出.jpg" style="zoom:50%;">

<p>假设词汇表维度是 6，那么输出最大概率词汇的过程如下：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-最终输出结果.jpg" style="zoom:50%;">



<h3 id="十一、Transformer-动态流程展示"><a href="#十一、Transformer-动态流程展示" class="headerlink" title="十一、Transformer 动态流程展示"></a>十一、Transformer 动态流程展示</h3><p>首先我们来看看拿 Transformer 作翻译时，如何生成翻译结果的：</p>
<p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/Users/linli/Desktop/tf-动态生成.gif" alt="img"></p>
<p>继续进行：</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-动态结果-2.gif" style="zoom:50%;">

<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/tf-整体框架.jpg" style="zoom:33%;">

<p>假设上图是训练模型的某一个阶段，我们来结合 Transformer 的完整框架描述下这个动态的流程图：</p>
<ol>
<li>输入 “je suis etudiant” 到 Encoders，然后得到一个 $K_e$、$V_e$ 矩阵；</li>
<li>输入 “I am a student” 到 Decoders ，首先通过 Masked Multi-head Attention 层得到 “I am a student” 的 attention 值 $Q_d$，然后用 attention 值 $Q_d$ 和 Encoders 的输出 $K_e$、$V_e$ 矩阵进行 attention 计算，得到第 1 个输出 “I”；</li>
<li>输入 “I am a student” 到 Decoders ，首先通过 Masked Multi-head Attention 层得到 “I am a student” 的 attention 值 $Q_d$，然后用 attention 值 $Q_d$ 和 Encoders 的输出 $K_e$、$V_e$ 矩阵进行 attention 计算，得到第 2 个输出 “am”；</li>
<li>……</li>
</ol>
<p>现在我们来解释我们之前遗留的两个问题。</p>
<h4 id="11-1-为什么-Decoder-需要做-Mask"><a href="#11-1-为什么-Decoder-需要做-Mask" class="headerlink" title="11.1 为什么 Decoder 需要做 Mask"></a>11.1 为什么 Decoder 需要做 Mask</h4><blockquote>
<p>机器翻译：源语句（我爱中国），目标语句（I love China）</p>
<p>为了解决训练阶段和测试阶段的 gap（不匹配）</p>
<p>训练阶段：解码器会有输入，这个输入是目标语句，就是 I love China，通过已经生成的词，去让解码器更好的生成（每一次都会把所有信息告诉解码器）</p>
<p>测试阶段：解码器也会有输入，但是此时，测试的时候是不知道目标语句是什么的，这个时候，你每生成一个词，就会有多一个词放入目标语句中，每次生成的时候，都是已经生成的词（测试阶段只会把已经生成的词告诉解码器）</p>
<p>为了匹配，为了解决这个 gap，masked Self-Attention 就登场了，我在训练阶段，我就做一个 masked，当你生成第一个词，我啥也不告诉你，当你生成第二个词，我告诉第一个词</p>
</blockquote>
<ul>
<li><p>训练阶段：我们知道 “je suis etudiant” 的翻译结果为 “I am a student”，我们把 “I am a student” 的 Embedding 输入到 Decoders 里面，翻译第一个词 “I” 时</p>
<ul>
<li>如果对 “I am a student”  attention 计算不做 mask，“am，a，student” 对 “I” 的翻译将会有一定的贡献</li>
<li>如果对 “I am a student”  attention 计算做 mask，“am，a，student” 对 “I” 的翻译将没有贡献</li>
</ul>
</li>
<li><p>测试阶段：我们不知道 “我爱中国” 的翻译结果为 “I love China”，我们只能随机初始化一个 Embedding 输入到 Decoders 里面，翻译第一个词 “I” 时：</p>
<ul>
<li>无论是否做 mask，“love，China” 对 “I” 的翻译都不会产生贡献</li>
<li>但是翻译了第一个词 “I” 后，随机初始化的 Embedding 有了 “I” 的 Embedding，也就是说在翻译第二词 “love” 的时候，“I” 的 Embedding 将有一定的贡献，但是 “China” 对 “love” 的翻译毫无贡献，随之翻译的进行，<strong>已经翻译的结果将会对下一个要翻译的词都会有一定的贡献，这就和做了 mask 的训练阶段做到了一种匹配</strong></li>
</ul>
</li>
</ul>
<p>总结下就是：Decoder 做 Mask，是为了让训练阶段和测试阶段行为一致，不会出现间隙，避免过拟合</p>
<h4 id="11-2-为什么-Encoder-给予-Decoders-的是-K、V-矩阵"><a href="#11-2-为什么-Encoder-给予-Decoders-的是-K、V-矩阵" class="headerlink" title="11.2 为什么 Encoder 给予 Decoders 的是 K、V 矩阵"></a>11.2 为什么 Encoder 给予 Decoders 的是 K、V 矩阵</h4><blockquote>
<p>Q来源解码器，K&#x3D;V来源于编码器</p>
<p>Q是查询变量，Q 是已经生成的词</p>
<p>K&#x3D;V 是源语句</p>
<p>当我们生成这个词的时候，通过已经生成的词和源语句做自注意力，就是确定源语句中哪些词对接下来的词的生成更有作用，首先他就能找到当前生成词</p>
<p>我爱中国</p>
<p>通过部分（生成的词）去全部（源语句）的里面挑重点</p>
<p>Q 是源语句，K，V 是已经生成的词，源语句去已经生成的词里找重点 ，找信息，已经生成的词里面压根就没有下一个词</p>
</blockquote>
<p>我们在讲解 Attention 机制中曾提到，Query 的目的是借助它从一堆信息中找到重要的信息。</p>
<p>现在 Encoder 提供了 $K_e、V_e$ 矩阵，Decoder 提供了 $Q_d$ 矩阵，通过  “我爱中国” 翻译为 “I love China” 这句话详细解释下。</p>
<p><strong>当我们翻译 “I” 的时候，由于 Decoder 提供了 $Q_d$ 矩阵，通过与 $K_e、V_e$ 矩阵的计算，它可以在 “我爱中国” 这四个字中找到对 “I” 翻译最有用的单词是哪几个，并以此为依据翻译出 “I” 这个单词，这就很好的体现了注意力机制想要达到的目的，把焦点放在对自己而言更为重要的信息上。</strong></p>
<ul>
<li>其实上述说的就是 Attention 里的 soft attention机制，解决了曾经的 Encoder-Decoder 框架的一个问题，在这里不多做叙述，有兴趣的可以参考网上的一些资料。<ul>
<li>早期的 Encoder-Decoder 框架中的 Encoder 通过 LSTM 提取出源句（Source） “我爱中国” 的特征信息 C，然后 Decoder 做翻译的时候，目标句（Target）“I love China” 中的任何一个单词的翻译都来源于相同特征信息 C，这种做法是极其不合理的，例如翻译 “I” 时应该着眼于 “我”，翻译 “China” 应该着眼于 “中国”，而早期的这种做法并没有体现出，然而 Transformer 却通过 Attention 的做法解决了这个问题。</li>
</ul>
</li>
</ul>
<blockquote>
<p>解决了以前的 seq2seq 框架的问题</p>
<p>lstm 做编码器（得到词向量 C），再用 lstm 做解码器做生成</p>
<p>用这种方法去生成词，每一次生成词，都是通过 C 的全部信息去生成</p>
<p>很多信息对于当前生成词而言都是没有意义的</p>
</blockquote>
<h3 id="十二、GPT-模型"><a href="#十二、GPT-模型" class="headerlink" title="十二、GPT 模型"></a>十二、GPT 模型</h3><p><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/image-20230529192819243.png" alt="image-20230529192819243"></p>
<blockquote>
<p>BERT和GPT其实和ELMo基本类似</p>
<p>ELMo的基本模型是Lstm，BERT和GPT的基本模型是Transformer</p>
<p>BERT的本质是通过transformer的编码器(encoder)去认识万事万物，然后通过下游任务的改造去发觉这个物与物之间的联系，去达到你想做到的目的</p>
<p>GPT的本质是通过transformer的解码器Decoder(mask self-attention)去做生成式的任务</p>
<p>缺点：训练参数量特别大</p>
</blockquote>
<h4 id="12-1-GPT-模型的预训练"><a href="#12-1-GPT-模型的预训练" class="headerlink" title="12.1 GPT 模型的预训练"></a>12.1 GPT 模型的预训练</h4><p>在讲解 ELMo 的时候，我们说到 ELMo 这一类预训练的方法被称为 “Feature-based Pre-Training”。并且如果把 ELMo 这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。</p>
<p>除了以 ELMo 为代表的这种基于特征融合的预训练方法外，NLP 里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为 “基于Fine-tuning的模式”，而 GPT 就是这一模式的典型开创者，下面先让我们看看 GPT 的网络结构。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/gpt-模型.jpg" style="zoom:50%;">

<p>GPT 是 “Generative Pre-Training” 的简称，从名字看其含义是指的生成式的预训练。</p>
<p>GPT也采用两阶段过程：</p>
<ol>
<li>第一个阶段：利用语言模型进行预训练；</li>
<li>第二个阶段：通过 Fine-tuning 的模式解决下游任务。</li>
</ol>
<p>上图展示了 GPT 的预训练过程，其实和 ELMo 是类似的，主要不同在于两点：</p>
<ol>
<li>首先，特征抽取器用的不是 RNN，而是用的 Transformer，它的特征抽取能力要强于RNN，这个选择很明显是很明智的；</li>
<li>其次，<ol>
<li>GPT 的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓 “单向” 的含义是指：语言模型训练的任务目标是根据 $w_i$ 单词的上下文去正确预测单词 $w_i$ ， $w_i$ 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。</li>
<li>ELMo 在做语言模型预训练的时候，预测单词 $w_i$ 同时使用了上文和下文，而 GPT 则只采用 Context-before 这个单词的上文来进行预测，而抛开了下文。</li>
<li>GPT 这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到 Word Embedding 中，是很吃亏的，白白丢掉了很多信息。</li>
</ol>
</li>
</ol>
<h4 id="12-2-GPT-模型的-Fine-tuning"><a href="#12-2-GPT-模型的-Fine-tuning" class="headerlink" title="12.2 GPT 模型的 Fine-tuning"></a>12.2 GPT 模型的 Fine-tuning</h4><p>上面讲的是 GPT 如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和 ELMO 的方式大有不同。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/gpt-fine.jpg" style="zoom:50%;">

<p>上图展示了 GPT 在第二阶段如何使用：</p>
<ol>
<li><p>首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向 GPT 的网络结构看齐，把任务的网络结构改造成和 GPT 一样的网络结构。</p>
</li>
<li><p>然后，在做下游任务的时候，利用第一步预训练好的参数初始化 GPT 的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了。</p>
</li>
<li><p>再次，你可以用手头的任务去训练这个网络，对网络参数进行 Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。</p>
</li>
</ol>
<p>这有没有让你想起最开始提到的图像领域如何做预训练的过程，对，这跟那个预训练的模式是一模一样的。</p>
<p>对于 NLP 各种花样的不同任务，怎么改造才能靠近 GPT 的网络结构呢？由于 GPT 对下游任务的改造过程和 BERT 对下游任务的改造极其类似，并且我们主要目的是为了讲解 BERT，所以这个问题将会在 BERT 那里得到回答。</p>
<blockquote>
<p>GPT对于Elmo的优点：</p>
<p>1、可以做生成式任务，Elmo只适用于提取特征（词向量） </p>
</blockquote>
<h3 id="十三、BERT-模型"><a href="#十三、BERT-模型" class="headerlink" title="十三、BERT 模型"></a>十三、BERT 模型</h3><blockquote>
<p>BERT的意义：从大量无标记数据集中训练得到的深度模型，可以显著提高各项自然语言处理任务的准确率</p>
</blockquote>
<h4 id="13-1-BERT：公认的里程碑"><a href="#13-1-BERT：公认的里程碑" class="headerlink" title="13.1 BERT：公认的里程碑"></a>13.1 BERT：公认的里程碑</h4><p>BERT 模型可以作为公认的里程碑式的模型，但是它最大的优点不是创新，而是集大成者，并且这个集大成者有了各项突破，下面让我们看看 BERT 是怎么集大成者的。</p>
<ul>
<li>BERT 的意义在于：从大量无标记数据集中训练得到的深度模型，可以显著提高各项自然语言处理任务的准确率。</li>
<li>近年来优秀预训练语言模型的集大成者：参考了 ELMO 模型的双向编码思想、借鉴了 GPT 用 Transformer 作为特征提取器的思路、采用了 word2vec 所使用的 CBOW 方法</li>
<li>BERT 和 GPT 之间的区别：<ul>
<li>GPT：<strong>GPT 使用 Transformer Decoder 作为特征提取器、具有良好的文本生成能力</strong>，然而当前词的语义只能由其前序词决定，并且在语义理解上不足</li>
<li>BERT：使用了 Transformer Encoder 作为特征提取器，并使用了与其配套的掩码训练方法。<strong>虽然使用双向编码让 BERT 不再具有文本生成能力，但是 BERT 的语义信息提取能力更强</strong></li>
</ul>
</li>
<li>单向编码和双向编码的差异，以该句话举例 “今天天气很{}，我们不得不取消户外运动”，分别从单向编码和双向编码的角度去考虑 {} 中应该填什么词：<ul>
<li>单向编码：单向编码只会考虑 “今天天气很”，以人类的经验，大概率会从 “好”、“不错”、“差”、“糟糕” 这几个词中选择，这些词可以被划为截然不同的两类</li>
<li>双向编码：<strong>双向编码会同时考虑上下文的信息</strong>，即除了会考虑 “今天天气很” 这五个字，还会考虑 “我们不得不取消户外运动” 来帮助模型判断，则大概率会从 “差”、“糟糕” 这一类词中选择</li>
</ul>
</li>
</ul>
<h4 id="13-2-BERT-的结构：强大的特征提取能力"><a href="#13-2-BERT-的结构：强大的特征提取能力" class="headerlink" title="13.2 BERT 的结构：强大的特征提取能力"></a>13.2 BERT 的结构：强大的特征提取能力</h4><blockquote>
<p>ELMo、GPT 和 BERT 三者的区别：</p>
<p>BERT是真正意义上的获取上下文，得到很准确的词向量。（不能做生成）</p>
<p>ELMo是双层LSTM结构，在获取上文的时候是看不到下文的信息的，LSTM是序列型的架构，只能看到一个方向的东西。随着上下文增多，会梯度消失（200个词左右）</p>
<p>GPT利用Transformer Block 取代 LSTM 作为特征提取器，实现了单向编码，可以解决生成问题。</p>
</blockquote>
<ul>
<li><p>如下图所示，我们来看看 ELMo、GPT 和 BERT 三者的区别</p>
<ul>
<li><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/BERT-GPT-比较.png" style="zoom:50%;"></li>
<li>ELMo 使用自左向右编码和自右向左编码的两个 LSTM 网络，分别以 $P(w_i|w_1,\cdots,w_{i-1})$ 和 $P(w_i|w_{i+1},\cdots,w_n)$ 为目标函数独立训练，<strong>将训练得到的特征向量以拼接的形式实现双向编码，本质上还是单向编码，只不过是两个方向上的单向编码的拼接而成的双向编码</strong>。</li>
<li>GPT 使用 Transformer Decoder 作为 Transformer Block，以 $P(w_i|w_1,\cdots,w_{i-1})$ 为目标函数进行训练，<strong>用 Transformer Block 取代 LSTM 作为特征提取器，实现了单向编码，是一个标准的预训练语言模型，即使用 Fine-Tuning 模式解决下游任务。</strong></li>
<li>BERT 也是一个标准的预训练语言模型，<strong>它以 $P(w_i|w_1,\cdots,w_{i-1},w_{i+1},\cdots,w_n)$ 为目标函数进行训练，BERT 使用的编码器属于双向编码器</strong>。<ul>
<li>BERT 和 ELMo 的区别在于使用 Transformer Block 作为特征提取器，加强了语义特征提取的能力；</li>
<li>BERT 和 GPT 的区别在于使用 Transformer Encoder 作为 Transformer Block，并且将 GPT 的单向编码改成双向编码，也就是说 BERT 舍弃了文本生成能力，换来了更强的语义理解能力。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>BERT 的模型结构如下图所示：</p>
<blockquote>
<p>图上虽然只画了两层，但实际上有12层或者24层，ELMo就2层的LSTM，解释为句法特征和语法特征，但是BERT有24层，可以学到更多的特征，是无法用语言解释的。</p>
</blockquote>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/bert-模型结构.jpg" style="zoom:51%;">

<p>从上图可以发现，BERT 的模型结构其实就是 Transformer Encoder 模块的堆叠。在模型参数选择上，论文给出了两套大小不一致的模型。</p>
<blockquote>
<p>$BERT_{BASE}$ ：L &#x3D; 12，H &#x3D; 768，A &#x3D; 12，总参数量为 1.1 亿</p>
<p>$BERT_{LARGE}$：L &#x3D; 24，H &#x3D; 1024，A &#x3D; 16，总参数量为 3.4 亿</p>
</blockquote>
<p>其中 L 代表 Transformer Block 的层数；H 代表特征向量的维数（此处默认 Feed Forward 层中的中间隐层的维数为 4H）；A 表示 Self-Attention 的头数，使用这三个参数基本可以定义 BERT的量级。</p>
<p>BERT 参数量级的计算公式：<br>$$<br>\begin{align*}<br>&amp; 词向量参数+ 12 * （Multi-Heads参数 + 全连接层参数 + layernorm参数）\<br>&amp; &#x3D; （30522+512 + 2）* 768 + 768 * 2 \<br>&amp; + 12 * （768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2） \<br>&amp; &#x3D; 108808704.0 \<br>&amp; \approx 110M<br>\end{align*}<br>$$<br>训练过程也是很花费计算资源和时间的，<strong>总之表示膜拜，普通人即便有 idea 没有算力也只能跪着。</strong></p>
<h4 id="13-3-BERT-之无监督训练"><a href="#13-3-BERT-之无监督训练" class="headerlink" title="13.3 BERT 之无监督训练"></a>13.3 BERT 之无监督训练</h4><p>和 GPT 一样，BERT 也采用二段式训练方法：</p>
<ol>
<li>第一阶段：使用易获取的大规模无标签余料，来训练基础语言模型；</li>
<li>第二阶段：根据指定任务的少量带标签训练数据进行微调训练。</li>
</ol>
<p>不同于 GPT 等标准语言模型使用 $P(w_i|w_1,\cdots,w_{i-1})$ 为目标函数进行训练，能看到全局信息的 BERT 使用 $P(w_i|w_1,\cdots,w_{i-1},w_{i+1},\cdots,w_n)$ 为目标函数进行训练。</p>
<p>并且 BERT 用语言掩码模型（MLM）方法训练词的语义理解能力；用下句预测（NSP）方法训练句子之间的理解能力，从而更好地支持下游任务。</p>
<h4 id="13-4-BERT之语言掩码模型（MLM）"><a href="#13-4-BERT之语言掩码模型（MLM）" class="headerlink" title="13.4 BERT之语言掩码模型（MLM）"></a>13.4 BERT之语言掩码模型（MLM）</h4><blockquote>
<p>无法使用完形填空的思想，CBOW，预测中间这个词是什么</p>
<p>对于一句话，我输入的时候mask掉15%的词（token)，然后让模型预测这个token是什么（训练阶段的时候才会这样做，但是测试阶段没有mask的词）</p>
<p>测试阶段给你一句话，你就要给我一句话的句向量</p>
</blockquote>
<p>BERT 作者认为，<strong>使用自左向右编码和自右向左编码的单向编码器拼接而成的双向编码器，在性能、参数规模和效率等方面，都不如直接使用深度双向编码器强大</strong>，这也是为什么 BERT 使用 Transformer Encoder 作为特征提取器，而不使用自左向右编码和自右向左编码的两个 Transformer Decoder作为特征提取器的原因。</p>
<p>由于无法使用标准语言模型的训练模式，<strong>BERT 借鉴完形填空任务和 CBOW 的思想</strong>，使用语言掩码模型（MLM ）方法训练模型。</p>
<p>MLM 方法也就是随机去掉句子中的部分 token（单词），然后模型来预测被去掉的 token 是什么。<strong>这样实际上已经不是传统的神经网络语言模型(类似于生成模型)了，而是单纯作为分类问题</strong>，根据这个时刻的 hidden state 来预测这个时刻的 token 应该是什么，而不是预测下一个时刻的词的概率分布了。</p>
<p>随机去掉的 token 被称作掩码词，在训练中，掩码词将以 15% 的概率被替换成 [MASK]，也就是说随机 mask 语料中 15% 的 token，这个操作则称为掩码操作。注意：<strong>在CBOW 模型中，每个词都会被预测一遍。</strong></p>
<p>但是这样设计 MLM 的训练方法会引入弊端：<strong>在模型微调训练阶段或模型推理（测试）阶段，输入的文本中将没有 [MASK]，进而导致产生由训练和预测数据偏差导致的性能损失。</strong></p>
<blockquote>
<p>之前的做法，会让模型所有精力全部聚焦在mask上面，对于其他的词不管了</p>
<p>用了下述的方法，所有词都有可能是mask词，这样模型会把精力聚焦在大部分词上（包括mask)</p>
</blockquote>
<p>考虑到上述的弊端，BERT 并没有总用 [MASK] 替换掩码词，而是按照一定比例选取替换词。在选择 15% 的词作为掩码词后这些掩码词有三类替换选项：</p>
<ul>
<li>80% 训练样本中：将选中的词用 [MASK] 来代替，例如：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“地球是[MASK]八大行星之一”</span><br></pre></td></tr></table></figure>

<ul>
<li>10% 的训练样本中：选中的词不发生变化，<strong>该做法是为了缓解训练文本和预测文本的偏差带来的性能损失</strong>，例如：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“地球是太阳系八大行星之一”</span><br></pre></td></tr></table></figure>

<ul>
<li>10% 的训练样本中：将选中的词用任意的词来进行代替，<strong>该做法是为了让 BERT 学会根据上下文信息自动纠错</strong>，例如：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“地球是苹果八大行星之一”</span><br></pre></td></tr></table></figure>

<p>作者在论文中提到这样做的好处是，编码器不知道哪些词需要预测的，哪些词是错误的，因此<strong>被迫需要学习每一个 token 的表示向量</strong>，另外作者也<strong>表示双向编码器比单项编码器训练要慢</strong>，进而导致BERT 的训练效率低了很多，但是实验也证明 MLM 训练方法可以让 BERT 获得超出同期所有预训练语言模型的语义理解能力，牺牲训练效率是值得的。</p>
<h4 id="13-5-BERT-之下句预测（NSP）"><a href="#13-5-BERT-之下句预测（NSP）" class="headerlink" title="13.5 BERT 之下句预测（NSP）"></a>13.5 BERT 之下句预测（NSP）</h4><p>在很多自然语言处理的下游任务中，如问答和自然语言推断，都基于两个句子做逻辑推理，而语言模型并不具备直接捕获句子之间的语义联系的能力，或者可以说成<strong>单词预测粒度的训练到不了句子关系这个层级</strong>，为了<strong>学会捕捉句子之间的语义联系</strong>，BERT 采用了下句预测（NSP ）作为无监督预训练的一部分。</p>
<p>NSP 的具体做法是，BERT 输入的语句将由两个句子构成，其中，50% 的概率将语义连贯的两个连续句子作为训练文本（<strong>连续句对一般选自篇章级别的语料，以此确保前后语句的语义强相关</strong>），另外 50% 的概率将完全随机抽取两个句子作为训练文本。</p>
<blockquote>
<p>连续句对：[CLS]今天天气很糟糕[SEP]下午的体育课取消了[SEP]</p>
<p>随机句对：[CLS]今天天气很糟糕[SEP]鱼快被烤焦啦[SEP]</p>
</blockquote>
<p>其中 [SEP]  标签表示分隔符。 [CLS] 表示标签用于类别预测，结果为 1，表示输入为连续句对；结果为 0，表示输入为随机句对。</p>
<p>通过训练 [CLS] 编码后的输出标签，<strong>BERT 可以学会捕捉两个输入句对的文本语义</strong>，在连续句对的预测任务中，BERT 的正确率可以达到 97%-98%。</p>
<blockquote>
<p>BERT经过transformer的编码器之后，每个CLS都包含了句子的信息</p>
</blockquote>
<h4 id="13-6-BERT-之输入表示"><a href="#13-6-BERT-之输入表示" class="headerlink" title="13.6 BERT 之输入表示"></a>13.6 BERT 之输入表示</h4><blockquote>
<p>BERT的位置编码是通过训练得到的。</p>
</blockquote>
<p>BERT 在预训练阶段使用了前文所述的两种训练方法，<strong>在真实训练中一般是两种方法混合使用</strong>。</p>
<p>由于 BERT 通过 Transformer 模型堆叠而成，所以 BERT 的输入需要两套 Embedding 操作：</p>
<ol>
<li>一套为 One-hot 词表映射编码（对应下图的 Token Embeddings）；</li>
<li>另一套为位置编码（对应下图的 Position Embeddings），<strong>不同于 Transformer 的位置编码用三角函数表示，BERT 的位置编码将在预训练过程中训练得到（训练思想类似于Word Embedding 的 Q 矩阵）</strong></li>
<li>由于在 MLM 的训练过程中，存在单句输入和双句输入的情况，因此 BERT 还需要一套区分输入语句的分割编码（对应下图的 Segment Embeddings），BERT 的分割编码也将在预训练过程中训练得到</li>
</ol>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/BERT-输入.jpg" style="zoom:51%;">

<p>对于分割编码，Segment Embeddings 层只有两种向量表示。前一个向量是把 0 赋给第一个句子中的各个 token，后一个向量是把 1 赋给第二个句子中的各个 token ；如果输入仅仅只有一个句子，那么它的 segment embedding 就是全 0，下面我们简单举个例子描述下：</p>
<p>[CLS]I like dogs[SEP]I like cats[SEP] 对应编码 0 0 0 0 0 1 1 1 1</p>
<p>[SEP]I Iike dogs and cats[SEP] 对应编码 0 0 0 0 0 0 0</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/segment-embeddings.jpg" style="zoom:50%;">

<h3 id="十四、BERT-下游任务改造"><a href="#十四、BERT-下游任务改造" class="headerlink" title="十四、BERT 下游任务改造"></a>十四、BERT 下游任务改造</h3><p>BERT 根据自然语言处理下游任务的输入和输出的形式，将微调训练支持的任务分为四类，分别是句对分类、单句分类、文本问答和单句标注，接下来我们将简要的介绍下 BERT 如何通过微调训练适应这四类任务的要求。</p>
<blockquote>
<p>BERT模型学会了句子的语义信息，接下来我们只要让他学习一些特定的任务，比如这个句子是正面的还是负面的（情感分类），这就是BERT的下游任务改造。</p>
</blockquote>
<h4 id="14-1-句对分类"><a href="#14-1-句对分类" class="headerlink" title="14.1 句对分类"></a>14.1 句对分类</h4><p>给定两个句子，判断它们的关系，称为句对分类，例如判断句对是否相似、判断后者是否为前者的答案。</p>
<p>针对句对分类任务，BERT 在预训练过程中就使用了 NSP 训练方法获得了直接捕获句对语义关系的能力。</p>
<p><strong>如下图所示，句对用 [SEP] 分隔符拼接成文本序列，在句首加入标签 [CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。</strong></p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/句对分类.jpg" style="zoom:80%;">



<p>针对二分类任务，BERT 不需要对输入数据和输出数据的结构做任何改动，直接使用与 NSP 训练方法一样的输入和输出结构就行。</p>
<p>针对多分类任务，需要在句首标签 [CLS] 的输出特征向量后接一个全连接层和 Softmax 层，保证输出维数与类别数目一致，最后通过 arg max 操作（取最大值时对应的索引序号）得到相对应的类别结果。</p>
<p>下面给出句对分相似性任务的实例：</p>
<blockquote>
<p>任务：判断句子 “我很喜欢你” 和句子 “我很中意你” 是否相似</p>
<p>输入改写：“[CLS]我很喜欢你[SEP]我很中意你”</p>
<p>取 “[CLS]” 标签对应输出：[0.02, 0.98]</p>
<p>通过 arg max 操作得到相似类别为 1（类别索引从 0 开始），即两个句子相似</p>
</blockquote>
<h4 id="14-2-单句分类"><a href="#14-2-单句分类" class="headerlink" title="14.2 单句分类"></a>14.2 单句分类</h4><p>给定一个句子，判断该句子的类别，统称为单句分类，例如判断情感类别、判断是否为语义连贯的句子。</p>
<p>针对单句二分类任务，也无须对 BERT 的输入数据和输出数据的结构做任何改动。</p>
<p><strong>如下图所示，单句分类在句首加入标签 [CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。</strong></p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/单句分类.jpg" style="zoom:80%;">

<p>同样，针对多分类任务，需要在句首标签 [CLS] 的输出特征向量后接一个全连接层和 Softmax 层，保证输出维数与类别数目一致，最后通过 argmax 操作得到相对应的类别结果。</p>
<p>下面给出语义连贯性判断任务的实例：</p>
<blockquote>
<p>任务：判断句子“海大球星饭茶吃” 是否为一句话</p>
<p>输入改写：“[CLS]海大球星饭茶吃”</p>
<p>取 “[CLS]” 标签对应输出：[0.99, 0.01]</p>
<p>通过 arg max 操作得到相似类别为 0，即这个句子不是一个语义连贯的句子</p>
</blockquote>
<h4 id="14-3-文本问答"><a href="#14-3-文本问答" class="headerlink" title="14.3 文本问答"></a>14.3 文本问答</h4><p>给定一个问句和一个蕴含答案的句子，找出答案在后这种的位置，称为文本问答，例如给定一个问题（句子 A），在给定的段落（句子 B）中标注答案的其实位置和终止位置。</p>
<p><strong>文本问答任何和前面讲的其他任务有较大的差别，无论是在优化目标上，还是在输入数据和输出数据的形式上，都需要做一些特殊的处理。</strong></p>
<p>为了标注答案的起始位置和终止位置，BERT 引入两个辅助向量 s（start，判断答案的起始位置） 和 e（end，判断答案的终止位置）。</p>
<p><strong>如下图所示，BERT 判断句子 B 中答案位置的做法是，将句子 B 中的每一个次得到的最终特征向量 $T_i’$ 经过全连接层（利用全连接层将词的抽象语义特征转化为任务指向的特征）后，分别与向量 s 和 e 求内积，对所有内积分别进行 softmax 操作，即可得到词 Tok m（$m\in [1,M]$）作为答案其实位置和终止位置的概率。最后，去概率最大的片段作为最终的答案</strong>。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/文本问答.jpg" style="zoom:80%;">

<p>文本回答任务的微调训练使用了两个技巧：</p>
<ol>
<li>用全连接层把 BERT 提取后的深层特征向量转化为用于判断答案位置的特征向量</li>
<li>引入辅助向量 s 和 e 作为答案其实位置和终止位置的基准向量，明确优化目标的方向和度量方法</li>
</ol>
<p>下面给出文本问答任务的实例：</p>
<blockquote>
<p>任务：给定问句 “今天的最高温度是多少”，在文本 “天气预报显示今天最高温度 37 摄氏度” 中标注答案的起始位置和终止位置</p>
<p>输入改写：“[CLS]今天的最高温度是多少[SEP]天气预报显示今天最高温度 37 摄氏度”</p>
<p>BERT Softmax 结果：</p>
<table>
<thead>
<tr>
<th align="center">篇章文本</th>
<th align="center">天气</th>
<th align="center">预报</th>
<th align="center">显示</th>
<th align="center">今天</th>
<th align="center">最高温</th>
<th align="center">37</th>
<th align="center">摄氏度</th>
</tr>
</thead>
<tbody><tr>
<td align="center">起始位置概率</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.04</td>
<td align="center">0.10</td>
<td align="center">0.80</td>
<td align="center">0.03</td>
</tr>
<tr>
<td align="center">终止位置概率</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.03</td>
<td align="center">0.04</td>
<td align="center">0.10</td>
<td align="center">0.80</td>
</tr>
</tbody></table>
<p>对 Softmax 的结果取 arg max，得到答案的起始位置为 6，终止位置为 7，即答案为 “37 摄氏度”</p>
</blockquote>
<h4 id="14-4-单句标注"><a href="#14-4-单句标注" class="headerlink" title="14.4 单句标注"></a>14.4 单句标注</h4><blockquote>
<p>本质和文本问答一样</p>
</blockquote>
<p>给定一个句子，标注每个次的标签，称为单句标注。例如给定一个句子，标注句子中的人名、地名和机构名。</p>
<p>单句标注任务和 BERT 预训练任务具有较大差异，但与文本问答任务较为相似。</p>
<p><strong>如下图所示，在进行单句标注任务时，需要在每个词的最终语义特征向量之后添加全连接层，将语义特征转化为序列标注任务所需的特征，单句标注任务需要对每个词都做标注，因此不需要引入辅助向量，直接对经过全连接层后的结果做 Softmax 操作，即可得到各类标签的概率分布。</strong></p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/单句标注.jpg" style="zoom:50%;">

<p>由于 BERT 需要对输入文本进行分词操作，独立词将会被分成若干子词，因此 BERT 预测的结果将会是 5 类（细分为 13 小类）：</p>
<ul>
<li>O（非人名地名机构名，O 表示 Other）</li>
<li>B-PER&#x2F;LOC&#x2F;ORG（人名&#x2F;地名&#x2F;机构名初始单词，B 表示 Begin）</li>
<li>I-PER&#x2F;LOC&#x2F;ORG（人名&#x2F;地名&#x2F;机构名中间单词，I 表示 Intermediate）</li>
<li>E-PER&#x2F;LOC&#x2F;ORG（人名&#x2F;地名&#x2F;机构名终止单词，E 表示 End）</li>
<li>S-PER&#x2F;LOC&#x2F;ORG（人名&#x2F;地名&#x2F;机构名独立单词，S 表示 Single）</li>
</ul>
<p>将 5 大类的首字母结合，可得 IOBES，这是序列标注最常用的标注方法。</p>
<p>下面给出命名实体识别（NER）任务的示例：</p>
<blockquote>
<p>任务：给定句子 “爱因斯坦在柏林发表演讲”，根据 IOBES 标注 NER 结果</p>
<p>输入改写：“[CLS]爱 因 斯坦 在 柏林 发表 演讲”</p>
<p>BERT Softmax 结果：</p>
<table>
<thead>
<tr>
<th align="center">BOBES</th>
<th align="center">爱</th>
<th align="center">因</th>
<th align="center">斯坦</th>
<th align="center">在</th>
<th align="center">柏林</th>
<th align="center">发表</th>
<th align="center">演讲</th>
</tr>
</thead>
<tbody><tr>
<td align="center">O</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.90</td>
<td align="center">0.01</td>
<td align="center">0.90</td>
<td align="center">0.90</td>
</tr>
<tr>
<td align="center">B-PER</td>
<td align="center">0.90</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="center">I-PER</td>
<td align="center">0.01</td>
<td align="center">0.90</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="center">E-PER</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.90</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="center">S-LOC</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
<td align="center">0.01</td>
</tr>
</tbody></table>
<p>对 Softmax 的结果取 arg max，得到最终地 NER 标注结果为：“爱因斯坦” 是人名；“柏林” 是地名</p>
</blockquote>
<h4 id="14-5-BERT效果展示"><a href="#14-5-BERT效果展示" class="headerlink" title="14.5 BERT效果展示"></a>14.5 BERT效果展示</h4><p>无论如何，从上述讲解可以看出，NLP 四大类任务都可以比较方便地改造成 Bert 能够接受的方式，总之不同类型的任务需要对模型做不同的修改，但是修改都是非常简单的，最多加一层神经网络即可。这其实是 Bert 的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p>
<p>但是讲了这么多，<strong>一个新模型好不好，效果才是王道。</strong>那么Bert 采用这种两阶段方式解决各种 NLP 任务效果如何？</p>
<p>在 11 个各种类型的 NLP 任务中达到目前最好的效果，某些任务性能有极大的提升。</p>
<img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/bert-效果.jpg" style="zoom:50%;">

<h3 id="十五、预训练语言模型总结"><a href="#十五、预训练语言模型总结" class="headerlink" title="十五、预训练语言模型总结"></a>十五、预训练语言模型总结</h3><img src="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/总结.jpg" style="zoom:50%;">

<p>到这里我们可以再梳理下几个模型之间的演进关系。</p>
<p>从上图可见，Bert 其实和 ELMO 及 GPT 存在千丝万缕的关系，比如如果我们把 GPT 预训练阶段换成双向语言模型，那么就得到了 Bert；而如果我们把 ELMO 的特征抽取器换成 Transformer，那么我们也会得到Bert。</p>
<p>所以你可以看出：Bert最关键两点，一点是特征抽取器采用 Transformer；第二点是预训练的时候采用双向语言模型。</p>
<p>那么新问题来了：对于 Transformer 来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看 ELMO 的网络结构图，只需要把两个 LSTM 替换成两个 Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。</p>
<p>当然这是我自己的改造，Bert 没这么做。那么 Bert 是怎么做的呢？我们前面不是提过 Word2Vec 吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个 CBOW 训练方法，所谓写作时候埋伏笔的 “草蛇灰线，伏脉千里”，大概就是这个意思吧？</p>
<p>前面提到了 CBOW 方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文 Context-Before 和下文 Context-afte r去预测单词。</p>
<p>其实 Bert 怎么做的？Bert 就是这么做的。从这里可以看到方法间的继承关系。当然 Bert 作者没提 Word2Vec 及 CBOW 方法，这是我的判断，Bert 作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过 CBOW 估计是不太可能的。</p>
<p>从这里可以看出，在文章开始我说过 Bert 在模型方面其实没有太大创新，更像一个最近几年 NLP 重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实 Bert 本身的效果好和普适性强才是最大的亮点。</p>
<p><strong>最后，我讲讲我对Bert的评价和看法</strong>，我觉得 Bert 是 NLP 里里程碑式的工作，对于后面 NLP 的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert 借鉴了 ELMO，GPT 及 CBOW，主要提出了 Masked 语言模型及 Next Sentence Prediction，但是这里 Next Sentence Prediction 基本不影响大局，而 Masked LM 明显借鉴了 CBOW 的思想。所以说 Bert 的模型没什么大的创新，更像最近几年 NLP 重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。</p>
<p>如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务 Fine-tuning 或者做特征集成；第二是特征抽取要用Transformer 作为特征提取器而不是 RNN 或者 CNN；第三，双向语言模型可以采取 CBOW 的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert 最大的亮点在于效果好及普适性强，几乎所有 NLP 任务都可以套用 Bert 这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在 NLP 应用领域，Transformer 将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p>
<p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是 NLP 尤其是深度学习场景下的 NLP 的主要目标之一，不过一直没有太好的解决办法，而 ELMO&#x2F;GPT&#x2F;Bert 的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p>
<p>对于当前 NLP 的发展方向，我个人觉得有两点非常重要：</p>
<ol>
<li>一个是需要更强的特征抽取器，目前看 Transformer 会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；</li>
<li>第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。</li>
</ol>
<p>目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p>
<p>完了，这就是预训练语言模型的前世今生。</p>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p>[1] <a target="_blank" rel="noopener" href="https://www.cnblogs.com/nickchen121/p/15105048.html">https://www.cnblogs.com/nickchen121/p/15105048.html</a></p>
<p>[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37601161">https://zhuanlan.zhihu.com/p/37601161</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Lin Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/">http://example.com/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LinLi's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/" title="代码表征预训练语言模型学习指南：原理、分析和代码"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">代码表征预训练语言模型学习指南：原理、分析和代码</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/24/Type-Infer%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/" title="Type Infer文献综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Type Infer文献综述</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lin Li</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT"><span class="toc-number">1.</span> <span class="toc-text">预训练语言模型的前世今生 - 从Word Embedding到BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.</span> <span class="toc-text">一、预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 图像领域的预训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E6%80%9D%E6%83%B3"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 预训练的思想</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">二、语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 统计语言模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 神经网络语言模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E8%AF%8D%E5%90%91%E9%87%8F"><span class="toc-number">1.3.</span> <span class="toc-text">三、词向量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%8B%AC%E7%83%AD%EF%BC%88Onehot%EF%BC%89%E7%BC%96%E7%A0%81"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 独热（Onehot）编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Word-Embedding"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 Word Embedding</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Word2Vec-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">四、Word2Vec 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.</span> <span class="toc-text">五、自然语言处理的预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AD%E3%80%81RNN-%E5%92%8C-LSTM"><span class="toc-number">1.6.</span> <span class="toc-text">六、RNN 和 LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-RNN"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-RNN-%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 RNN 的梯度消失问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-LSTM"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 LSTM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-LSTM-%E8%A7%A3%E5%86%B3-RNN-%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 LSTM 解决 RNN 的梯度消失问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%83%E3%80%81ELMo-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.7.</span> <span class="toc-text">七、ELMo 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-ELMo-%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 ELMo 的预训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-ELMo-%E7%9A%84-Feature-based-Pre-Training"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 ELMo 的 Feature-based Pre-Training</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E3%80%81Attention"><span class="toc-number">1.8.</span> <span class="toc-text">八、Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-%E4%BA%BA%E7%B1%BB%E7%9A%84%E8%A7%86%E8%A7%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 人类的视觉注意力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-Attention-%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%80%9D%E6%83%B3"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 Attention 的本质思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-3-Self-Attention-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 Self Attention 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-4-Self-Attention-%E5%92%8C-RNN%E3%80%81LSTM-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.4 Self Attention 和 RNN、LSTM 的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-5-Masked-Self-Attention-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 Masked Self Attention 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-6-Multi-head-Self-Attention-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.8.6.</span> <span class="toc-text">8.6 Multi-head Self Attention 模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%9D%E3%80%81Position-Embedding"><span class="toc-number">1.9.</span> <span class="toc-text">九、Position Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E3%80%81Transformer"><span class="toc-number">1.10.</span> <span class="toc-text">十、Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-Transformer-%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.10.1.</span> <span class="toc-text">10.1 Transformer 的结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-2-Encoder"><span class="toc-number">1.10.2.</span> <span class="toc-text">10.2 Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-3-Decoder"><span class="toc-number">1.10.3.</span> <span class="toc-text">10.3 Decoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-4-Transformer-%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C"><span class="toc-number">1.10.4.</span> <span class="toc-text">10.4 Transformer 输出结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81Transformer-%E5%8A%A8%E6%80%81%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA"><span class="toc-number">1.11.</span> <span class="toc-text">十一、Transformer 动态流程展示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-1-%E4%B8%BA%E4%BB%80%E4%B9%88-Decoder-%E9%9C%80%E8%A6%81%E5%81%9A-Mask"><span class="toc-number">1.11.1.</span> <span class="toc-text">11.1 为什么 Decoder 需要做 Mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-2-%E4%B8%BA%E4%BB%80%E4%B9%88-Encoder-%E7%BB%99%E4%BA%88-Decoders-%E7%9A%84%E6%98%AF-K%E3%80%81V-%E7%9F%A9%E9%98%B5"><span class="toc-number">1.11.2.</span> <span class="toc-text">11.2 为什么 Encoder 给予 Decoders 的是 K、V 矩阵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%BA%8C%E3%80%81GPT-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.12.</span> <span class="toc-text">十二、GPT 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-1-GPT-%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.12.1.</span> <span class="toc-text">12.1 GPT 模型的预训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-GPT-%E6%A8%A1%E5%9E%8B%E7%9A%84-Fine-tuning"><span class="toc-number">1.12.2.</span> <span class="toc-text">12.2 GPT 模型的 Fine-tuning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%B8%89%E3%80%81BERT-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.13.</span> <span class="toc-text">十三、BERT 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#13-1-BERT%EF%BC%9A%E5%85%AC%E8%AE%A4%E7%9A%84%E9%87%8C%E7%A8%8B%E7%A2%91"><span class="toc-number">1.13.1.</span> <span class="toc-text">13.1 BERT：公认的里程碑</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-2-BERT-%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9A%E5%BC%BA%E5%A4%A7%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E8%83%BD%E5%8A%9B"><span class="toc-number">1.13.2.</span> <span class="toc-text">13.2 BERT 的结构：强大的特征提取能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-3-BERT-%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83"><span class="toc-number">1.13.3.</span> <span class="toc-text">13.3 BERT 之无监督训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-4-BERT%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%8E%A9%E7%A0%81%E6%A8%A1%E5%9E%8B%EF%BC%88MLM%EF%BC%89"><span class="toc-number">1.13.4.</span> <span class="toc-text">13.4 BERT之语言掩码模型（MLM）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-5-BERT-%E4%B9%8B%E4%B8%8B%E5%8F%A5%E9%A2%84%E6%B5%8B%EF%BC%88NSP%EF%BC%89"><span class="toc-number">1.13.5.</span> <span class="toc-text">13.5 BERT 之下句预测（NSP）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-6-BERT-%E4%B9%8B%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.13.6.</span> <span class="toc-text">13.6 BERT 之输入表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E5%9B%9B%E3%80%81BERT-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0"><span class="toc-number">1.14.</span> <span class="toc-text">十四、BERT 下游任务改造</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-1-%E5%8F%A5%E5%AF%B9%E5%88%86%E7%B1%BB"><span class="toc-number">1.14.1.</span> <span class="toc-text">14.1 句对分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-%E5%8D%95%E5%8F%A5%E5%88%86%E7%B1%BB"><span class="toc-number">1.14.2.</span> <span class="toc-text">14.2 单句分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-3-%E6%96%87%E6%9C%AC%E9%97%AE%E7%AD%94"><span class="toc-number">1.14.3.</span> <span class="toc-text">14.3 文本问答</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-%E5%8D%95%E5%8F%A5%E6%A0%87%E6%B3%A8"><span class="toc-number">1.14.4.</span> <span class="toc-text">14.4 单句标注</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-5-BERT%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">1.14.5.</span> <span class="toc-text">14.5 BERT效果展示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%BA%94%E3%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-number">1.15.</span> <span class="toc-text">十五、预训练语言模型总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%EF%BC%9A"><span class="toc-number">1.16.</span> <span class="toc-text">参考文献：</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/27/LearnedSQLGen-Constraint-aware-SQL-Generation-using-Reinforcement-Learning/" title="LearnedSQLGen Constraint-aware SQL Generation using Reinforcement Learning">LearnedSQLGen Constraint-aware SQL Generation using Reinforcement Learning</a><time datetime="2023-09-27T07:05:56.000Z" title="发表于 2023-09-27 15:05:56">2023-09-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/18/Griffin-Grammar-Free-DBMS-Fuzzing/" title="Griffin Grammar-Free DBMS Fuzzing">Griffin Grammar-Free DBMS Fuzzing</a><time datetime="2023-08-18T07:28:26.000Z" title="发表于 2023-08-18 15:28:26">2023-08-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/24/sql%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C/" title="sql的编译与执行">sql的编译与执行</a><time datetime="2023-07-24T03:19:59.000Z" title="发表于 2023-07-24 11:19:59">2023-07-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/23/Finding-Bugs-in-Database-Systems-via-Query-Partitioning/" title="Finding Bugs in Database Systems via Query Partitioning">Finding Bugs in Database Systems via Query Partitioning</a><time datetime="2023-07-23T03:07:25.000Z" title="发表于 2023-07-23 11:07:25">2023-07-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/22/go-randgen%E4%BD%BF%E7%94%A8/" title="go-randgen使用">go-randgen使用</a><time datetime="2023-07-22T03:08:23.000Z" title="发表于 2023-07-22 11:08:23">2023-07-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Lin Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>