<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation | LinLi's Blog</title><meta name="author" content="Lin Li"><meta name="copyright" content="Lin Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and MitigationAbstract代码生成旨在根据输入需求自动生成代码，从而显著提升开发效率。近年来，基于大型语言模型（LLMs）的方法在代码生成任务上取得了显著成果，彻底改变了该领域。然而，尽管表现出色，LLMs 在代码生成场景中仍然容易产生幻觉">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation">
<meta property="og:url" content="http://example.com/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/index.html">
<meta property="og:site_name" content="LinLi&#39;s Blog">
<meta property="og:description" content="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and MitigationAbstract代码生成旨在根据输入需求自动生成代码，从而显著提升开发效率。近年来，基于大型语言模型（LLMs）的方法在代码生成任务上取得了显著成果，彻底改变了该领域。然而，尽管表现出色，LLMs 在代码生成场景中仍然容易产生幻觉">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2025-01-30T12:14:09.000Z">
<meta property="article:modified_time" content="2025-01-30T12:14:54.014Z">
<meta property="article:author" content="Lin Li">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-30 20:14:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">119</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="LinLi's Blog"><span class="site-name">LinLi's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-30T12:14:09.000Z" title="发表于 2025-01-30 20:14:09">2025-01-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-30T12:14:54.014Z" title="更新于 2025-01-30 20:14:54">2025-01-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation"><a href="#LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation" class="headerlink" title="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation"></a>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>代码生成旨在根据输入需求自动生成代码，从而显著提升开发效率。近年来，基于大型语言模型（LLMs）的方法在代码生成任务上取得了显著成果，彻底改变了该领域。然而，尽管表现出色，LLMs 在代码生成场景中仍然容易产生幻觉（hallucinations），特别是在实际开发过程中需要处理复杂上下文依赖的情况下。虽然已有研究分析了 LLM 在代码生成中的幻觉问题，但其研究范围主要局限于独立函数的生成。在本文中，我们进行了一项实证研究，探讨 LLM 幻觉在更实际且更复杂的开发环境中的现象、机制及其缓解方法，重点关注仓库级代码生成场景。首先，我们手动检查六种主流 LLM 的代码生成结果，建立了一套 LLM 生成代码的幻觉分类体系。随后，我们详细阐述幻觉现象，并分析其在不同模型中的分布情况。接着，我们探究幻觉产生的原因，并识别出四个可能导致幻觉的关键因素。最后，我们提出了一种基于 RAG（Retrieval-Augmented Generation）的缓解方法，该方法在所有研究的 LLM 上均表现出稳定的效果。我们的复现包（包含代码、数据和实验结果）可在以下地址获取：<br><a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination%E3%80%82">https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination。</a></p>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p><font color="red">Background and related work</font></p>
<p><strong>代码生成的背景</strong></p>
<ul>
<li>代码生成是一种自动化技术，通过自然语言描述的规格高效生成代码，减少开发者的手动编码工作。</li>
<li>近年来，基于 Transformer 架构的大型语言模型（LLMs）取得了显著进展，并在代码生成任务上表现优异，如 GPT-4 在 HumanEval 和 MBPP 基准测试中取得了最先进的结果。</li>
</ul>
<p><strong>现实开发需求的复杂性</strong></p>
<ul>
<li>实际开发场景中的代码生成需求远比生成独立函数更复杂，需要处理上下文依赖（如调用用户自定义函数、项目定义的数据协议）。</li>
<li>为了更准确评估 LLM 在实际代码生成任务中的表现，提出了新的基准测试（CoderEval、ClassEval、EvoCodeBench）。</li>
<li>这些基准测试主要关注 LLM 代码生成的功能正确性（通过测试用例通过率衡量），<strong>但缺乏对失败原因的深入分析</strong>。</li>
</ul>
<p><strong>LLM 代码生成中的幻觉问题</strong></p>
<ul>
<li>现有研究表明，LLMs 在生成自然语言文本时存在幻觉问题，主要包括三种类型：<ol>
<li><strong>输入冲突幻觉（Input-Conflicting Hallucination）</strong></li>
<li><strong>事实冲突幻觉（Fact-Conflicting Hallucination）</strong></li>
<li><strong>上下文冲突幻觉（Context-Conflicting Hallucination）</strong></li>
</ol>
</li>
<li>在代码生成领域，Liu 等人的研究分析了 LLM 生成代码的幻觉问题，并基于 HumanEval 和 DS-1000 进行分类，发现如“死代码（Dead Code）”和“重复代码（Repetition）”等现象。</li>
</ul>
<p><strong>本研究的贡献</strong></p>
<ul>
<li><strong>研究范围拓展</strong>：不同于 Liu 等人的研究仅针对独立函数&#x2F;脚本的生成，本研究聚焦于 <strong>仓库级别的代码生成</strong>，分析更实际和复杂的开发环境中的幻觉问题。</li>
<li><strong>研究视角扩展</strong>：从 <strong>现象（Phenomena）、机制（Mechanism）和缓解方法（Mitigation）</strong> 三个角度系统研究幻觉问题，而不仅仅是从问题表现角度分类。</li>
<li><strong>研究目标</strong>：通过对 LLM 代码生成幻觉的全面研究，弥补现有研究的不足，提供对 LLM 代码生成能力的更深入理解。</li>
</ul>
<p><font color="red">Empirical Study</font></p>
<p><strong>研究目标</strong></p>
<p>本研究旨在揭示 <strong>LLM 代码生成中的幻觉现状及其根本原因</strong>，并围绕以下三个研究问题（RQs）展开分析：</p>
<ul>
<li><strong>RQ1（幻觉分类）：</strong> 代码生成中的幻觉表现形式及其分布情况如何？</li>
<li><strong>RQ2（LLM 比较）：</strong> 不同 LLM 之间的幻觉发生率和模式有何差异？</li>
<li><strong>RQ3（幻觉根因）：</strong> 代码生成中的幻觉产生的根本原因是什么？</li>
</ul>
<p><strong>研究方法</strong></p>
<ul>
<li>选取 <strong>六种主流 LLM</strong>（ChatGPT、CodeGen、PanGu-α、StarCoder2、DeepSeekCoder、CodeLlama）并基于 <strong>CoderEval 数据集</strong> 进行实验。</li>
<li>采用 <strong>开放编码（open coding）</strong> 方法，对 LLM 生成的代码进行手动标注，逐步构建 <strong>幻觉分类体系</strong>。</li>
<li>通过 <strong>迭代标注</strong>（初始标注 10%，然后扩展至 90%）完善分类体系，并围绕研究问题进行深入分析。</li>
</ul>
<p><strong>研究发现</strong></p>
<ol>
<li>LLM 代码生成的幻觉可以分为三大类（任务需求冲突、事实知识冲突、项目上下文冲突），并进一步细分为 八个子类别：<ul>
<li><strong>任务需求冲突</strong>：功能需求违背（Functional Requirement Violation）、非功能需求违背（Non-Functional Requirement Violation）</li>
<li><strong>事实知识冲突</strong>：背景知识冲突（Background Knowledge Conflicts）、库知识冲突（Library Knowledge Conflicts）、API 知识冲突（API Knowledge Conflicts）</li>
<li><strong>项目上下文冲突</strong>：环境冲突（Environment Conflicts）、依赖冲突（Dependency Conflicts）、非代码资源冲突（Non-code Resource Conflicts）</li>
</ul>
</li>
<li><strong>不同 LLM 的幻觉分布分析</strong>：所有模型中最常见的幻觉类型是 <strong>任务需求冲突</strong>。</li>
<li>幻觉的<strong>四个潜在原因</strong>：<ul>
<li>训练数据质量（Training Data Quality）</li>
<li>任务意图理解能力（Intention Understanding Capacity）</li>
<li>知识获取能力（Knowledge Acquisition Capacity）</li>
<li>仓库级上下文感知能力（Repository-Level Context Awareness）</li>
</ul>
</li>
</ol>
<p><strong>幻觉的缓解方法</strong></p>
<ul>
<li><p>基于 RAG（Retrieval-Augmented Generation）</p>
<p> 的轻量级缓解方法：</p>
<ul>
<li><strong>构建检索库</strong>，从仓库中提取与当前任务相关的代码片段。</li>
<li>通过 <strong>相似度匹配</strong>，将相关代码片段作为提示词，辅助 LLM 代码生成。</li>
<li><strong>实验结果表明</strong>，该方法能 <strong>稳定提升所有研究 LLM 的代码生成表现</strong>。</li>
</ul>
</li>
</ul>
<p><font color="red">Contributions</font></p>
<p><strong>实证研究</strong>：分析 LLM 在真实开发场景中的幻觉类型，并构建 LLM 代码生成幻觉分类体系。</p>
<p><strong>幻觉现象分析</strong>：详细研究幻觉的表现，并分析其在不同 LLM 中的分布情况。</p>
<p><strong>幻觉成因分析</strong>：识别并归纳导致幻觉的 <strong>四个关键因素</strong>。</p>
<p><strong>缓解方法提出</strong>：基于幻觉成因，提出 <strong>RAG（检索增强生成）</strong> 方法，并在多个 LLM 上实验其有效性。</p>
<p><strong>复现包发布</strong>：提供 <strong>代码、数据、实验结果</strong>，支持后续研究（<a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination">项目地址</a>）。</p>
<h2 id="BACKGROUND-amp-RELATED-WORK"><a href="#BACKGROUND-amp-RELATED-WORK" class="headerlink" title="BACKGROUND &amp; RELATED WORK"></a>BACKGROUND &amp; RELATED WORK</h2><h3 id="A-LLM-based-Code-Generation"><a href="#A-LLM-based-Code-Generation" class="headerlink" title="A. LLM-based Code Generation"></a><em>A. LLM-based Code Generation</em></h3><p><strong>真实开发场景</strong>：</p>
<ul>
<li>开发者通常依赖 <strong>代码仓库</strong> 进行编程，公司出于安全性和功能性考虑，往往构建 <strong>内部代码仓库</strong>，其中包含许多 <strong>私有 API</strong>，LLM 训练时无法接触到这些 API。</li>
<li>现实开发中的代码往往 <strong>依赖已有的 API 和函数</strong>，而不仅仅是基于函数描述和签名独立生成代码。</li>
</ul>
<p><strong>依赖函数的普遍性</strong>：</p>
<ul>
<li>研究分析了 <strong>GitHub 上 100 个最受欢迎的 Java 和 Python 项目</strong>，发现 <strong>超过 70% 的函数是依赖函数</strong>，需要调用代码仓库中的 API，而非独立实现。</li>
</ul>
<p><strong>现有基准测试与 LLM 表现</strong>：</p>
<ul>
<li><strong>CoderEval、ClassEval、EvoCodeBench</strong> 等基准测试通过收集 <strong>真实代码仓库的代码片段与描述</strong>，并使用 <strong>测试用例</strong> 检验代码正确性。</li>
<li>LLMs 在这些基准上的表现极差，主要问题：<ul>
<li><strong>无法基于问题描述生成正确代码</strong>。</li>
<li><strong>倾向于生成独立代码段</strong>，而不是使用当前开发环境中的已有函数。</li>
</ul>
</li>
</ul>
<h3 id="B-Hallucinations-in-LLMs"><a href="#B-Hallucinations-in-LLMs" class="headerlink" title="B. Hallucinations in LLMs"></a><em>B. Hallucinations in LLMs</em></h3><p><strong>幻觉的定义（NLP 领域）</strong></p>
<ul>
<li>幻觉指 <strong>模型生成的文本与输入或预期输出环境不一致、缺乏意义或违背事实</strong> 的现象。</li>
<li>在文本生成任务（如文本补全、摘要生成、机器翻译）中尤为明显，影响输出的一致性和可靠性。</li>
<li>根据幻觉的性质，可分为三类：<ol>
<li><strong>输入冲突幻觉（Input-Conflicting Hallucinations）</strong>：输出内容偏离原始输入，可能源于错误解析或内部表示不准确。</li>
<li><strong>上下文冲突幻觉（Context-Conflicting Hallucinations）</strong>：输出内容自相矛盾，反映模型难以保持上下文一致性。</li>
<li><strong>事实冲突幻觉（Fact-Conflicting Hallucinations）</strong>：输出内容与现实世界的知识或事实不符，可能由训练数据质量、知识更新滞后或推理能力不足导致。</li>
</ol>
</li>
</ul>
<p><strong>代码生成领域的幻觉问题</strong></p>
<ul>
<li>现有研究 <strong>缺乏对代码生成幻觉的深入定义和研究</strong>。</li>
<li>尽管已有许多基于 LLM 的代码生成优化方法，但这些工作 <strong>未明确界定代码生成中的幻觉问题</strong>。</li>
<li>幻觉会影响代码的质量、可维护性，并可能引发 <strong>性能问题、安全漏洞</strong>，威胁软件的稳定性和安全性。</li>
</ul>
<p><strong>现有研究的不足</strong></p>
<ul>
<li>之前的研究 [8] 为代码生成任务定义了新的幻觉分类 （共五种类型），但 未考虑真实开发环境中的关键因素，如：<ul>
<li><strong>开发环境</strong></li>
<li><strong>系统资源</strong></li>
<li><strong>外部约束</strong></li>
<li><strong>代码仓库依赖</strong></li>
</ul>
</li>
<li>这些因素导致 LLM <strong>在实际开发中的可用性和准确性较低</strong>。</li>
</ul>
<p><strong>本研究的贡献</strong></p>
<ul>
<li>本研究 <strong>基于真实开发数据集</strong> 进行实证分析，定义新的幻觉类型，弥补现有研究的不足。</li>
<li>研究成果为 <strong>探索 LLM 代码生成幻觉问题提供新思路</strong>，推动后续研究。</li>
</ul>
<h2 id="EVALUATION-SETUP"><a href="#EVALUATION-SETUP" class="headerlink" title="EVALUATION SETUP"></a>EVALUATION SETUP</h2><h3 id="A-Dataset"><a href="#A-Dataset" class="headerlink" title="A. Dataset"></a><em>A. Dataset</em></h3><p><strong>数据来源</strong>：基于 <strong>CoderEval 基准测试</strong>，选取真实 Python 代码仓库中的编程任务。</p>
<p><strong>数据规模</strong>：包含 <strong>230 个 Python 代码生成任务</strong>，覆盖不同类型的项目。</p>
<p><strong>数据组成</strong>：每个任务包括：</p>
<ul>
<li><strong>自然语言描述</strong></li>
<li><strong>标准代码片段（ground-truth code）</strong></li>
<li><strong>测试用例</strong></li>
<li><strong>与任务相关的项目环境上下文</strong></li>
</ul>
<p><strong>目的</strong>：更好地模拟实际开发场景，以评估 LLM 代码生成能力。</p>
<h3 id="B-Studied-LLMs"><a href="#B-Studied-LLMs" class="headerlink" title="B. Studied LLMs"></a><em>B. Studied LLMs</em></h3><p><strong>研究范围</strong>：使用 <strong>6 种主流 LLM</strong> 进行代码生成，涵盖 <strong>开源与闭源</strong> 模型，参数规模多样。</p>
<p><strong>具体模型</strong>：</p>
<ol>
<li>**ChatGPT (GPT-3.5-Turbo)**：多语言通用模型，代码生成能力强。</li>
<li><strong>CodeGen-350M-Mono</strong>：专用于程序合成的自回归模型系列。</li>
<li><strong>PanGu-α-2.6B</strong>：支持多语言代码生成的模型。</li>
<li><strong>DeepSeekCoder-6.7B</strong>：开源模型，在多种编程语言和基准测试中表现良好。</li>
<li><strong>CodeLlama-7B-Python-hf</strong>：专为代码生成预训练和微调的模型。</li>
<li><strong>StarCoder2-7B</strong>：专注于代码生成的开源 LLM。</li>
</ol>
<p><strong>实验设置</strong>：</p>
<ul>
<li>每个任务 <strong>生成 10 个代码片段</strong>。</li>
<li>采用 <strong>核采样策略（nuclear sampling）</strong>，温度设为 <strong>0.6</strong>，与 <strong>CoderEval 设定一致</strong>。</li>
</ul>
<h3 id="C-Taxonomy-Annotation"><a href="#C-Taxonomy-Annotation" class="headerlink" title="C. Taxonomy Annotation"></a><em>C. Taxonomy Annotation</em></h3><p><strong>初始开放编码（Initial Open Coding）</strong></p>
<ul>
<li>选取 <strong>CoderEval 数据集的 10%（23 个任务）</strong> 进行初步分析。</li>
<li>使用 <strong>6 种 LLM（ChatGPT、CodeGen、PanGu-α、DeepSeekCoder、CodeLlama、StarCoder2）</strong>，每个任务生成 <strong>10 个代码片段</strong>，共 <strong>1,380 个代码片段</strong>。</li>
<li>在实际开发环境中测试代码正确性，并由 <strong>两位研究者</strong> 比较 LLM 生成代码与标准代码，讨论并记录潜在幻觉现象。</li>
</ul>
<p><strong>初步分类构建（Preliminary Taxonomy Construction）</strong></p>
<ul>
<li>记录 <strong>幻觉类型及其在代码中的位置</strong>，每个代码片段可能包含多个幻觉。</li>
<li>通过讨论对 <strong>相似幻觉进行归类</strong>，形成初步幻觉分类体系，明确幻觉类型及其含义。</li>
</ul>
<p><strong>完整分类构建（Full Taxonomy Construction）</strong></p>
<ul>
<li>由 <strong>三名具有丰富 Python 经验的志愿者</strong>（两名 10+ 年经验，一名 4 年经验）对剩余代码片段进行独立标注。</li>
<li>如果发现新的幻觉类型，标注者需撰写描述，并进行讨论，<strong>扩展和完善分类体系</strong>。</li>
</ul>
<h2 id="EVALUATION-RESULTS"><a href="#EVALUATION-RESULTS" class="headerlink" title="EVALUATION RESULTS"></a>EVALUATION RESULTS</h2><p><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130191535309.png" alt="image-20250130191535309"></p>
<h3 id="A-RQ1-Hallucination-Taxonomy"><a href="#A-RQ1-Hallucination-Taxonomy" class="headerlink" title="A. RQ1: Hallucination Taxonomy"></a><em>A. RQ1: Hallucination Taxonomy</em></h3><p>本研究通过手动标注，构建了 <strong>LLM 代码生成幻觉分类体系</strong>，包含 <strong>3 大类别、8 个子类别</strong>，并分析其分布情况。</p>
<p><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130191915285.png" alt="image-20250130191915285"></p>
<p><strong>1. 任务需求冲突（Task Requirement Conflicts，43.53%）</strong></p>
<p><strong>描述</strong>：生成的代码未满足任务的功能或非功能需求，对应于 NLP 领域的 <strong>输入冲突幻觉</strong>。</p>
<ul>
<li>功能需求违背（Functional Requirement Violation，36.66%）：<ul>
<li>代码未能正确实现预期功能，可能导致逻辑错误或运行时错误。</li>
<li>进一步细分为 <strong>错误功能（Wrong Functionality）</strong> 和 <strong>缺失功能（Missing Functionality）</strong>。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130192008960.png" alt="image-20250130192008960" style="zoom:67%;"></li>
</ul>
</li>
<li>非功能需求违背（Non-functional Requirement Violation，6.86%）：<ul>
<li>代码不符合 <strong>安全性、性能、编码风格、代码质量</strong> 要求，可能引入安全漏洞或降低可维护性。</li>
<li>例如，LLM 生成代码可能错误地使用 <strong>不安全的 YAML 解析函数</strong>，引发安全风险。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130192409131.png" alt="image-20250130192409131" style="zoom: 67%;"></li>
</ul>
</li>
</ul>
<p><strong>2. 事实知识冲突（Factual Knowledge Conflicts，31.91%）</strong></p>
<p><strong>描述</strong>：代码不符合 <strong>已有背景知识、库&#x2F;框架知识、API 规范</strong>，对应于 <strong>知识冲突幻觉</strong>。</p>
<ul>
<li>背景知识冲突（Background Knowledge Conflicts，8.82%）：<ul>
<li>代码违背特定领域的标准或规范，如 <strong>汽车软件 AUTOSAR 规范</strong>。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130193615597.png" alt="image-20250130193615597" style="zoom:67%;"></li>
</ul>
</li>
<li>库知识冲突（Library Knowledge Conflicts，2.68%）：<ul>
<li>误用第三方库（如错误调用 <strong>asyncio API</strong>，导致异步处理问题）。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130194007134.png" alt="image-20250130194007134" style="zoom:67%;"></li>
</ul>
</li>
<li>API 知识冲突（API Knowledge Conflicts，20.41%）：<ul>
<li>API <strong>参数错误、条件检查错误、错误&#x2F;废弃 API 调用、异常处理不当</strong>，可能导致功能错误或系统不稳定。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130194058566.png" alt="image-20250130194058566" style="zoom:67%;"></li>
</ul>
</li>
</ul>
<p><strong>3. 项目上下文冲突（Project Context Conflicts，24.56%）</strong></p>
<p><strong>描述</strong>：生成的代码未正确使用 <strong>项目特定的环境、依赖和非代码资源</strong>，属于 <strong>上下文冲突幻觉</strong>。</p>
<ul>
<li>环境冲突（Environment Conflicts，0.94%）：<ul>
<li>代码不兼容当前开发环境（如 Python 版本不支持的语法）。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130194144050.png" alt="image-20250130194144050" style="zoom:80%;"></li>
</ul>
</li>
<li>依赖冲突（Dependency Conflicts，11.26%）：<ul>
<li>代码调用了 <strong>未定义或未导入的函数、变量、API</strong>，导致运行错误。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130194211221.png" alt="image-20250130194211221" style="zoom:67%;"></li>
</ul>
</li>
<li>非代码资源冲突（Non-code Resource Conflicts，12.36%）：<ul>
<li>代码错误处理 <strong>数据、配置、资源文件、网络连接</strong>，可能导致文件访问失败、数据不一致或安全问题。</li>
<li><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130194334307.png" alt="image-20250130194334307" style="zoom:67%;"></li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li><strong>任务需求冲突（43.53%）</strong> 是最常见的幻觉类型。</li>
<li><strong>事实知识冲突（31.91%）</strong> 和 <strong>项目上下文冲突（24.56%）</strong> 也广泛存在，影响 LLM 代码生成的可靠性。</li>
<li>本研究构建的 <strong>LLM 代码生成幻觉分类体系</strong> 提供了更全面的分析框架，为未来优化 LLM 代码生成提供指导。</li>
</ul>
<h3 id="B-RQ2-LLM-Comparison"><a href="#B-RQ2-LLM-Comparison" class="headerlink" title="B. RQ2: LLM Comparison"></a><em>B. RQ2: LLM Comparison</em></h3><ol>
<li>任务需求冲突（Task Requirement Conflicts）最常见：<ul>
<li>所有 LLM 中，该类型幻觉发生频率最高。</li>
<li><strong>CodeGen 和 StarCoder2</strong> 在此类别的幻觉最严重，而 <strong>DeepSeekCoder 和 CodeLlama</strong> 发生率最低。</li>
<li>可能原因：模型对任务需求的理解能力不同，受 <strong>模型大小、训练语料</strong> 影响。</li>
</ul>
</li>
<li>事实知识冲突（Factual Knowledge Conflicts）和项目上下文冲突（Project Context Conflicts）频率相近：<ul>
<li><strong>PanGu-α</strong> 的事实知识冲突最多，可能因为其主要基于 <strong>中文语料训练</strong>，对 <strong>英语领域的特定知识</strong> 了解有限。</li>
</ul>
</li>
</ol>
<p><strong>总结</strong></p>
<ul>
<li><strong>任务需求冲突是所有 LLM 最常见的幻觉类型</strong>，尤其在 <strong>CodeGen 和 StarCoder2</strong> 中更为明显。</li>
<li><strong>模型训练数据的广度和多样性影响幻觉发生率</strong>，例如 DeepSeekCoder 和 CodeLlama 由于 <strong>同时训练于广泛的代码和文本数据</strong>，幻觉发生率较低。</li>
</ul>
<p><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130200058698.png" alt="image-20250130200058698"></p>
<h3 id="C-RQ3-Root-Cause-Analysis"><a href="#C-RQ3-Root-Cause-Analysis" class="headerlink" title="C. RQ3: Root Cause Analysis"></a><em>C. RQ3: Root Cause Analysis</em></h3><p>本研究进一步分析 LLM 代码生成幻觉的可能成因，并归纳出 <strong>四个关键因素</strong>。</p>
<p><strong>1. 训练数据质量（Training Data Quality）</strong></p>
<ul>
<li><strong>影响</strong>：训练数据的质量直接影响 LLM 的推理能力。</li>
<li>问题：<ul>
<li>训练数据中可能存在 <strong>错误注释、低效或不安全的代码</strong>、API 误用、过时的库文档等。</li>
<li>LLM 在训练过程中 <strong>学习了这些错误模式</strong>，导致幻觉生成。</li>
</ul>
</li>
<li><strong>影响的幻觉类型</strong>：主要导致 <strong>任务需求冲突（Task Requirement Conflicts）</strong> 和 <strong>事实知识冲突（Factual Knowledge Conflicts）</strong>。</li>
<li><strong>优化方向</strong>：需要 <strong>构建高质量的代码训练数据</strong> 以减少幻觉。</li>
</ul>
<p><strong>2. 任务意图理解能力（Intention Understanding Capacity）</strong></p>
<ul>
<li><strong>影响</strong>：LLMs <strong>难以准确理解用户需求</strong>，影响代码生成的准确性。</li>
<li>问题：<ul>
<li>LLM <strong>依赖模式匹配</strong> 而非深度理解任务需求，可能遗漏关键信息。</li>
<li><strong>复杂逻辑、多步骤操作</strong> 需求难以被正确解析，导致代码 <strong>部分实现正确但不满足完整业务逻辑</strong>。</li>
</ul>
</li>
<li><strong>案例</strong>：如某任务要求处理 <strong>LocalTime</strong>，但 LLM 忽略该需求，未在代码中正确实现。</li>
<li><strong>优化方向</strong>：提升 LLM <strong>对复杂需求的理解能力</strong>，减少需求偏差导致的幻觉。</li>
</ul>
<p><strong>3. 知识获取能力（Knowledge Acquisition Capacity）</strong></p>
<ul>
<li><strong>影响</strong>：LLMs 可能<strong>学习错误知识</strong>，或<strong>缺乏最新领域知识</strong>。</li>
<li>问题：<ul>
<li>由于训练数据质量问题，LLM 可能 <strong>学习到错误或过时的事实知识</strong>，导致幻觉。</li>
<li>训练后的 LLM <strong>无法主动更新知识</strong>，导致其对<strong>新技术、库更新</strong> 无法适应。</li>
</ul>
</li>
<li><strong>案例</strong>：某任务要求生成符合 <strong>OCFL 规范</strong> 的数据存储代码，但 LLM 生成的代码 <strong>不符合最新规范</strong>，可能因其 <strong>未见过最新 OCFL 相关知识</strong>。</li>
<li>优化方向：<ul>
<li><strong>引入 RAG（检索增强生成）</strong> 机制，使 LLM 可获取最新知识并进行补充校正。</li>
</ul>
</li>
</ul>
<p><strong>4. 仓库级上下文感知能力（Repository-Level Context Awareness）</strong></p>
<ul>
<li><strong>影响</strong>：LLM <strong>难以有效利用代码仓库的上下文信息</strong>，导致代码与项目不匹配。</li>
<li>问题：<ul>
<li>Transformer 架构存在 <strong>Token 长度限制</strong>（如 8K、12K），难以完整输入整个代码仓库。</li>
<li>直接输入所有项目上下文 <strong>可能引入大量无关信息</strong>，影响 LLM 代码生成的准确性。</li>
</ul>
</li>
<li>优化方向：<ul>
<li>采用 <strong>静态分析工具</strong> 或 <strong>RAG 机制</strong>，从仓库中检索<strong>与当前任务最相关的上下文</strong>，提高 LLM 的代码适配性。</li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<p>本研究识别了 <strong>四个导致 LLM 代码生成幻觉的主要因素</strong>：</p>
<ol>
<li><strong>训练数据质量</strong>（数据错误或低质量导致幻觉）</li>
<li><strong>任务意图理解能力</strong>（LLM 无法准确解析需求）</li>
<li><strong>知识获取能力</strong>（LLM 无法更新或正确获取领域知识）</li>
<li><strong>仓库级上下文感知能力</strong>（LLM 无法有效利用项目上下文信息）</li>
</ol>
<p><strong>优化方向</strong>：提升数据质量、改进任务解析、引入 <strong>RAG 机制</strong>，以减少幻觉在实际开发中的影响。</p>
<h2 id="MITIGATION-APPROACH"><a href="#MITIGATION-APPROACH" class="headerlink" title="MITIGATION APPROACH"></a>MITIGATION APPROACH</h2><h3 id="A-Motivation"><a href="#A-Motivation" class="headerlink" title="A. Motivation"></a><em>A. Motivation</em></h3><p><strong>LLM 代码生成幻觉的根本问题</strong></p>
<p>LLM 代码生成中的幻觉主要源于 <strong>推理阶段</strong> 的三个关键限制：</p>
<ol>
<li><strong>对任务需求理解不足</strong>（Incorrect&#x2F;insufficient task requirement understanding）。</li>
<li><strong>缺乏与任务相关的事实知识</strong>（Lack of factual knowledge）。</li>
<li><strong>无法访问仓库中的代码和非代码资源</strong>（Inability to access necessary repository resources）。</li>
</ol>
<p>这些限制 <strong>严重影响</strong> LLM 在实际开发环境中的代码生成能力。</p>
<p><strong>研究思路</strong></p>
<ul>
<li><strong>借鉴现有研究</strong> [26]–[35]，特别是 <strong>仓库级代码生成研究</strong> [62]，寻找优化方案。</li>
<li>探索 RAG（检索增强生成）方法，通过提供与任务相关的代码片段，帮助 LLM：<ol>
<li>更准确地理解任务需求。</li>
<li>获取更精准的事实知识。</li>
<li>适应具体的项目上下文，减少幻觉发生。</li>
</ol>
</li>
</ul>
<h3 id="B-RAG-based-Mitigation"><a href="#B-RAG-based-Mitigation" class="headerlink" title="B. RAG-based Mitigation"></a><em>B. RAG-based Mitigation</em></h3><p><strong>1. 代码检索库构建</strong></p>
<ul>
<li><strong>数据来源</strong>：从 <strong>CoderEval 数据集</strong> 中收集所有代码仓库。</li>
<li>方法：遵循 RepoCoder 方法 [62]，使用 滑动窗口 处理代码文件：<ul>
<li><strong>窗口大小</strong>：20 行代码。</li>
<li><strong>滑动步长</strong>：2 行代码。</li>
<li><strong>防止泄露答案</strong>：排除包含或紧邻 <strong>ground-truth 代码</strong> 的代码行。</li>
</ul>
</li>
<li><strong>结果</strong>：为每个代码仓库构建 <strong>检索语料库（retrieval corpus）</strong>，存储代码片段。</li>
</ul>
<p><strong>2. 代码片段检索</strong></p>
<ul>
<li><strong>检索机制</strong>：使用 <strong>稀疏词袋模型（Bag-of-Words, BOW）</strong> 计算文本相似度。</li>
<li>计算方法：<ul>
<li>将 <strong>查询（query）</strong> 和 <strong>候选代码片段</strong> 转换为 <strong>token 集合</strong>。</li>
<li>使用 <strong>Jaccard 指数</strong> 衡量相似度（交集大小 &#x2F; 并集大小）。</li>
<li>选择 <strong>前 10 个得分最高的代码片段</strong>，作为 LLM 生成代码的提示信息（prompt）。</li>
</ul>
</li>
</ul>
<p><strong>目的</strong></p>
<ul>
<li>通过 <strong>检索相关代码片段</strong>，帮助 LLM 更好地理解任务需求、获取事实知识、适应项目上下文，从而 <strong>减少幻觉发生</strong>。</li>
</ul>
<p><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130200659319.png" alt="image-20250130200659319"></p>
<h3 id="C-Evaluation"><a href="#C-Evaluation" class="headerlink" title="C. Evaluation"></a><em>C. Evaluation</em></h3><p><strong>1. 评估方法</strong></p>
<ul>
<li><strong>数据集</strong>：在 <strong>CoderEval 数据集</strong> 上评估 <strong>6 种 LLM</strong>（CodeGen、PanGu-α、ChatGPT、DeepSeekCoder、CodeLlama、StarCoder2）。</li>
<li>对比方法：<ul>
<li><strong>Raw 方法</strong>：仅提供 <strong>docstrings 和函数签名</strong> 作为输入。</li>
<li><strong>RAG 方法</strong>：在提供 docstrings 和函数签名的基础上，<strong>检索 10 个相关代码片段</strong> 作为 LLM 生成代码的提示信息（prompt）。</li>
</ul>
</li>
<li>评估指标：<ul>
<li>使用 <strong>Pass@1</strong> 评估代码的功能正确性，即 <strong>生成代码是否能通过测试用例</strong>。</li>
</ul>
</li>
</ul>
<p><strong>2. 结果分析</strong></p>
<ul>
<li><strong>表 1 结果</strong>：RAG 方法在 <strong>所有 6 个 LLM</strong> 上均能提高 <strong>Pass@1 分数</strong>，表明其 <strong>有效减少幻觉</strong>。</li>
<li><strong>提升幅度有限</strong>：由于当前方法仍处于 <strong>初步探索阶段</strong>，未来可进一步研究 <strong>模型微调（fine-tuning）</strong> 或 <strong>多智能体框架（multi-agent framework with tool use）</strong> 以提高效果。</li>
</ul>
<p><strong>3. 案例分析</strong></p>
<ul>
<li>案例 1（功能需求冲突缓解）：<ul>
<li><strong>Raw 方法</strong>：CodeGen <strong>错误使用 replace() 函数</strong>，未能正确将脚本转换为单行命令。</li>
<li><strong>RAG 方法</strong>：CodeGen <strong>正确使用 splitlines() 函数</strong>，符合 ground-truth 需求。</li>
</ul>
</li>
<li>案例 2（项目上下文冲突缓解）：<ul>
<li><strong>Raw 方法</strong>：ChatGPT <strong>错误调用 self.items.popitem()（仓库中不存在）</strong>，导致幻觉代码。</li>
<li><strong>RAG 方法</strong>：ChatGPT **正确使用 self.pop()**，符合仓库上下文要求。</li>
</ul>
</li>
</ul>
<p><strong>4. 总结</strong></p>
<ul>
<li>RAG 方法能 <strong>有效缓解 LLM 代码生成中的幻觉问题</strong>，提升代码正确性。</li>
<li>未来可探索 <strong>更高级的缓解策略</strong>（如 <strong>模型微调、多智能体框架</strong>）以进一步优化 LLM 代码生成质量。</li>
</ul>
<p><img src="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/image-20250130200837956.png" alt="image-20250130200837956"></p>
<h2 id="DISCUSSION"><a href="#DISCUSSION" class="headerlink" title="DISCUSSION"></a>DISCUSSION</h2><p><strong>1. 幻觉识别技术的开发</strong></p>
<ul>
<li>本研究发现 <strong>3 大类 LLM 代码生成幻觉</strong>，部分可通过 <strong>静态分析（如未定义变量）</strong> 或 <strong>动态测试（如运行时错误）</strong> 检测。</li>
<li>但某些幻觉（如 <strong>功能不完整、安全漏洞</strong>）难以检测，可能通过所有测试，最终进入生产环境，带来 <strong>系统不稳定和安全风险</strong>。</li>
<li><strong>现有 LLM 自反馈检测方法</strong> [71], [72] 有一定效果，但 <strong>受限于模型自身能力，无法根本解决训练数据缺陷</strong>。</li>
<li><strong>未来研究方向</strong>：开发 <strong>更精准和高效的幻觉识别技术</strong>，快速定位 LLM 生成代码中的幻觉问题。</li>
</ul>
<p><strong>2. 更有效的幻觉缓解技术</strong></p>
<ul>
<li>本研究探索了 基于 RAG 的轻量化方法，可缓解部分幻觉（如未定义属性），但仍存在局限：<ul>
<li><strong>仅基于当前代码仓库</strong> 构建检索库，未能覆盖 <strong>背景知识冲突</strong> 等问题。</li>
<li><strong>改进方向</strong>：未来可整合 <strong>在线搜索引擎、API 文档、StackOverflow 讨论</strong> 等更全面的知识源。</li>
</ul>
</li>
<li>除 RAG 之外，还可探索其他缓解策略：<ul>
<li><strong>输入查询优化（Input Query Refinement）</strong> [4], [49]，提升 LLM 对任务需求的理解。</li>
<li>多智能体系统（Multi-Agent Systems） [73]，构建 迭代优化流程：<ol>
<li><strong>明确任务需求</strong></li>
<li><strong>生成代码</strong></li>
<li><strong>运行测试</strong></li>
<li><strong>缓解幻觉</strong></li>
</ol>
</li>
<li><strong>关键挑战</strong>：需要 <strong>设计合理的智能体交互协议</strong>，结合 <strong>搜索引擎、静态分析工具</strong>，并采用 <strong>合适的提示策略（Prompting Strategies）</strong>。</li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<p>未来研究可从 <strong>幻觉识别</strong> 和 <strong>缓解技术</strong> 两个方向入手，结合 <strong>更全面的知识源、多智能体系统、查询优化</strong> 等方法，提升 LLM 代码生成的准确性与可靠性。</p>
<h2 id="THREATS-TO-VALIDITY"><a href="#THREATS-TO-VALIDITY" class="headerlink" title="THREATS TO VALIDITY"></a>THREATS TO VALIDITY</h2><p><strong>1. 外部有效性（External Validity）</strong></p>
<ul>
<li>潜在威胁：研究结果的 通用性 可能受限于以下因素：<ul>
<li>研究仅基于 <strong>Python 代码</strong>，未涵盖其他编程语言。</li>
<li>采用的 <strong>CoderEval 数据集规模有限（230 个任务）</strong>。</li>
</ul>
</li>
<li>缓解措施：<ul>
<li><strong>未来研究方向</strong>：构建 <strong>多语言幻觉分类体系</strong> 并进行对比分析。</li>
<li>选取 <strong>6 个 LLM</strong>，每个生成 <strong>10 个代码片段</strong>，确保标注数据的充足性。</li>
</ul>
</li>
</ul>
<p><strong>2. 内部有效性（Internal Validity）</strong></p>
<ul>
<li><strong>潜在威胁</strong>：手动标注过程中可能存在 <strong>主观偏差</strong>，影响幻觉分类的准确性。</li>
<li>缓解措施：<ul>
<li><strong>无正式一致性度量</strong> → 通过 <strong>标注者会议讨论并统一标注标准</strong>，确保一致性。</li>
<li><strong>可能存在模型偏差</strong> → 在标注前 <strong>混合 6 个 LLM 的生成结果</strong>，减少对特定模型的偏见。</li>
<li><strong>额外审查</strong>：一位作者对所有标注数据进行二次复核。</li>
</ul>
</li>
</ul>
<p><strong>3. 构造有效性（Construct Validity）</strong></p>
<ul>
<li><strong>潜在威胁</strong>：幻觉缓解方法的评估方式可能存在局限性。</li>
<li>缓解措施：<ul>
<li>采用 <strong>CoderEval 数据集的测试用例</strong> 作为 <strong>标准代码正确性评估方法</strong>，确保实验可靠性。</li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<p>本研究采取了多种措施 <strong>减少研究偏差</strong>，但未来仍需扩大数据规模、涵盖多种编程语言，并进一步优化标注和评估方法，以提高研究的通用性和可信度。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Lin Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/">http://example.com/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LinLi's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/02/01/DeepSeek-Coder-When-the-Large-Language-Model-Meets-Programming-The-Rise-of-Code-Intelligence/" title="DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence</div></div></a></div><div class="next-post pull-right"><a href="/2025/01/24/Benchmarking-Bias-in-Large-Language-Models-during-Role-Playing/" title="Benchmarking Bias in Large Language Models during Role-Playing"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Benchmarking Bias in Large Language Models during Role-Playing</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/04/PLUMBER/" title="PLUMBER"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-04</div><div class="title">PLUMBER</div></div></a></div><div><a href="/2023/03/08/%E9%80%9A%E8%BF%87NPM%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BE%9D%E8%B5%96%E6%A0%91%E6%8F%AD%E5%BC%80%E8%84%86%E5%BC%B1%E6%80%A7%E4%BC%A0%E6%92%AD%E5%8F%8A%E5%85%B6%E6%BC%94%E5%8C%96%E7%9A%84%E7%A5%9E%E7%A7%98%E9%9D%A2%E7%BA%B1/" title="通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-08</div><div class="title">通过NPM生态系统的依赖树揭开脆弱性传播及其演化的神秘面纱</div></div></a></div><div><a href="/2023/04/03/Flexible-and-Optimal-Dependency-Management-via-Max-SMT/" title="Flexible and Optimal Dependency Management via Max-SMT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-03</div><div class="title">Flexible and Optimal Dependency Management via Max-SMT</div></div></a></div><div><a href="/2023/04/11/What-the-Fork-Finding-Hidden-Code-Clones-in-npm/" title="What the Fork Finding Hidden Code Clones in npm"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">What the Fork Finding Hidden Code Clones in npm</div></div></a></div><div><a href="/2023/04/12/Static-Type-Inference-for-Foreign-Functions-of-Python/" title="Static Type Inference for Foreign Functions of Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="title">Static Type Inference for Foreign Functions of Python</div></div></a></div><div><a href="/2023/04/13/Where-to-Start-Studying-Type-Annotation-Practices-in-Python/" title="Where to Start Studying Type Annotation Practices in Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-13</div><div class="title">Where to Start Studying Type Annotation Practices in Python</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lin Li</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">119</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation"><span class="toc-number">1.</span> <span class="toc-text">LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#INTRODUCTION"><span class="toc-number">1.2.</span> <span class="toc-text">INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BACKGROUND-amp-RELATED-WORK"><span class="toc-number">1.3.</span> <span class="toc-text">BACKGROUND &amp; RELATED WORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-LLM-based-Code-Generation"><span class="toc-number">1.3.1.</span> <span class="toc-text">A. LLM-based Code Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Hallucinations-in-LLMs"><span class="toc-number">1.3.2.</span> <span class="toc-text">B. Hallucinations in LLMs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EVALUATION-SETUP"><span class="toc-number">1.4.</span> <span class="toc-text">EVALUATION SETUP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Dataset"><span class="toc-number">1.4.1.</span> <span class="toc-text">A. Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Studied-LLMs"><span class="toc-number">1.4.2.</span> <span class="toc-text">B. Studied LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Taxonomy-Annotation"><span class="toc-number">1.4.3.</span> <span class="toc-text">C. Taxonomy Annotation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EVALUATION-RESULTS"><span class="toc-number">1.5.</span> <span class="toc-text">EVALUATION RESULTS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-RQ1-Hallucination-Taxonomy"><span class="toc-number">1.5.1.</span> <span class="toc-text">A. RQ1: Hallucination Taxonomy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-RQ2-LLM-Comparison"><span class="toc-number">1.5.2.</span> <span class="toc-text">B. RQ2: LLM Comparison</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-RQ3-Root-Cause-Analysis"><span class="toc-number">1.5.3.</span> <span class="toc-text">C. RQ3: Root Cause Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MITIGATION-APPROACH"><span class="toc-number">1.6.</span> <span class="toc-text">MITIGATION APPROACH</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Motivation"><span class="toc-number">1.6.1.</span> <span class="toc-text">A. Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-RAG-based-Mitigation"><span class="toc-number">1.6.2.</span> <span class="toc-text">B. RAG-based Mitigation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Evaluation"><span class="toc-number">1.6.3.</span> <span class="toc-text">C. Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DISCUSSION"><span class="toc-number">1.7.</span> <span class="toc-text">DISCUSSION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#THREATS-TO-VALIDITY"><span class="toc-number">1.8.</span> <span class="toc-text">THREATS TO VALIDITY</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning/" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a><time datetime="2025-02-01T11:11:48.000Z" title="发表于 2025-02-01 19:11:48">2025-02-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/01/DeepSeek-Coder-When-the-Large-Language-Model-Meets-Programming-The-Rise-of-Code-Intelligence/" title="DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence">DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence</a><time datetime="2025-02-01T02:43:19.000Z" title="发表于 2025-02-01 10:43:19">2025-02-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/30/LLM-Hallucinations-in-Practical-Code-Generation-Phenomena-Mechanism-and-Mitigation/" title="LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation">LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</a><time datetime="2025-01-30T12:14:09.000Z" title="发表于 2025-01-30 20:14:09">2025-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/24/Benchmarking-Bias-in-Large-Language-Models-during-Role-Playing/" title="Benchmarking Bias in Large Language Models during Role-Playing">Benchmarking Bias in Large Language Models during Role-Playing</a><time datetime="2025-01-24T14:55:55.000Z" title="发表于 2025-01-24 22:55:55">2025-01-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/24/Diversity-Drives-Fairness-Ensemble-of-Higher-Order-Mutants-for-Intersectional-Fairness-of-Machine-Learning-Software/" title="Diversity Drives Fairness Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software">Diversity Drives Fairness Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software</a><time datetime="2025-01-24T12:23:10.000Z" title="发表于 2025-01-24 20:23:10">2025-01-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Lin Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>